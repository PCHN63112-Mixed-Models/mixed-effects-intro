{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Generalised Least Squares\n",
    "We will start our journey into the world of mixed-effects models by first examining a *related* approach that we have seen before: Generalised Least Squares (GLS). The reason for doing this is twofold. Firstly, GLS actually provides a simpler solution to many of the issues with the repeated measures ANOVA and thus presents a more logical starting point. Secondly, limitations in the way that GLS does this will provide some motivation for mixed-effects as a more complex, but ultimately more flexible, method of dealing with this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb16c4",
   "metadata": {},
   "source": [
    "## GLS Theory\n",
    "We previously came across GLS in the context of allowing different variances for different groups of data in ANOVA-type models. This was motivated as a way of lifting the assumption of *homogeneity of variance*. However, GLS is actually a much more general technique. To see this, note that the probability model for GLS is\n",
    "\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ can take on *any structure*. In other words, GLS has exactly the same probability model as the normal linear model, except that it allows for a flexible specification of the variance-covariance matrix. In our previous examples, we used GLS to populate the variance-covariance matrix with different variances for each group. For instance, if we had two groups with three subjects each, our GLS model would be\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "y_{21} \\\\\n",
    "y_{31} \\\\\n",
    "y_{12} \\\\\n",
    "y_{22} \\\\\n",
    "y_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "\\sim\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "\\sigma^{2}_{1}  & 0              & 0              & 0              & 0              & 0              \\\\\n",
    "0               & \\sigma^{2}_{1} & 0              & 0              & 0              & 0              \\\\\n",
    "0               & 0              & \\sigma^{2}_{1} & 0              & 0              & 0              \\\\\n",
    "0               & 0              & 0              & \\sigma^{2}_{2} & 0              & 0              \\\\\n",
    "0               & 0              & 0              & 0              & \\sigma^{2}_{2} & 0              \\\\\n",
    "0               & 0              & 0              & 0              & 0              & \\sigma^{2}_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "This was actually a special case of GLS known as *weighted least squares* (WLS)[^weights-foot], where all the off-diagonal elements of $\\boldsymbol{\\Sigma}$ are 0. However, the crucial point is that  we can use GLS to impose differences in *both* the variances *and* the covariances. So while we did not do this previously, we can include *correlation* in the GLS model. Thus, if our general problem with repeated measures is that the variance-covariance structure is not correctly handled by the normal linear model, GLS provides a direct solution. Furthermore, if a core complaint of the repeated measures ANOVA is that the covariance structure that is assumed is too restrictive, GLS again provides a direct solution. So, on the face of it, GLS directly solves many of the issues we encountered last week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f1f5c",
   "metadata": {},
   "source": [
    "### How Does GLS Work?\n",
    "Technically, the machinery behind GLS is based on assuming we know $\\boldsymbol{\\Sigma}$ *a priori*. Although this would seem a silly place to start (given that we will almost *never* know this), we can go along with it for a little bit and see where it gets us. So, *if* we know what the true covariance structure is, GLS provides a mechanism for *removing* it from the data[^white-foot]. Once removed, the errors return to $i.i.d.$ and we are back in the world of a regular regression model. This is a very enticing prospect because all the difficulties associated with correlation effectively *disappear* and we can treat the data as a regular collection of independent values. So, GLS provides the mechanism for this, the only question is how to apply it?\n",
    "\n",
    "In terms of the real world, if we do not known $\\boldsymbol{\\Sigma}$ then we cannot technically use GLS. This would seem a bit of a dead-end. However, it is possible to use a method such as REML to *estimate* $\\boldsymbol{\\Sigma}$ from the data. This is known as *feasible generalised least squares* (FGLS). The question then becomes, how does working with $\\hat{\\boldsymbol{\\Sigma}}$ rather than $\\boldsymbol{\\Sigma}$ change things? The usual case for justifying FGLS is when the *sample size is large*. Why? Because as the sample size gets bigger, the closer $\\hat{\\boldsymbol{\\Sigma}}$ gets to $\\boldsymbol{\\Sigma}$. In the limit, as $n \\rightarrow \\infty$, our estimate will become *identical* to the true value and we can just continue *as if* we knew the true value all along. This makes FGLS *asymptotically* correct. So, if we have a large sample size, we can effectively *ignore* any problems created by estimating $\\boldsymbol{\\Sigma}$ and just continue as if we knew it all along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869d667",
   "metadata": {},
   "source": [
    "`````{admonition} How Large is \"Large\"?\n",
    ":class: tip\n",
    "If we want to lean on asymptotic theory and pretend that everything is fine when using FGLS, the obvious question is \"how big does $n$ need to be?\". The problem is that the definition is based on a *limit*, so it says that the approximation gets better and better as $n$ moves towards infinity. For our purpose, $n$ is the *number of subjects*, rather than the total amount of data. So, the answer is not that there is some magic sample size that is suddenly large enough, the answer is that the approximation will get better the larger $n$ becomes. The question then is more about what our tolerance for error is. The point of the asymptotic theory is to say that the error that comes from estimation becomes more negligible as $n$ grows, as does the penalty for estimating $\\boldsymbol{\\Sigma}$ from the data. So, unfortunately, there is *no honest numeric answer to this question*. The way to think about it is as a *degree of comfort*. If you are using FGLS with $n = 5$, you should feel *very uncomfortable*. If you are using $n = 50$, you should probably feel *cautious* and if you have $n > 200$ you should probably be feeling *reasonably comfortable*. As $n$ increases beyond that, you should probable feel perfectly fine about the FGLS assumptions. These are only ballpark figures, but the point is really to think of $n$ as a *continuum of comfort*, rather than as a *threshold*. \n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98aba0",
   "metadata": {},
   "source": [
    "So, what happens when the sample size is *small*, or when we are not comfortable relying on asymptotic results? This is unfortunately where everything starts to unravel. If we *do not* know $\\boldsymbol{\\Sigma}$ (are are unwilling to pretend as if we did), then the term $\\boldsymbol{\\Sigma}$ is not longer a *fixed constant*. Instead, we have $\\hat{\\boldsymbol{\\Sigma}}$, which is a *random variable*. This is an additional layer of uncertainty that causes some major issues. The full story is given in the drop-down below, but the short version is that treating $\\hat{\\boldsymbol{\\Sigma}}$ as an *estimate* means we no longer know what null distribution our test statistic has. The whole concept of degrees of freedom suddenly disappear, we have no sense of uncertainty in that estimate and we have no way of calculating a $p$-value. In short, *the inferential machinery of NHST falls apart*. We will discuss how this is accommodated practically a little further below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76209ff",
   "metadata": {},
   "source": [
    "`````{admonition} How Estimating $\\boldsymbol{\\Sigma}$ Breaks Inference\n",
    ":class: tip, dropdown\n",
    "\n",
    "To understand why the NHST machinery breaks, we need to go back to some of the information covered last semester on [statistical inference](https://pchn63101-advanced-data-skills.github.io/Inference-Linear-Model/2.estimation-uncertainty.html). Recall that in the normal linear model the uncertainty that comes from estimating $\\sigma^{2}$ affects inference via the *denominator* of the test statistic. If we *know* $\\sigma^{2}$, then our test statistic is a $z$-statistic and is distributed as $z \\sim \\mathcal{N}(0,1)$. However, when $\\sigma^{2}$ is *estimated*, the test statistic is a $t$-statistic and is distributed as $t \\sim \\mathcal{T}(\\nu)$. Here, $\\nu$ is the *degrees of freedom*, which characterises the *uncertainty* in the estimate of $\\sigma^{2}$. This is what controls the *width* of the $t$-distribution, which will approach the standard normal as the sample size increases.\n",
    "\n",
    "Now, the whole reason the $t$-distribution exists is because it can be derived from the structure of the test statistic. Because both the numerator and denominator are *estimates*, they are both random variables with a given sampling distribution. In order to work out the distribution of their ratio, *both* sampling distributions need to be derived. Last semester, we showed that the distribution of the parameter estimates was a known quantity. For a single slope from a typical regression model, we have\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{1} \\sim \\mathcal{N}\\left(\\beta_{1}, \\frac{\\sigma^{2}}{\\sum{(x_{i} - \\bar{x})^{2}}}\\right).\n",
    "$$\n",
    "\n",
    "Importantly, the variance of this distribution depends upon knowing $\\sigma^{2}$, which we do not. If we replace this with an *estimate*, $\\hat{\\sigma}^{2}$, we introduce another layer of uncertainty. In order to characterise this addition layer, we need to know the distribution of $\\hat{\\sigma}^{2}$. We glossed-over this last semester, but under the normal linear model this estimate has the following sampling distribution\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} \\sim \\frac{\\sigma^{2}}{\\nu}\\chi^{2}(\\nu)\n",
    "$$\n",
    "\n",
    "This is a $\\chi^{2}$ distribution with $\\nu$ degrees of freedom that is scaled into the same units as the variance. The thing to notice here is that this distribution is *where degrees of freedom come from*. The reason degrees of freedom exist is because they appear as a parameter that governs the *width* of this sampling distribution. Thus, degrees of freedom directly encode uncertainty around the true value of $\\sigma^{2}$. As the sample size goes up, the degrees of freedom go up and the $\\chi^{2}$ gets *narrower* until it collapses into a single point centred on $\\sigma^{2}$. Because the standard error of the parameter estimate depends upon the estimate of $\\sigma^{2}$, the scaled $\\chi^{2}$ distribution passes this uncertainty on to the $t$-distribution. As such, the $t$-distribution is similarly parameterised by the degrees of freedom. The point where the $\\chi^{2}$ collapses to a single point is *exactly* when the $t$-distribution and the standard normal become *the same*. At that point, uncertainty is effectively 0 and the degrees of freedom are no longer important. \n",
    "\n",
    "Now, getting back to FGLS, what happens to the standard error when $\\sigma^{2}$ is no longer *a single number* and is instead a complex function of different elements of an unstructured variance-covariance matrix? Well, the formula for the standard error remains *correct*, but the distribution can no longer be derived analytically. It ceases to be a consistent object across all models and, in effect, becomes *unknowable*. This means that the scaled $\\chi^{2}$ distribution disappears and, along with it, so too does the concept of degrees of freedom as a means of characterising uncertainty. If we divide our parameter estimate by its standard error it is no longer the ratio of two random variables with known distributions. It is the ratio of a random variable with a known distribution and a random variable *with no known distribution*. This makes the null distribution of this test statistic *also unknown*. And without a known null, there is no way to calculate a $p$-value. So, if we accept that $\\hat{\\boldsymbol{\\Sigma}}$ is an *estimate* then our test statistic has no known null distribution, our ability to calculate $p$-values disappears and the concept of degrees of freedom vanishes. In short, we are stuck.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09afb49",
   "metadata": {},
   "source": [
    "### Covariance Constraints\n",
    "As well as understanding that the very process of estimating $\\boldsymbol{\\Sigma}$ causes problems, we also need to understand that we cannot have free reign to estimate any old covariance structure we like. One of the most important elements to recognise is that some sort of *constraint* is always needed when estimating a variance-covariance matrix. To see this, note that for a repeated measures experiment there are $nt \\times nt$ values in this matrix. The values above and below the diagonal are a mirror image, so the true number of unknown values is $\\frac{nt(nt + 1)}{2}$. For instance, if we had $n = 5$ subjects and $t = 3$ repeated measures, there would be $\\frac{15 \\times 16}{2} = 120$ unique values in the variance-covariance matrix. If we allowed it to be completely unstructured, we would have 120 values to estimate *just* for the covariance structure. Indeed, this is not really possible unless the amount of data we have *exceeds* the number of parameters. So, the data itself imposes a *constraint* on how unstructured the covariance matrix can be.\n",
    "\n",
    "Luckily, for most applications, we not only assume that $\\boldsymbol{\\Sigma}$ has a block-diagonal structure (so most off-diagonal entries are 0), but that many of the off-diagonal elements are actually *identical*. We saw this previously with the repeated measures ANOVA. Even though $\\boldsymbol{\\Sigma}$ may have *hundreds* of values we *could* fill-in, if we assume compound symmetry only within each subject, there are only *two* covariance parameters to be estimated: $\\sigma^{2}_{b}$ and $\\sigma^{2}_{w}$. The whole matrix can then be constructed using those two alone. This is an example of *extreme simplification*, but it does highlight that we generally do not estimate the *whole* variance-covariance matrix. We only estimate *small parts* of it. Indeed, making the covariance matrix more general is often a risky move because of the number of additional parameters needed. The more we estimate from the same data, the greater our uncertainty will become because each element of the covariance-matrix is supported by *less data*. Complexity always comes at a price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4b682",
   "metadata": {},
   "source": [
    "## GLS in `R`\n",
    "We have seen some examples of using the `gls()` function from `nlme` last semester. At that point, we only focused on the use of the `weights=` argument with different variance structures (e.g. `varIdent()`, `varPower()` etc.). However, there is also a `correlation=` argument that similarly takes a number of pre-specified correlation structures. We can use these two arguments together to form a final variance-covariance matrix that consists of correlation and heterogenous variance groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7130e8",
   "metadata": {},
   "source": [
    "### The Paired $t$-test Using GLS\n",
    "We can start with the most simple example of the paired $t$-test using GLS. Importantly, this is an unnecessary step theoretically because the paired $t$-test is a perfectly acceptable technique. When there are only two-repeats there are no arguments about the covariance structure. There can only be a single correlation term. So compound symmetry always works. However, it is useful for us as a *starting point* because it is the simplest example of the problem.\n",
    "\n",
    "In order to specify a correlation structure, we need to pass one of the predefined correlation functions as an argument to `correlation=`. These structures include functions such as `corCompSymm()`, `corSpher()`, `corAR1()` and `corSymm()`[^corfunc-foot]. For this example, we will use `corCompSymm()`, which constructs a compound symmetric structure.\n",
    "\n",
    "In order to use `corCompSymm()`, we need to supply it with a description of how we want it structured in relation to our data. This is done using the `form=` argument, which takes a one-sided formula expressing the structure we want. For this example, we will use `corCompSymm(form= ~1|subject)`. This indicates that we want a constant correlation (`1`) grouped by subject (`|subject`). So, the term on the *right* of `|` is key here. This gives a *grouping factor* such that any observations from the same level will share a constant correlation. Because we have used `subject`, each level represents a *different* subject and thus any observations that come from the same subject will be correlated. This therefore defines our *block-diagonal* covariance structure, where the term on the right of `|` forms the *blocks*. We will see ways to visualise this in order to provide more intuition a little later.\n",
    "\n",
    "Returning to our example, we will use the `mice2` data from `datarium` again, which we have converted to long-format as discussed last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1380ad0e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   time weight\n",
      "1   1 before  187.2\n",
      "2   1  after  429.5\n",
      "3   2 before  194.2\n",
      "4   2  after  404.4\n",
      "5   3 before  231.7\n",
      "6   3  after  405.6\n",
      "7   4 before  200.5\n",
      "8   4  after  397.2\n",
      "9   5 before  201.7\n",
      "10  5  after  377.9\n",
      "11  6 before  235.0\n",
      "12  6  after  445.8\n",
      "13  7 before  208.7\n",
      "14  7  after  408.4\n",
      "15  8 before  172.4\n",
      "16  8  after  337.0\n",
      "17  9 before  184.6\n",
      "18  9  after  414.3\n",
      "19 10 before  189.6\n",
      "20 10  after  380.3\n"
     ]
    }
   ],
   "source": [
    "library('datarium')\n",
    "library('reshape2')\n",
    "data('mice2')\n",
    "\n",
    "# repeats and number of subjects\n",
    "t <- 2\n",
    "n <- dim(mice2)[1]\n",
    "\n",
    "# reshape wide -> long\n",
    "mice2.long <- melt(mice2,                       # wide data frame\n",
    "                   id.vars='id',                # what stays fixed?\n",
    "                   variable.name=\"time\",        # name for the new predictor\n",
    "                   value.name=\"weight\")         # name for the new outcome\n",
    "\n",
    "mice2.long <- mice2.long[order(mice2.long$id),] # order by ID\n",
    "rownames(mice2.long) <- seq(1,n*t)              # fix row names\n",
    "mice2.long$id <- as.factor(mice2.long$id)\n",
    "\n",
    "print(mice2.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb71201",
   "metadata": {},
   "source": [
    "To fit this model using GLS, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd3d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nlme)\n",
    "\n",
    "gls.mod <- gls(weight ~ time, correlation=corCompSymm(form=~1|id), data=mice2.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa569916",
   "metadata": {},
   "source": [
    "where we can see the use of the `correlation=` argument with the `corCompSymm()` function. We could also optionally include a `weights=` argument if we wanted the diagonal elements of the covariance matrix to differ by `time`. This would take the form `weights=varIdent(form= ~1|time)`. However, we will keep this simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df83e3",
   "metadata": {},
   "source": [
    "### Inference Using GLS\n",
    "Although we should check the assumptions of the GLS model, we will leave that to one side given that we covered it last semester. The more pressing issue for us is to discuss *inference* using the GLS model, especially given the problems highlighted earlier. To begin with, we can treat the returned objects from `gls()` just like an object from `lm()` and call `summary()` to examine the model estimates and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f15bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized least squares fit by REML\n",
      "  Model: weight ~ time \n",
      "  Data: mice2.long \n",
      "      AIC      BIC    logLik\n",
      "  177.349 180.9105 -84.67449\n",
      "\n",
      "Correlation Structure: Compound symmetry\n",
      " Formula: ~1 | id \n",
      " Parameter estimate(s):\n",
      "      Rho \n",
      "0.5332493 \n",
      "\n",
      "Coefficients:\n",
      "             Value Std.Error  t-value p-value\n",
      "(Intercept) 200.56  8.081914 24.81591       0\n",
      "timeafter   199.48  7.808574 25.54628       0\n",
      "\n",
      " Correlation: \n",
      "          (Intr)\n",
      "timeafter -0.483\n",
      "\n",
      "Standardized residuals:\n",
      "        Min          Q1         Med          Q3         Max \n",
      "-2.46661859 -0.54818094  0.02112903  0.38482224  1.79048964 \n",
      "\n",
      "Residual standard error: 25.55725 \n",
      "Degrees of freedom: 20 total; 18 residual\n"
     ]
    }
   ],
   "source": [
    "print(summary(gls.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf9479",
   "metadata": {},
   "source": [
    "We can see here all the usual output that matches what `lm()` gives us. Indeed, despite what we mentioned earlier, we still get $t$-statistics and $p$-values (though note that the $p$-values are *rounded* rather than displayed using scientific notation). We also have information on the estimated correlation structure, with the single correlation parameter given by $\\hat{\\rho} = 0.53$. Of most importance is that both the *standard error* and *$t$-value* match what we saw last week from the paired $t$-test. This is evidence enough to show that the correlation *is* being taken into account. Importantly, this is being done within a linear model framework, but *without* the need to subtract the differences *or* to manually partition the errors by including `id` in the model formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8babf",
   "metadata": {},
   "source": [
    "#### The $p$-value Problem\n",
    "Although all the elements highlighted above are positives, there is something important we need to recognise about the reported $p$-values. As mentioned earlier, if we work with the idea that $\\boldsymbol{\\Sigma}$ is *estimated* then the null distribution of the test statistics become unknown and there is no way to calculate a $p$-value. And yet, the `gls()` function *does* produce $p$-values. How? *Because it is acting as if $\\boldsymbol{\\Sigma}$ is known*. In effect, even though we are using FGLS, the `gls()` function ignores this and acts as if we are doing *proper* GLS with a known covariance structure. All the calculations of $t$-statistics, degrees of freedom and $p$-values are based on assuming that $\\boldsymbol{\\Sigma}$ is known and thus we can perfectly remove the covariance structure from the data and end-up back in the world of a normal linear model.\n",
    "\n",
    "The problem with this is that the uncertainty in estimating $\\boldsymbol{\\Sigma}$ is not taken into account. Indeed, we have seen that there is no exact way of quantifying this uncertainty because the concept of degrees of freedom vanishes as soon as the sampling distribution becomes unknown. The `gls()` function is *pretending* that we known $\\boldsymbol{\\Sigma}$ and can return the model to a world where degrees of freedom exist and null distributions are known. The extent to which we can trust the $p$-values therefore depends upon the extent to which we are willing to follow along with this fiction. At best, we need to treat these values as *very rough approximations* and nothing more. This is especially true in *small samples*. As mentioned earlier, as $n$ gets larger the estimate of $\\hat{\\boldsymbol{\\Sigma}}$ gets closer to the true value and the uncertainty largely disappears. So if our sample is *large*, we have less concern with the GLS tests. However, great caution is needed if our sample is *small*.\n",
    "\n",
    "Part of the problem here is that FGLS knows *nothing* about the structure of the data. The very fact that we have repeated measurements from multiple subjects is somewhat lost inside of $\\boldsymbol{\\Sigma}$. All GLS knows is that there *is* correlation, but not *where it came from*. As we will see, mixed-effects models actively embed the structure that causes the data to be correlated. This does not magically get rid of the problem of an unknown sampling distribution under an arbitrary covariance structure, but it does allow for a better approximation of the uncertainty that comes from estimating $\\boldsymbol{\\Sigma}$. So, a mixed-effects model will not fix this situation, but it does give us better options beyond simply pretending that $\\boldsymbol{\\Sigma}$ is known. This is not the main reason for using mixed-effects over GLS, but it is an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f87375",
   "metadata": {},
   "source": [
    "#### Omnibus Tests and Follow-ups\n",
    "Putting aside the inferential issues above, the utility of the GLS framework is that we can simply treat the model in the same way as a result from `lm()`. This means we can compute ANOVA-style tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0769e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Denom. DF: 18 \n",
       "            numDF   F-value p-value\n",
       "(Intercept)     1 1800.9410  <.0001\n",
       "time            1  652.6124  <.0001"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova(gls.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be916e",
   "metadata": {},
   "source": [
    "generate confidence intervals (though these should similarly be treated cautiously),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84664f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               2.5 %   97.5 %\n",
       "(Intercept) 184.7197 216.4003\n",
       "timeafter   184.1755 214.7845"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confint(gls.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6acde",
   "metadata": {},
   "source": [
    "create plots using the `effects` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2e6d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: carData\n",
      "lattice theme set by effectsTheme()\n",
      "See ?effectsTheme for details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library('effects')\n",
    "plot(effect('time', gls.mod, residuals=TRUE), partial.residuals=list(smooth=FALSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963be8e",
   "metadata": {},
   "source": [
    "and compute follow-up tests using `emmeans`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3275588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to emmeans.\n",
      "Caution: You lose important information if you filter this package's results.\n",
      "See '? untidy'\n",
      " contrast       estimate   SE df t.ratio p.value\n",
      " before - after     -199 7.81 17 -25.546  <.0001\n",
      "\n",
      "Degrees-of-freedom method: df.error \n"
     ]
    }
   ],
   "source": [
    "library(emmeans)\n",
    "emm <- emmeans(gls.mod, pairwise ~ time, mode=\"df.error\")\n",
    "print(emm$contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f4411",
   "metadata": {},
   "source": [
    "where the `mode=` option is used so that the tests agree with those reported by `gls()`. \n",
    "\n",
    "So, if we put the inferential issues to one side, we can see that GLS provides a nice alternative to the repeated measures ANOVA because it exists within the linear model framework and thus allows us to use all the methods we have seen previously. Furthermore, we can lift the assumption of compound symmetry and use a variety of different covariance matrices. The main downside is that this comes with a price in terms of the $p$-value not taking the uncertainty in the estimation of the covariance structure into account. This is especially problematic in *small samples*, which means we should really treat the hypothesis tests from a GLS model as only *asymptotically correct*. If we are happy to do so, GLS becomes quite a useful method to have on hand. We will see a few more examples of using GLS with more complex ANOVA models in the associated workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ecb4f",
   "metadata": {},
   "source": [
    "## When Should We Use GLS?\n",
    "GLS is most useful in situations where we know what covariance structure we want to impose on the data and are not as concerned about the model not understanding any form of deeper structure in the data. However, if we do not know the covariance structure and know that the data has a deeper structure that the model should be able to exploit, this is where mixed-effects models come into their own. \n",
    "\n",
    "... The problem is that GLS does not know anything about the *structure* of the data. It has no sense of *subjects* as the experimental unit, nor the idea that the outcome variable is comprised of clusters of values taken from different subjects who might themselves form clusters of values from larger groups (e.g. patients vs controls). All that GLS knows is that there is a correlation structure that we want to remove. Unfortunately, this lack of appreciation for the structure of the data means that GLS cannot use that structure to its advantage. There is no separation of the information available by pooling observations across subjects, or subjects across groups. In effect, GLS is a very *crude* solution to a bigger problem with repeated measurements. Namely, that there is a larger *hierarchical* structure at play that the model should be able to take advantage of. We have seen this in a very general way through small-sample degrees of freedom, but really this is only a *symptom* of a larger problem. As we will come to learn, mixed-effects models are advantageous precisely *because* they embed this structure in the model. This has a number of consequences, not least the fact that correlation between measurements from the same experimental unit are *automatically* embedded in the model. This is not because we tell the model to include correlation, rather it is a *natural consequence* of the structure of the data. As such, mixed-effects models are useful because features such as correlation are a natural part of the modelling framework, precisely because it does take the structure into account in a way that GLS simply cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27232f2",
   "metadata": {},
   "source": [
    "[^summarystat-foot]: This approach is, unsurprisingly, known as a *summary statistics* approach and is typical of how data analysis is handled for fMRI and M/EEG data.\n",
    "\n",
    "[^weights-foot]: This is why the argument in `gls()` was `weights=`.\n",
    "\n",
    "[^corfunc-foot]: You can look up descriptions of all of these using `?corClasses` at the prompt. \n",
    "\n",
    "[^student-foot]: Remember that historically the uncertainty in the standard error was *not* taken into account. Statisticians assumed that they could treat the standard error as a *known constant*, at which point the null distribution is simply a scaled version of the distribution of the numerator. For the normal linear model, this would mean using a *standard normal* distribution (or $z$-distribution) for inference. It was Student's insight that this does not always hold because the uncertainty in the standard error *changes* the distribution, with this change becoming more extreme the smaller the sample. \n",
    "\n",
    "[^white-foot]: This is sometimes known as *whitening* the data, for reasons we will not go into. This is a term you may come across in the neuroimaging literature, particularly in relation to how fMRI is analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a60bd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}