{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# The Multilevel Framework\n",
    "We will start our journey into the world of mixed-effects models from the perspective of a *multilevel* model. This is primarily because we can build the pieces of a mixed-effects model very slowly from first principles, allowing the logic to become much clearer. In addition, the multilevel framework is often the most *intuitive* way to think about these models, they are just less frequently implemented in this fashion. So, we generally advise to *think* about these models in a multilevel fashion, even if *practically* we end up specifying them in a mixed-effects fashion. This will all become clearer once we have discussed *both* perspectives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca08be4",
   "metadata": {},
   "source": [
    "## Fitting a Model to One Subject\n",
    "To start understanding multilevel models, let us imagine that we only have the data for *one subject*. Going back to the long-formatted `selfesteem` data from `datarium`, let us extract the data associated with subject 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fffe213",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "library('datarium')\n",
    "library('reshape2')\n",
    "\n",
    "data('selfesteem')\n",
    "\n",
    "# repeats and number of subjects\n",
    "t <- 3\n",
    "n <- dim(selfesteem)[1]\n",
    "\n",
    "# reshape wide -> long\n",
    "selfesteem.long <- melt(selfesteem,            # wide data frame\n",
    "                        id.vars='id',          # what stays fixed?\n",
    "                        variable.name=\"time\",  # name for the new predictor\n",
    "                        value.name=\"score\")    # name for the new outcome\n",
    "\n",
    "selfesteem.long           <- selfesteem.long[order(selfesteem.long$id),] # order by ID\n",
    "rownames(selfesteem.long) <- seq(1,n*t)                                  # fix row names\n",
    "selfesteem.long$id        <- as.factor(selfesteem.long$id)               # convert ID to factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d574a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id time    score\n",
      "1  1   t1 4.005027\n",
      "2  1   t2 5.182286\n",
      "3  1   t3 7.107831\n"
     ]
    }
   ],
   "source": [
    "sub.1 <- selfesteem.long[selfesteem.long$id == '1',]\n",
    "print(sub.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866bb61",
   "metadata": {},
   "source": [
    "So, we can see that we have 3 repeated measurements associated with the 3 values of `time`. Importantly, there are no replications at each time-point, so this is all the information we have available. Now, we know these values will be *correlated* by virtue of coming from the same subject, but we will put that to one side for now because it is a distraction. Instead, our focus here is simply *what model of these data is possible*?\n",
    "\n",
    "We might be tempted to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4676d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.sub.1 <- lm(score ~ time, data=sub.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6fb62",
   "metadata": {},
   "source": [
    "However, there is a problem here. Because each level of `time` is associated with *one* data point, there is no sense in which the parameter estimates can be *average* effects, or *summaries* of any kind. They will simply be *identical* to the raw data. The fitting process aims to minimise the errors, so if it can make them 0 it has done the best job it can. In this example, the model will fit the data *perfectly* and we will be left with *no error*. This means we can get parameter estimates, but nothing else. We *need* residual error for everything else to work. So, we end up with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3dc1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = score ~ time, data = sub.1)\n",
       "\n",
       "Residuals:\n",
       "ALL 3 residuals are 0: no residual degrees of freedom!\n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)\n",
       "(Intercept)    4.005        NaN     NaN      NaN\n",
       "timet2         1.177        NaN     NaN      NaN\n",
       "timet3         3.103        NaN     NaN      NaN\n",
       "\n",
       "Residual standard error: NaN on 0 degrees of freedom\n",
       "Multiple R-squared:      1,\tAdjusted R-squared:    NaN \n",
       "F-statistic:   NaN on 2 and 0 DF,  p-value: NA\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lm.sub.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa836da8",
   "metadata": {},
   "source": [
    "This is an important point because the *data* is the element that *constrains* the model. We might *desire* something more complex, but we can only do so if the data supports it. This is quite important to understand as we go forward.\n",
    "\n",
    "So, if we cannot fit the model we want, what can we fit? Well, because the problem is that we have no replications within each level of `time`, we cannot use the variable `time` at all. Instead, the best we can do is just fit an intercept. The model for subject $i = 1$ from time-point $j$ is simply\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{1j}    &= \\mu_{1} + \\eta_{1j} \\\\\n",
    "    \\eta_{1j} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}\\right)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Where we have used $\\eta$ to refer to the errors rather than the usual $\\epsilon$, for reasons that will become clearer as we progress. In `R`, this model would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5662d807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = score ~ 1, data = sub.1)\n",
       "\n",
       "Residuals:\n",
       "      1       2       3 \n",
       "-1.4267 -0.2494  1.6761 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)  \n",
       "(Intercept)   5.4317     0.9043   6.006   0.0266 *\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 1.566 on 2 degrees of freedom\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.sub.1 <- lm(score ~ 1, data=sub.1)\n",
    "summary(lm.sub.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce65b9",
   "metadata": {},
   "source": [
    "So now we have residuals and everything else will work. There is nothing ground-breaking or Earth-shattering about any of this. All we are concluding is that, based on only having a *single* subject from this experiment, the best we could do is model a *subject-specific constant* and nothing else. In this example, the average value of `score` for subject 1 was estimated to be $\\hat{\\mu}_{1} = 5.43$. That is it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37914e0a",
   "metadata": {},
   "source": [
    "## Extending the Model to Multiple Subjects\n",
    "Of course, we do not *only* have subject 1. So let us introduce subject 2 into this framework and see where it gets us. Much like subject 1, if we extract the data for `id == '2'` and consider it in *isolation*, all we can do is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6fdad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id time    score\n",
      "4  2   t1 2.558124\n",
      "5  2   t2 6.912915\n",
      "6  2   t3 6.308434\n"
     ]
    }
   ],
   "source": [
    "sub.2 <- selfesteem.long[selfesteem.long$id == '2',]\n",
    "print(sub.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7048f82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = score ~ 1, data = sub.2)\n",
       "\n",
       "Residuals:\n",
       "     4      5      6 \n",
       "-2.702  1.653  1.049 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)  \n",
       "(Intercept)    5.260      1.362   3.862    0.061 .\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 2.359 on 2 degrees of freedom\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.sub.2 <- lm(score ~ 1, data=sub.2)\n",
    "summary(lm.sub.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59abe0",
   "metadata": {},
   "source": [
    "and then conclude that the average `score` for subject 2 is $\\hat{\\mu}_{2} = 5.260$. In isolation, we therefore have the following two models\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{1j} &= \\mu_{1} + \\eta_{1j} \\\\\n",
    "    y_{2j} &= \\mu_{2} + \\eta_{2j} \\\\\n",
    "\\end{alignat*}.\n",
    "$$\n",
    "\n",
    "But, of most importance, is that we are not *really* working in isolation. We have *both* subjects together. Recall that the problem with trying to fit an effect of `time` within a single subject was that there were no replications and thus *no variance*. This is true *within* each subject, but *across* the two subjects we *do* have replications of each level of `time`. If we put the two datasets together, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbe69e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  id time    score\n",
       "1  1   t1 4.005027\n",
       "2  1   t2 5.182286\n",
       "3  1   t3 7.107831\n",
       "4  2   t1 2.558124\n",
       "5  2   t2 6.912915\n",
       "6  2   t3 6.308434"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbind(sub.1,sub.2) # row-bind function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574a557",
   "metadata": {},
   "source": [
    "So, now we have *two* values of `t1`, *two* values of `t2` and *two* values of `t3`. This means we *can* introduce an effect of `time` that will be estimated *across* the subjects. If we call the effect of the $j$th level of `time` $\\alpha_{j}$, we can think of these two models as\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{1j} &= \\mu_{1} + \\alpha_{j} + \\eta_{1j} \\\\\n",
    "    y_{2j} &= \\mu_{2} + \\alpha_{j} + \\eta_{2j} \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "which, across all subjects, gives us\n",
    "\n",
    "$$\n",
    "y_{ij} = \\mu_{i} + \\alpha_{j} + \\eta_{ij}.\n",
    "$$\n",
    "\n",
    "So, we now have a *subject-specific* mean ($\\mu_{i}$) and an effect of `time` ($\\alpha_{j}$). But notice that there is no subject index on $\\alpha_{j}$ meaning that its value is *the same* irrespective of the specific subject. This is important because it indicates that $\\alpha_{j}$ captures something *universal* across *all* subjects. \n",
    "\n",
    "In `R`, this complete model would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1521bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = score ~ id + time, data = selfesteem.long)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-1.3509 -0.5233 -0.0888  0.5304  1.9560 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  3.33503    0.60780   5.487 3.28e-05 ***\n",
       "id2         -0.17189    0.78466  -0.219  0.82907    \n",
       "id3          0.39031    0.78466   0.497  0.62491    \n",
       "id4          0.06107    0.78466   0.078  0.93882    \n",
       "id5         -1.01940    0.78466  -1.299  0.21029    \n",
       "id6         -0.75183    0.78466  -0.958  0.35067    \n",
       "id7         -0.11610    0.78466  -0.148  0.88402    \n",
       "id8         -0.30895    0.78466  -0.394  0.69840    \n",
       "id9          0.02795    0.78466   0.036  0.97198    \n",
       "id10        -0.06029    0.78466  -0.077  0.93960    \n",
       "timet2       1.79382    0.42978   4.174  0.00057 ***\n",
       "timet3       4.49622    0.42978  10.462 4.44e-09 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.961 on 18 degrees of freedom\n",
       "Multiple R-squared:  0.8656,\tAdjusted R-squared:  0.7834 \n",
       "F-statistic: 10.54 on 11 and 18 DF,  p-value: 9.779e-06\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.all.subs <- lm(score ~ id + time, data=selfesteem.long)\n",
    "summary(lm.all.subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8679d2",
   "metadata": {},
   "source": [
    "So we now have *subject-specific* intercepts, as well as a *universal* effect of `time`. \n",
    "\n",
    "In general, what we have done here is create a model that has a *different predicted value for each subject*. Each subject gets their *own model*. Subject 1 is\n",
    "\n",
    "$$\n",
    "E(y_{1j}) = \\mu_{1} + \\alpha_{j} = \\mu_{1j},\n",
    "$$\n",
    "\n",
    "subject 2 is\n",
    "\n",
    "$$\n",
    "E(y_{2j}) = \\mu_{2} + \\alpha_{j} = \\mu_{2j}\n",
    "$$\n",
    "\n",
    "and so on. It is like the subjects form the *cells* of the design. Crucially, each subject's expected value is a *combination* of something specific to them $\\left(\\mu_{1}, \\mu_{2}, \\dots, \\mu_{n}\\right)$ and something *universal* from across subjects $\\left(\\alpha_{1},\\alpha_{2},\\alpha_{3}\\right)$. Thus, this model captures two important elements of our data: the *idiosyncrasies* of the individual and the *constant effect* in the population. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bf0fe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "````{admonition} Connections with the Repeated Measures ANOVA\n",
    ":class: tip\n",
    "Hopefully the connections with the repeated measures ANOVA are starting to emerge, but we will make this more explicit later in the lesson.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844a341",
   "metadata": {},
   "source": [
    "## Understanding $\\mu_{i}$ as a *Random Variable*\n",
    "In order to understand the next steps towards a complete multilevel model, we need to imagine that we ran the experiment *again* and collected a new subject. Take another look at the model\n",
    "\n",
    "$$\n",
    "y_{ij} = \\mu_{i} + \\alpha_{j} + \\eta_{ij}\n",
    "$$\n",
    "\n",
    "and remember that this is the theoretical *population-level* description of the data-generating process. As such, what do we imagine changes about this description for each *new* subject? Ponder this for a moment.\n",
    "\n",
    "If we have a new subject, we have a new value of $i$. So which of the terms above depend upon $i$? Certainly $\\mu_{i}$ does. Each subject has their own unique mean that is specific to them, so a *new* subject means a *new* value of $\\mu_{i}$. In addition, the errors $\\eta_{ij}$ depend upon the value of $i$ and will change with every observation. So, when we sample someone new both $\\mu_{i}$ and $\\eta_{ij}$ will change.\n",
    "\n",
    "Importantly, what does *not* change is $\\alpha_{j}$. Why? Because this has *no* subject-specific index. $\\alpha_{1}$ is the same whether $i = 1$ or $i = 1,427$. This is because this is a *universal effect* across all subjects. Its *estimate* would change with another subject (because there is now *more data*), but remember we are thinking about the true population description of the data-generating process here. In this sense, $\\alpha_{j}$ is a *constant*. It does not change with each sample, it remains a *fixed* and *unwavering* element of the universe.\n",
    "\n",
    "So, if $\\alpha_{j}$ is *fixed*, what does this mean for $\\mu_{i}$ and $\\eta_{ij}$? Well, what do we call a variable who's value changes every time we observe it? That's right: a *random variable*. \n",
    "\n",
    "So, both $\\mu_{i}$ and $\\eta_{ij}$ have to be *random variables*. We already know this about $\\eta_{ij}$ because we always think of the errors as random and ascribe them a probability distribution. So this is nothing new. What *is* new is having *another random variable in the model*. In fact, pretty much every complication that follows is a direct knock-on effect of having *multiple random variables* in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c03ef",
   "metadata": {},
   "source": [
    "\n",
    "````{admonition} Fixed-effects and Random-effects\n",
    ":class: tip\n",
    "Although we are currently focusing on the *multilevel* perspective, we can already see the *mixed-effects* perspective creeping in. A mixed-effects model is, by definition, a model that contains *both* population-level constants *and* random variables. These are usually referred to as *fixed-effects* and *random-effects*. So, in our example so far we have\n",
    "\n",
    "- $\\alpha_{j}$ - a *fixed-effect* \n",
    "- $\\mu_{i}$ - a *random-effect*\n",
    "\n",
    "We will discuss more about what it means to have these different types of effects in a model later in the lesson. For now, the only thing to note is that the *multilevel* and *mixed-effects* frameworks cannot be separated. A discussion of one is naturally a discussion of the other. They are simply different ways of viewing the *same thing*.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f74e4",
   "metadata": {},
   "source": [
    "Because *both* $\\mu_{i}$ and $\\eta_{ij}$ are *random variables* they will both, by definition, have some probability distribution that describes their behaviour over repeated samples. As we know, the $\\eta_{ij}$ are *errors* and thus reflect *deflections* around the expected value. As such, their distribution is the same as it always was \n",
    "\n",
    "$$\n",
    "\\eta_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^{2}_{1}\\right).\n",
    "$$\n",
    "\n",
    "But what about the $\\mu_{i}$? \n",
    "\n",
    "Well, as written above, these are *means* for each subject, so their expected value will not be 0. We saw already that our estimates for the first two subjects from `selfesteem` were $\\hat{\\mu}_{1} = 5.43$ and $\\hat{\\mu}_{2} = 5.26$. Clearly these are *not* 0 because they are on the same scale as `score`. So, instead, the expected value of each of these subject-specific means will be whatever the *population grand mean* is. If we stick with the convention of assuming that these subject means are normally distributed, we have\n",
    "\n",
    "$$\n",
    "\\mu_{i} \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}_{2}\\right),\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the grand mean of the population. \n",
    "\n",
    "There are several consequences of the assumptions made above. To begin with, there are now *two* probability distributions describing where our data come from. These distributions have different variances ($\\sigma^{2}_{1}$ and $\\sigma^{2}_{2}$), meaning we have *two* sources of error to now consider. We also need to *estimate* both of these variances from the data in order to complete the unknowns for this model. Last semester, our focus was almost exclusively on the *mean function*, but now we can see that we are shifting focus and adding complexity to the *variance function*. This means that our mental model of where our data comes from now consists of *two layers*. This is precisely the conceptualisation that a multilevel model makes explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda9aa6",
   "metadata": {},
   "source": [
    "## The Complete Multilevel Model\n",
    "Given the discussions above about the nature of $\\eta_{ij}$ and $\\mu_{i}$, we can now write our model as\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{ij}        &\\sim \\mathcal{N}(\\mu_{i} + \\alpha_{j}, \\sigma^{2}_{1}) \\\\\n",
    "    \\mu_{i}       &\\sim \\mathcal{N}(\\mu , \\sigma^{2}_{2})                 \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "In the multilevel framework, these equations are considered multiple *layers* or *levels* of the data, and are usually labelled like so \n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    y_{ij}        &\\sim \\mathcal{N}(\\mu_{i} + \\alpha_{j}, \\sigma^{2}_{1}) &\\quad\\text{Level 1} \\\\\n",
    "    \\mu_{i}       &\\sim \\mathcal{N}(\\mu , \\sigma^{2}_{2})                 &\\quad\\text{Level 2} \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "This also implies a *hierarchy* of data-generation, where the data at Level 1 depends upon the data at Level 2. This is why these types of model are also known as *hierarchical* linear models.\n",
    "\n",
    "We can also write this model in a slightly different way. As we know from last semester, we can always separate a probability model into an equation for the *fixed* expected value plus *random* error. For instance, we can write a simple regression model as\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}(\\beta_{0} + \\beta_{1}x_{i}, \\sigma^{2})\n",
    "$$\n",
    "\n",
    "or as\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{i}        &=    \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "    \\epsilon_{i} &\\sim \\mathcal{N}(0, \\sigma^{2})                \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "where, in the second form, we move the probabilistic behaviour into a new error term that captures the variation in $y$ around the expected value. So, if we apply the same principles here, we can rewrite the multilevel model as\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    y_{ij}    &= \\mu_{i} + \\alpha_{j} + \\eta_{ij}             &\\quad\\text{Level 1} \\\\\n",
    "    \\mu_{i}   &= \\mu + \\xi_{i}                                &\\quad\\text{Level 2} \\\\\n",
    "    \\xi_{i}   &\\sim \\mathcal{N}\\left(0,\\sigma^{2}_{1}\\right)  &\\\\\n",
    "    \\eta_{ij} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}_{2}\\right), &\\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "where we have added a new error term $\\xi_{i}$[^xi-foot] at Level 2. When written this way, we can see that actually what we have is two models containing *two error terms*. Each of these error terms captures a different form of *random deviation* and thus a different *source of variance*. We can understand this more by discussing each level in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160602f",
   "metadata": {},
   "source": [
    "### Level 1\n",
    "At Level 1, the model is\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{ij}    &=    \\mu_{i} + \\alpha_{j} + \\eta_{ij}         \\\\\n",
    "    \\eta_{ij} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}_{1}\\right) \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "So, our data are drawn from a normal distribution with a mean that depends upon two parts: $\\mu_{i}$, which is *unique* and *specific* to subject $i$, and $\\alpha_{j}$, which is *constant* and *fixed* across subjects. \n",
    "\n",
    "At this level, we imagine that the effect of the experimental manipulation is the *same* for every subject. This is no different to our usual assumption about *regression slopes* or *group means* or any other phenomena we are trying to estimate. If we had the *whole population* available we would know $\\alpha_{j}$ and our job would be done. The difference now is that we also imagine that each subject is *offset* by an amount unique to them. This implies that the *relative* differences in `score` between the levels of `time` is constant in the population, but that the *absolute* value of `score` depends upon the individual. This captures the idea that a single subject may have an overall *lower* or *higher* value of `score`, because each individual will have their own unique amount of self-esteem. While the `time` manipulation may serve to *increase* or *decrease* self-esteem, someone with *low* self-esteem will still remain *low* even after the effect of `time`. Similarly, someone with *high* self-esteem will still remain *high*, even after the effect of `time`. The measurements from each individual subject are therefore *connected* by the $\\mu_{i}$ term. \n",
    "\n",
    "Importantly, the errors at this level correspond to deviations in the value of `score` from the unique expected value of each subject. For instance, the errors for subject 1 correspond to the deviation $y_{1j} - (\\mu_{1} + \\alpha_{j})$, the errors for subject 2 correspond to the deviations $y_{2j} - (\\mu_{2} + \\alpha_{j})$ and so on. The variance captured by these deviations $\\left(\\sigma^{2}_{1}\\right)$ therefore corresponds to the *internal consistency* of each individual subject. It tells us how much a single subject varies in their value of `score` relative to *their own mean*. So, this is not how much the subjects differ from each other, this is how much each subject differs *from themselves*. As such, we usually call this the *within-subject* variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e908b69",
   "metadata": {},
   "source": [
    "\n",
    "````{admonition} Level 1 Summary\n",
    ":class: info\n",
    "In this model, Level 1 explains each subject as an individual entity. Their measured values of `score` can be decomposed into a term unique to them ($\\mu_{i}$) and a population-level effect of `time` ($\\alpha_{j}$). The errors at this level therefore correspond to deviations in the value of `score` from the unique expected value of each subject. As such, the variance at this level tells us how much the *repeated measurements* differ on average *within* a single subject. \n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc9f75",
   "metadata": {},
   "source": [
    "### Level 2\n",
    "At Level 2, the model is\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    \\mu_{i}  &=    \\mu + \\xi_{i}                             \\\\\n",
    "     \\xi_{i} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}_{2}\\right). \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Here, our outcome variable now consists of the individual *subject means* from Level 1. Because our outcome is always conceived as a *random variable*, this means that $\\mu_{i}$ is also a random variable. This fits with our conceptualisation from Level 1, because each $\\mu_{i}$ was *unique* to each subject. As such, its value will change with every new sample. Level 2 therefore explains where the individual values of $\\mu_{i}$ come from. As written above, we explain each unique value of $\\mu_{i}$ as a combination of a *fixed* population-level mean $(\\mu)$ and random error $(\\xi_{i})$. This is akin to our conceptualisation of a one-sample $t$-test style analysis. Each subject is drawn from an overall population with some constant mean that we want to estimate. The errors at this level therefore correspond to deviations in the subject means from the population mean. As such, the variance at this level $\\left(\\sigma^{2}_{2}\\right)$ corresponds to the *consistency of each subject with the group*. It tells us how much the subjects differ from *each other* and thus captures *between-subjects* variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fdb35",
   "metadata": {},
   "source": [
    "\n",
    "````{admonition} Level 2 Summary\n",
    ":class: info\n",
    "In this model, Level 2 explains the subjects as a group. Each unique subject mean can be decomposed into a group mean $(\\mu)$ plus random error. These errors therefore correspond to deviations in the subject means from the group mean. The variance at this level $\\left(\\sigma^{2}_{2}\\right)$ therefore tells us how much the subjects differ *from each other*.\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59be788",
   "metadata": {},
   "source": [
    "### Multilevel Visualisation\n",
    "To help further conceptualise the multilevel framework, {numref}`multilevel-fig` presents a diagrammatic representation of the model we have been working with so far. To navigate this, start from the *bottom* and work *upwards*. Two sampled datapoints are illustrated as $y_{i1}$ and $y_{i2}$. These are measured values of the repeated measures conditions $j = 1$ and $j = 2$ for subject $i$. These are conceptualised as drawn from individual distributions for that specific subject. The means of these distributions are then a combination of the unique individual subject mean $\\mu_{i}$ and the fixed effects of the two conditions, $\\alpha_{1}$ and $\\alpha_{2}$. The subject mean $\\mu_{i}$ is itself conceptualised as a random drawn from the population-level distribution of subjects, with a fixed mean of $\\mu$. \n",
    "\n",
    "```{figure} images/multilevel-diagram.png\n",
    "---\n",
    "width: 500px\n",
    "name: multilevel-fig\n",
    "---\n",
    "An illustration of a basic two-level model.\n",
    "```\n",
    "\n",
    "There are a few key takeaways from this illustration. Firstly, every term that contains the index $i$ will change with each new subject. These are therefore all *random variables*. If we conceptualised drawing a new subject, think of every term that contains an $i$ changing and every term that does *not* contain an $i$ staying the same. Secondly, notice that the variance terms $\\sigma^{2}_{1}$ and $\\sigma^{2}_{2}$ describe different kinds of variation. $\\sigma^{2}_{1}$ describes variation *between* different subjects, whereas $\\sigma^{2}_{2}$ describes variation *within* a single subject. Finally, we can think of this model as a *generalisation* of everything we have done previously. When our data were independent, we were thinking only in terms of Level 2. So, all the models we were looking at last semester were all representations of Level 2. The addition of repeats for each experimental unit creates the additional level of variation. So, all our previous models were really *single-level* and we can think of them as *special cases* of a multilevel model. From that perspective, the multilevel model is really *the* framework that underlies everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085635a",
   "metadata": {},
   "source": [
    "## Simulating a Multilevel Model in `R`\n",
    "As the final section in this part of the lesson, we will look at *simulating* the model above in `R`. This will serve two purposes. Firstly, it can be useful in understanding the abstract representation above by seeing code that implements it. Secondly, there are some not-so-obvious consequences of the multilevel framework that make it particularly suited for repeated measurements. Although these consequences can just be stated, it will make them more concrete when we can actually *see* them via simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e6e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(666)\n",
    "\n",
    "n.subs <- 50000\n",
    "n.t    <- 3\n",
    "y      <- rep(0,n.subs*n.t)\n",
    "\n",
    "# fixed-effects\n",
    "mu      <- 5\n",
    "alpha   <- c(-2,1,3)\n",
    "\n",
    "# variances\n",
    "sigma2.1 <- 3 # Level 1 (within-subject)\n",
    "sigma2.2 <- 2 # Level 2 (between-subjects)\n",
    "\n",
    "# Level 2\n",
    "mu.i <- rnorm(n.subs, mean=mu, sd=sqrt(sigma2.2))\n",
    "\n",
    "# Level 1\n",
    "idx <- 1\n",
    "for (i in 1:n.subs){\n",
    "  for (j in 1:n.t){\n",
    "    y[idx] <- rnorm(n=1, mean=mu.i[i] + alpha[j], sd=sqrt(sigma2.1))\n",
    "    idx    <- idx + 1\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b29b352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(mu.i, main='Level 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a64eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.t1 <- y[seq(1,n.subs*n.t,3)]\n",
    "y.t2 <- y[seq(2,n.subs*n.t,3)]\n",
    "y.t3 <- y[seq(3,n.subs*n.t,3)]\n",
    "\n",
    "par(mfrow=c(2,2))\n",
    "hist(y.t1, main='Level 1 (Time 1)')\n",
    "hist(y.t2, main='Level 1 (Time 2)')\n",
    "hist(y.t3, main='Level 1 (Time 3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2c847",
   "metadata": {},
   "source": [
    "Now, for the party trick. Let us first combine together the simulated values of `y` for each time point into different columns of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b20b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y <- cbind(y.t1,y.t2,y.t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1afe30",
   "metadata": {},
   "source": [
    "Now, let us calculate the variance-covariance matrix of these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8accaae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         y.t1     y.t2     y.t3\n",
       "y.t1 4.999452 2.009662 1.980636\n",
       "y.t2 2.009662 4.994427 1.994638\n",
       "y.t3 1.980636 1.994638 4.971735"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d163dc",
   "metadata": {},
   "source": [
    "Notice anything familiar here? For starters, the simulated values of `time` are *correlated*. This is not necessarily surprising, except for the fact that *we did not simulate any correlation*. There is no multivariate normal distribution with a given correlation structure anywhere in the code above. So where did the correlation come from? This is actually a *natural consequence* of the algebra of a multilevel model. We do not need to specify correlation because it happens *automatically* as part of the connection between the levels of the model. This feels a bit like a magic trick, but we can see it clearly above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c693ac8",
   "metadata": {},
   "source": [
    "- Multilevel structure creates correlation naturally, without specifying any form of variance-covariance matrix\n",
    "- Diagonal elements are $\\sigma^{2}_{1} + \\sigma^{2}_{2} = 2 + 3 = 5$\n",
    "- Off-diagonal elements are $\\sigma^{2}_{1} = 2$\n",
    "- This structure implies a compound-symmetric covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c92ba8",
   "metadata": {},
   "source": [
    "[^xi-foot]: The Greek letter *xi*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb371c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
