{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca8032b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(emmeans))\n",
    "suppressPackageStartupMessages(library(car))\n",
    "suppressPackageStartupMessages(library(effects))\n",
    "suppressPackageStartupMessages(library(nlme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Generalised Least Squares\n",
    "We will start our journey into the world of mixed-effects models by first examining a *related* approach that we have seen before: Generalised Least Squares (GLS). The reason for doing this is twofold. Firstly, GLS actually provides a simpler solution to many of the issues with the repeated measures ANOVA and thus presents a more logical starting point. Secondly, limitations in the way that GLS does this will provide some motivation for mixed-effects as a more complex, but ultimately more flexible, method of dealing with this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb16c4",
   "metadata": {},
   "source": [
    "## GLS Theory\n",
    "We previously came across GLS in the context of allowing different variances for different groups of data in ANOVA-type models. This was motivated as a way of lifting the assumption of *homogeneity of variance*. However, GLS is actually a much more general technique. To see this, note that the probability model for GLS is\n",
    "\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ can take on *any structure*. In other words, GLS has exactly the same probability model as the normal linear model, except that it allows for a flexible specification of the variance-covariance matrix. In our previous examples, we used GLS to populate the variance-covariance matrix with different variances for each group. For instance, if we had two groups with three subjects each, our GLS model would be\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "y_{21} \\\\\n",
    "y_{31} \\\\\n",
    "y_{12} \\\\\n",
    "y_{22} \\\\\n",
    "y_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "\\sim\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "\\sigma^{2}_{1}  & 0              & 0              & 0              & 0              & 0              \\\\\n",
    "0               & \\sigma^{2}_{1} & 0              & 0              & 0              & 0              \\\\\n",
    "0               & 0              & \\sigma^{2}_{1} & 0              & 0              & 0              \\\\\n",
    "0               & 0              & 0              & \\sigma^{2}_{2} & 0              & 0              \\\\\n",
    "0               & 0              & 0              & 0              & \\sigma^{2}_{2} & 0              \\\\\n",
    "0               & 0              & 0              & 0              & 0              & \\sigma^{2}_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "This was actually a special case of GLS known as *weighted least squares* (WLS)[^weights-foot], where all the off-diagonal elements of $\\boldsymbol{\\Sigma}$ are 0. However, the crucial point is that  we can use GLS to impose differences in *both* the variances *and* the covariances. So while we did not do this previously, we can include *correlation* in the GLS model. Thus, if our general problem with repeated measures is that the variance-covariance structure is not correctly handled by the normal linear model, GLS provides a direct solution. Furthermore, if a core complaint of the repeated measures ANOVA is that the assumed covariance structure is too restrictive, GLS again provides a direct solution. So, on the face of it, GLS directly solves many of the issues we encountered last week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f1f5c",
   "metadata": {},
   "source": [
    "### What Does GLS Do?\n",
    "Technically, the machinery behind GLS is based on assuming we know $\\boldsymbol{\\Sigma}$ *a priori*. Although this would seem a silly place to start (given that we will almost *never* know this), we can go along with it and see where it gets us. So, *if* we know what the true covariance structure is, GLS provides a way of *removing* it from the data[^white-foot]. Once removed, the errors return to $i.i.d.$ and we are back in the world of the normal linear model. This is a very enticing prospect because all the difficulties associated with correlation effectively *disappear* and we can treat the data as a regular collection of independent values. So, although this *removal* procedure happens behind the scenes, we can conceptualise GLS as effectively a *transformation* that treats the covariance structure as *nuisance* and removes it, allowing all the theory from last semester to still apply.\n",
    "\n",
    "In terms of the real world, if we do not known $\\boldsymbol{\\Sigma}$ then we cannot technically use GLS. This would seem a bit of a dead-end. However, it is possible to use a method such as REML to *estimate* $\\boldsymbol{\\Sigma}$ from the data. This is known as *feasible generalised least squares* (FGLS). The question then becomes, how does working with $\\hat{\\boldsymbol{\\Sigma}}$ rather than $\\boldsymbol{\\Sigma}$ change things? This is a much bigger question that goes *beyond* FGLS and extends to *any* methods where the covariance structure is *estimated*, mixed-effects included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86defaa",
   "metadata": {},
   "source": [
    "## Estimating the Covariance Structure\n",
    "If we *do not* know $\\boldsymbol{\\Sigma}$ ahead of time, then the term $\\boldsymbol{\\Sigma}$ is no longer a *fixed constant*. Instead, we have $\\hat{\\boldsymbol{\\Sigma}}$, which is a *random variable*. This introduces an additional layer of uncertainty that causes some major issues. The full story is given in the drop-down below, but the short version is that treating $\\hat{\\boldsymbol{\\Sigma}}$ as an *estimate* means we no longer know what null distribution our test statistic has. In fact, the whole concept of degrees of freedom as a metric that captures this uncertainty suddenly disappear. This means we do not know how the standard errors are distributed, do not know how our test statistic is distributed and have no way of calculating a $p$-value. In short, *our inferential machinery falls apart*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76209ff",
   "metadata": {},
   "source": [
    "`````{admonition} How Estimating $\\boldsymbol{\\Sigma}$ Breaks Inference\n",
    ":class: tip, dropdown\n",
    "\n",
    "To understand why the NHST machinery breaks, we need to go back to some of the information covered last semester on [statistical inference](https://pchn63101-advanced-data-skills.github.io/Inference-Linear-Model/2.estimation-uncertainty.html). Recall that in the normal linear model the uncertainty that comes from estimating $\\sigma^{2}$ affects inference via the *denominator* of the test statistic. If we *know* $\\sigma^{2}$, then our test statistic is a $z$-statistic and is distributed as $z \\sim \\mathcal{N}(0,1)$. However, when $\\sigma^{2}$ is *estimated*, the test statistic is a $t$-statistic and is distributed as $t \\sim \\mathcal{T}(\\nu)$. Here, $\\nu$ is the *degrees of freedom*, which characterises the *uncertainty* in the estimate of $\\sigma^{2}$. This is what controls the *width* of the $t$-distribution, which will approach the standard normal as the sample size increases. In other words, we can think of the degrees of freedom as \"the amount to which the null distribution deviates from a standard normal due to uncertainty in the estimation of $\\sigma^{2}$\". So the degrees of freedom are *key*.\n",
    "\n",
    "Now, the whole reason the $t$-distribution exists is because it can be derived from the structure of the test statistic. Because both the numerator and denominator are *estimates*, they are both random variables with a given sampling distribution. In order to work out the distribution of their ratio, *both* sampling distributions need to be derived. Last semester, we showed that the distribution of the parameter estimates was a known quantity. For a single slope from a typical regression model, we have\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{1} \\sim \\mathcal{N}\\left(\\beta_{1}, \\frac{\\sigma^{2}}{\\sum{(x_{i} - \\bar{x})^{2}}}\\right).\n",
    "$$\n",
    "\n",
    "Importantly, the variance of this distribution depends upon knowing $\\sigma^{2}$, which we do not. If we replace this with an *estimate*, $\\hat{\\sigma}^{2}$, we introduce another layer of uncertainty. In order to characterise this addition layer, we need to know the distribution of $\\hat{\\sigma}^{2}$. We glossed-over this last semester, but under the normal linear model this estimate has the following sampling distribution\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} \\sim \\frac{\\sigma^{2}}{\\nu}\\chi^{2}(\\nu)\n",
    "$$\n",
    "\n",
    "This is a $\\chi^{2}$ distribution with $\\nu$ degrees of freedom that is scaled into the same units as the variance. The thing to notice here is that this distribution is *where degrees of freedom come from*. The reason degrees of freedom exist is because they appear as a parameter that governs the *width* of this sampling distribution. Thus, degrees of freedom directly encode uncertainty around the true value of $\\sigma^{2}$. As the sample size goes up, the degrees of freedom go up and the $\\chi^{2}$ gets *narrower* until it collapses into a single point centred on $\\sigma^{2}$. Because the standard error of the parameter estimate depends upon the estimate of $\\sigma^{2}$, the scaled $\\chi^{2}$ distribution passes this uncertainty on to the $t$-distribution. As such, the $t$-distribution is similarly parameterised by the degrees of freedom. The point where the $\\chi^{2}$ collapses to a single point is *exactly* when the $t$-distribution and the standard normal become *the same*. At that point, uncertainty is effectively 0 and the degrees of freedom are no longer important. \n",
    "\n",
    "Now, getting back to FGLS, what happens to the standard error when $\\sigma^{2}$ is no longer *a single number* and is instead a complex function of different elements of an unstructured variance-covariance matrix? Well, the formula for the standard error remains *correct*, but the distribution can no longer be derived analytically. It ceases to be a consistent object across all models and, in effect, becomes *unknowable*. This means that the scaled $\\chi^{2}$ distribution disappears and, along with it, so too does the concept of degrees of freedom as a means of characterising uncertainty. If we divide our parameter estimate by its standard error it is no longer the ratio of two random variables with known distributions. It is the ratio of a random variable with a known distribution and a random variable *with no known distribution*. This makes the null distribution of this test statistic *also unknown*. And without a known null, there is no way to calculate a $p$-value. So, if we accept that $\\hat{\\boldsymbol{\\Sigma}}$ is an *estimate* then our test statistic has no known null distribution, our ability to calculate $p$-values disappears and the concept of degrees of freedom vanishes. In short, we are stuck.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71267aa",
   "metadata": {},
   "source": [
    "There are generally 3 ways to deal with this situation and we will see that different packages have differing opinions on the best way to go about it. This is true both for FGLS and for mixed-effects models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef70c5e",
   "metadata": {},
   "source": [
    "### 1. Ignore the Problem\n",
    "Our first option is to *ignore* the problem. If we treat our estimate as *exactly* the population value, then we can carry on without any issues. So, if we take $\\hat{\\boldsymbol{\\Sigma}} = \\boldsymbol{\\Sigma}$ then there are no problems any more. In the context of GLS, this means we can remove the covariance structure *perfectly* and the whole problem reduces back to a regular regression model with $i.i.d.$ errors. So, we simply act as if we knew $\\boldsymbol{\\Sigma}$ all along.\n",
    "\n",
    "Although this is *practically* appealing, because all the mess indicated above disappears, it comes with some consequences:\n",
    "\n",
    "- The extra uncertainty from estimating $\\boldsymbol{\\Sigma}$ is simply ignored. This means the model contains no penalty for estimating all the variance and covariance parameters.\n",
    "- This means that standard errors may be too small, test statistics too large and $p$-values overly-optimistic, especially in small samples.\n",
    "- We are pretending that degrees of freedom exist, but they technically do not.\n",
    "\n",
    "As we will see, this what the `gls()` function choses to do by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40910189",
   "metadata": {},
   "source": [
    "### 2. Calculate *Effective* Degrees of Freedom\n",
    "Our second option is to accept that $\\hat{\\boldsymbol{\\Sigma}}$ is an estimate and accept that we need to accommodate this uncertainty somehow. In order to do this, we can create *fictitious* degrees of freedom to allow a $p$-value to be calculated. So, although we fully accept that degrees of freedom no longer exist, what we can do is *find* a null distribution that matches our model and then use the degrees of freedom from that distribution as our metric of uncertainty. For instance, we can use information in the model to approximate the *variance* of the calculated test statistic. If we know that the variance of the $t$-distribution is $\\frac{\\nu}{\\nu - 2}$, then we can use our approximated variance to solve for $\\nu$. This gives us a $t$-distribution with the *correct width* for our calculated test statistic. These fictitious degrees of freedom are known as *effective* degrees of freedom and allow a $p$-value to be calculated.\n",
    "\n",
    "This method is perhaps more appealing than simply pretending there is no problem, though it also comes with some consequences:\n",
    "\n",
    "- We are assuming that the true null distribution only differs from known null distributions (such as the $t$ and $F$) by its width, but not the general shape.\n",
    "- This still remains an *approximation*, though it should behave better in smaller samples when degrees of freedom become more necessary.\n",
    "- Degrees of freedom can become fractional and no longer have a clear theoretical grounding. They are more devices to encode \"tail-heaviness\" within a familiar language of $t$ and $F$ distributions. \n",
    "\n",
    "As we will see, this is what some methods associated with FGLS models use and is often the solution employed in mixed-effects models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9692d",
   "metadata": {},
   "source": [
    "### 3. Produce Results that are *Asymptotically* Correct\n",
    "Finally, our third option is to side-step all the messiness with degrees of freedom entirely. Recall from the normal linear model that the uncertainty that comes from estimating $\\sigma^{2}$ effectively *disappears* once the same size is large enough. This is because $\\hat{\\sigma}^{2} = \\sigma^{2}$, for all practical purposes. Thus, we can treat everything as if $\\sigma^{2}$ is known, because our uncertainty is effectively 0. We saw this in the shape of the $t$-distribution. Once the sample size is big enough, the $t$-distribution *becomes* a standard normal distribution whose width is fixed, rather than adaptive. When this happens, the degrees of freedom disappear. So, whilst we normally work with something like\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{1}}{\\text{SE}(\\hat{\\beta}_{1})} \\sim \\mathcal{T}(\\nu),\n",
    "$$\n",
    "\n",
    "it is not wrong to work with\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{1}}{\\text{SE}(\\hat{\\beta}_{1})} \\sim \\mathcal{N}(0,1).\n",
    "$$\n",
    "\n",
    "The only caveat is that the sample size needs to be *big enough* for the second option to be accurate. However, notice that this second option *does not need degrees of freedom*. We say that this test is *asymptotically correct*, meaning it gets more accurate as $n \\rightarrow \\infty$. All we need to do is make the assumption that we have enough data so that we can effectively treat our estimate of $\\sigma^{2}$ as the *true value*. At that point it becomes a *constant*. So, there is no uncertainty to deal with, no sampling distribution to know, no concept of degrees of freedom and all the messiness disappears.\n",
    "\n",
    "Although this is not necessary with the normal linear model, once we are in the realm of estimating a complex covariance structure this approach becomes more appealing. There is a *statistical purity* to this result because we do not need to pretend degrees of freedom still exist nor invent fictitious degrees of freedom based on the model. However, there are some clear issues here\n",
    "\n",
    "- We need to be comfortable assuming that our $n$ is *large-enough* for this to work, but this is an *unanswerable* question (see box below).\n",
    "- We need to be comfortable with the idea of dismissing uncertainty in the estimation of $\\boldsymbol{\\Sigma}$ as negligible.\n",
    "- Any sense of \"exactness\" is gone.\n",
    "- In small samples, this will result in inference that is *optimistic* (though we can at least be open about this)\n",
    "\n",
    "As we will see, this is also what some methods associated with FGLS and mixed-effects models use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869d667",
   "metadata": {},
   "source": [
    "`````{admonition} How Large is \"Large\"?\n",
    ":class: tip\n",
    "If we want to lean on asymptotic theory and pretend that everything is fine when using FGLS, the obvious question is \"how big does $n$ need to be?\". The problem is that the definition is based on a *limit*, so it says that the approximation gets better and better as $n$ moves towards infinity. For our purpose, $n$ is the *number of subjects*, rather than the total amount of data. So, the answer is not that there is some magic sample size that is suddenly large enough, the answer is that the approximation will get better the larger $n$ becomes. The question then is more about what our tolerance for error is. The point of the asymptotic theory is to say that the error that comes from estimation becomes more negligible as $n$ grows, as does the penalty for estimating $\\boldsymbol{\\Sigma}$ from the data. So, unfortunately, there is *no honest numeric answer to this question*. The way to think about it is as a *degree of comfort*. If you are using FGLS with $n = 5$, you should feel *very uncomfortable*. If you are using $n = 50$, you should probably feel *cautious* and if you have $n > 200$ you should probably be feeling *reasonably comfortable*. As $n$ increases beyond that, you should probable feel perfectly fine about the FGLS assumptions. These are only ballpark figures, but the point is really to think of $n$ as a *continuum of comfort*, rather than as a *threshold*. \n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09afb49",
   "metadata": {},
   "source": [
    "### Covariance Constraints\n",
    "As well as understanding that the very process of estimating $\\boldsymbol{\\Sigma}$ causes problems, we also need to understand that we cannot have free reign to estimate any old covariance structure we like. One of the most important elements to recognise is that some sort of *constraint* is always needed when estimating a variance-covariance matrix. To see this, note that for a repeated measures experiment there are $nt \\times nt$ values in this matrix. The values above and below the diagonal are a mirror image, so the true number of unknown values is $\\frac{nt(nt + 1)}{2}$. For instance, if we had $n = 5$ subjects and $t = 3$ repeated measures, there would be $\\frac{15 \\times 16}{2} = 120$ unique values in the variance-covariance matrix. If we allowed it to be completely unstructured, we would have 120 values to estimate *just* for the covariance structure. Indeed, this is not really possible unless the amount of data we have *exceeds* the number of parameters. So, the data itself imposes a *constraint* on how unstructured the covariance matrix can be.\n",
    "\n",
    "Luckily, for most applications, we not only assume that $\\boldsymbol{\\Sigma}$ has a block-diagonal structure (so most off-diagonal entries are 0), but that many of the off-diagonal elements are actually *identical*. We saw this previously with the repeated measures ANOVA. Even though $\\boldsymbol{\\Sigma}$ may have *hundreds* of values we *could* fill-in, if we assume compound symmetry only within each subject, there are only *two* covariance parameters to be estimated: $\\sigma^{2}_{b}$ and $\\sigma^{2}_{w}$. The whole matrix can then be constructed using those two alone. This is an example of *extreme simplification*, but it does highlight that we generally do not estimate the *whole* variance-covariance matrix. We only estimate *small parts* of it. Indeed, making the covariance matrix more general is often a risky move because of the number of additional parameters needed. The more we estimate from the same data, the greater our uncertainty will become because each element of the covariance-matrix is supported by *less data*. Complexity always comes at a price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4b682",
   "metadata": {},
   "source": [
    "## GLS in `R`\n",
    "We have seen some examples of using the `gls()` function from `nlme` last semester. At that point, we only focused on the use of the `weights=` argument with different variance structures (e.g. `varIdent()`, `varPower()` etc.). However, there is also a `correlation=` argument that similarly takes a number of pre-specified correlation structures. We can use these two arguments together to form a final variance-covariance matrix that consists of correlation and heterogenous variance groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7130e8",
   "metadata": {},
   "source": [
    "### The Paired $t$-test Using GLS\n",
    "We can start with the most simple example of the paired $t$-test using GLS. Importantly, this is an unnecessary step theoretically because the paired $t$-test is a perfectly acceptable technique. When there are only two-repeats there are no arguments about the covariance structure. There can only be a single correlation term. So compound symmetry always works. However, it is useful for us as a *starting point* because it is the simplest example of the problem.\n",
    "\n",
    "In order to specify a correlation structure, we need to pass one of the predefined correlation functions as an argument to `correlation=`. These structures include functions such as `corCompSymm()`, `corSpher()`, `corAR1()` and `corSymm()`[^corfunc-foot]. For this example, we will use `corCompSymm()`, which constructs a compound symmetric structure.\n",
    "\n",
    "In order to use `corCompSymm()`, we need to supply it with a description of how we want it structured in relation to our data. This is done using the `form=` argument, which takes a one-sided formula expressing the structure we want. For this example, we will use `corCompSymm(form= ~1|subject)`. This indicates that we want a constant correlation (`1`) grouped by subject (`|subject`). So, the term on the *right* of `|` is key here. This gives a *grouping factor* such that any observations from the same level will share a constant correlation. Because we have used `subject`, each level represents a *different* subject and thus any observations that come from the same subject will be correlated. This therefore defines our *block-diagonal* covariance structure, where the term on the right of `|` forms the *blocks*. We will see ways to visualise this in order to provide more intuition a little later.\n",
    "\n",
    "Returning to our example, we will use the `mice2` data from `datarium` again, which we have converted to long-format as discussed last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1380ad0e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   time weight\n",
      "1   1 before  187.2\n",
      "2   1  after  429.5\n",
      "3   2 before  194.2\n",
      "4   2  after  404.4\n",
      "5   3 before  231.7\n",
      "6   3  after  405.6\n",
      "7   4 before  200.5\n",
      "8   4  after  397.2\n",
      "9   5 before  201.7\n",
      "10  5  after  377.9\n",
      "11  6 before  235.0\n",
      "12  6  after  445.8\n",
      "13  7 before  208.7\n",
      "14  7  after  408.4\n",
      "15  8 before  172.4\n",
      "16  8  after  337.0\n",
      "17  9 before  184.6\n",
      "18  9  after  414.3\n",
      "19 10 before  189.6\n",
      "20 10  after  380.3\n"
     ]
    }
   ],
   "source": [
    "library('datarium')\n",
    "library('reshape2')\n",
    "data('mice2')\n",
    "\n",
    "# repeats and number of subjects\n",
    "t <- 2\n",
    "n <- dim(mice2)[1]\n",
    "\n",
    "# reshape wide -> long\n",
    "mice2.long <- melt(mice2,                       # wide data frame\n",
    "                   id.vars='id',                # what stays fixed?\n",
    "                   variable.name=\"time\",        # name for the new predictor\n",
    "                   value.name=\"weight\")         # name for the new outcome\n",
    "\n",
    "mice2.long <- mice2.long[order(mice2.long$id),] # order by ID\n",
    "rownames(mice2.long) <- seq(1,n*t)              # fix row names\n",
    "mice2.long$id <- as.factor(mice2.long$id)\n",
    "\n",
    "print(mice2.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb71201",
   "metadata": {},
   "source": [
    "To fit this model using GLS, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd3d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nlme)\n",
    "\n",
    "gls.mod <- gls(weight ~ time, correlation=corCompSymm(form=~1|id), data=mice2.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa569916",
   "metadata": {},
   "source": [
    "where we can see the use of the `correlation=` argument with the `corCompSymm()` function. We could also optionally include a `weights=` argument if we wanted the diagonal elements of the covariance matrix to differ by `time`. This would take the form `weights=varIdent(form= ~1|time)`. However, we will keep this simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df83e3",
   "metadata": {},
   "source": [
    "### Inference Using GLS\n",
    "Although we should check the assumptions of the GLS model, we will leave that to one side given that we covered it last semester. The more pressing issue for us is to discuss *inference* using the GLS model, especially given the problems highlighted earlier. To begin with, we can treat the returned objects from `gls()` just like an object from `lm()` and call `summary()` to examine the model estimates and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f15bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized least squares fit by REML\n",
      "  Model: weight ~ time \n",
      "  Data: mice2.long \n",
      "      AIC      BIC    logLik\n",
      "  177.349 180.9105 -84.67449\n",
      "\n",
      "Correlation Structure: Compound symmetry\n",
      " Formula: ~1 | id \n",
      " Parameter estimate(s):\n",
      "      Rho \n",
      "0.5332493 \n",
      "\n",
      "Coefficients:\n",
      "             Value Std.Error  t-value p-value\n",
      "(Intercept) 200.56  8.081914 24.81591       0\n",
      "timeafter   199.48  7.808574 25.54628       0\n",
      "\n",
      " Correlation: \n",
      "          (Intr)\n",
      "timeafter -0.483\n",
      "\n",
      "Standardized residuals:\n",
      "        Min          Q1         Med          Q3         Max \n",
      "-2.46661859 -0.54818094  0.02112903  0.38482224  1.79048964 \n",
      "\n",
      "Residual standard error: 25.55725 \n",
      "Degrees of freedom: 20 total; 18 residual\n"
     ]
    }
   ],
   "source": [
    "print(summary(gls.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf9479",
   "metadata": {},
   "source": [
    "We can see here all the usual output that matches what `lm()` gives us. Indeed, despite what we mentioned earlier, we still get $t$-statistics and $p$-values (though note that the $p$-values are *rounded* rather than displayed using scientific notation). We also have information on the estimated correlation structure, with the single correlation parameter given by $\\hat{\\rho} = 0.53$. Of most importance is that both the *standard error* and *$t$-value* match what we saw last week from the paired $t$-test. This is evidence enough to show that the correlation *is* being taken into account. Importantly, this is being done within a linear model framework, but *without* the need to subtract the differences *or* to manually partition the errors by including `id` in the model formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8babf",
   "metadata": {},
   "source": [
    "#### The $p$-value Problem\n",
    "Although all the elements highlighted above are positives, there is something important we need to recognise about the reported $p$-values. As mentioned earlier, if we work with the idea that $\\boldsymbol{\\Sigma}$ is *estimated* then the null distribution of the test statistics become unknown and there is no way to calculate a $p$-value. And yet, the `gls()` function *does* produce $p$-values. How? *Because it is acting as if $\\boldsymbol{\\Sigma}$ is known*. In effect, even though we are using FGLS, the `gls()` function ignores this and acts as if we are doing *proper* GLS with a known covariance structure. All the calculations of $t$-statistics, degrees of freedom and $p$-values are based on assuming that $\\boldsymbol{\\Sigma}$ is known and thus we can perfectly remove the covariance structure from the data and end-up back in the world of a normal linear model.\n",
    "\n",
    "The problem with this is that the uncertainty in estimating $\\boldsymbol{\\Sigma}$ is not taken into account. Indeed, we have seen that there is no exact way of quantifying this uncertainty because the concept of degrees of freedom vanishes as soon as the sampling distribution becomes unknown. The `gls()` function is *pretending* that we known $\\boldsymbol{\\Sigma}$ and can return the model to a world where degrees of freedom exist and null distributions are known. The extent to which we can trust the $p$-values therefore depends upon the extent to which we are willing to follow along with this fiction. At best, we need to treat these values as *very rough approximations* and nothing more. This is especially true in *small samples*. As mentioned earlier, as $n$ gets larger the estimate of $\\hat{\\boldsymbol{\\Sigma}}$ gets closer to the true value and the uncertainty largely disappears. So if our sample is *large*, we have less concern with the GLS tests. However, great caution is needed if our sample is *small*.\n",
    "\n",
    "Part of the problem here is that FGLS knows *nothing* about the structure of the data. The very fact that we have repeated measurements from multiple subjects is somewhat lost inside of $\\boldsymbol{\\Sigma}$. All GLS knows is that there *is* correlation, but not *where it came from*. As we will see, mixed-effects models actively embed the structure that causes the data to be correlated. This does not magically get rid of the problem of an unknown sampling distribution under an arbitrary covariance structure, but it does allow for a better approximation of the uncertainty that comes from estimating $\\boldsymbol{\\Sigma}$. So, a mixed-effects model will not fix this situation, but it does give us better options beyond simply pretending that $\\boldsymbol{\\Sigma}$ is known. This is not the main reason for using mixed-effects over GLS, but it is an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f87375",
   "metadata": {},
   "source": [
    "#### Omnibus Tests and Follow-ups\n",
    "Putting aside the inferential issues above, the utility of the GLS framework is that we can simply treat the model in the same way as a result from `lm()`. This means we can compute ANOVA-style tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0769e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Denom. DF: 18 \n",
       "            numDF   F-value p-value\n",
       "(Intercept)     1 1800.9410  <.0001\n",
       "time            1  652.6124  <.0001"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova(gls.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be916e",
   "metadata": {},
   "source": [
    "generate confidence intervals (though these should similarly be treated cautiously),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84664f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               2.5 %   97.5 %\n",
       "(Intercept) 184.7197 216.4003\n",
       "timeafter   184.1755 214.7845"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confint(gls.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6acde",
   "metadata": {},
   "source": [
    "create plots using the `effects` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2e6d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library('effects')\n",
    "plot(effect('time', gls.mod, residuals=TRUE), partial.residuals=list(smooth=FALSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963be8e",
   "metadata": {},
   "source": [
    "and compute follow-up tests using `emmeans`[^emmeans-foot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3275588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " contrast       estimate   SE df t.ratio p.value\n",
      " before - after     -199 7.81 17 -25.546  <.0001\n",
      "\n",
      "Degrees-of-freedom method: df.error \n"
     ]
    }
   ],
   "source": [
    "library(emmeans)\n",
    "emm <- emmeans(gls.mod, pairwise ~ time, mode=\"df.error\")\n",
    "print(emm$contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f4411",
   "metadata": {},
   "source": [
    "So, if we put the inferential issues to one side, we can see that GLS provides a nice alternative to the repeated measures ANOVA because it exists within the linear model framework and thus allows us to use all the methods we have seen previously. Furthermore, we can lift the assumption of compound symmetry and use a variety of different covariance matrices. The main downside is that this comes with a price in terms of the $p$-value not taking the uncertainty in the estimation of the covariance structure into account. This is especially problematic in *small samples*, which means we should really treat the hypothesis tests from a GLS model as only *asymptotically correct*. If we are happy to do so, GLS becomes quite a useful method to have on hand. We will see a few more examples of using GLS with more complex ANOVA models in the associated workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ecb4f",
   "metadata": {},
   "source": [
    "## When Should We Use GLS?\n",
    "GLS is most useful in situations where we know what covariance structure we want to impose on the data and are not as concerned about the model not understanding any form of deeper structure in the data. However, if we do not know the covariance structure and know that the data has a deeper structure that the model should be able to exploit, this is where mixed-effects models come into their own. \n",
    "\n",
    "... The problem is that GLS does not know anything about the *structure* of the data. It has no sense of *subjects* as the experimental unit, nor the idea that the outcome variable is comprised of clusters of values taken from different subjects who might themselves form clusters of values from larger groups (e.g. patients vs controls). All that GLS knows is that there is a correlation structure that we want to remove. Unfortunately, this lack of appreciation for the structure of the data means that GLS cannot use that structure to its advantage. There is no separation of the information available by pooling observations across subjects, or subjects across groups. In effect, GLS is a very *crude* solution to a bigger problem with repeated measurements. Namely, that there is a larger *hierarchical* structure at play that the model should be able to take advantage of. We have seen this in a very general way through small-sample degrees of freedom, but really this is only a *symptom* of a larger problem. As we will come to learn, mixed-effects models are advantageous precisely *because* they embed this structure in the model. This has a number of consequences, not least the fact that correlation between measurements from the same experimental unit are *automatically* embedded in the model. This is not because we tell the model to include correlation, rather it is a *natural consequence* of the structure of the data. As such, mixed-effects models are useful because features such as correlation are a natural part of the modelling framework, precisely because it does take the structure into account in a way that GLS simply cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27232f2",
   "metadata": {},
   "source": [
    "[^weights-foot]: This is why the argument in `gls()` was `weights=`.\n",
    "\n",
    "[^corfunc-foot]: You can look up descriptions of all of these using `?corClasses` at the prompt. \n",
    "\n",
    "[^white-foot]: This is sometimes known as *whitening* the data. This is a term you may come across in the neuroimaging literature, particularly in relation to how fMRI is analysed.\n",
    "\n",
    "[^emmeans-foot]: The `mode=` option has been set to `df.error` so that the reported test matches the table from `summary()`. `emmeans` actually has some better ways of adjusting the degrees of freedom to accommodate the uncertainty in estimating $\\boldsymbol{\\Sigma}$, but this is a complication we will leave to one side for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a60bd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
