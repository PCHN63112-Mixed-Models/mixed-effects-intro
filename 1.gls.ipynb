{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Generalised Least Squares\n",
    "We will start our journey into the world of mixed-effects models by first examining a *related* approach that we have seen before: Generalised Least Squares (GLS). The reason for doing this is twofold. Firstly, GLS actually provides a simpler solution to many of the issues with the repeated measures ANOVA and thus presents a more logical starting point. Secondly, limitations in the way that GLS does this will provide some motivation for mixed-effects as a more complex, but ultimately more flexible, method of dealing with this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb16c4",
   "metadata": {},
   "source": [
    "## GLS Theory\n",
    "We previously came across GLS in the context of allowing different variances for different groups of data in ANOVA-type models. This was motivated as a way of lifting the assumption of *homogeneity of variance*. However, GLS is actually a much more general technique. To see this, note that the probability model for GLS is\n",
    "\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ can take on *any structure*. In other words, GLS has exactly the same probability model as the normal linear model, except that it allows for a flexible specification of the variance-covariance matrix. In our previous examples, we used GLS to populate the variance-covariance matrix with different variances for each group. For instance, if we had two groups with three subjects each, our GLS model would be\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "y_{21} \\\\\n",
    "y_{31} \\\\\n",
    "y_{12} \\\\\n",
    "y_{22} \\\\\n",
    "y_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "\\sim\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{1} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\mu_{2} \\\\\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "\\sigma^{2}_{1}  & 0              & 0              & 0              & 0              & 0              \\\\\n",
    "0               & \\sigma^{2}_{1} & 0              & 0              & 0              & 0              \\\\\n",
    "0               & 0              & \\sigma^{2}_{1} & 0              & 0              & 0              \\\\\n",
    "0               & 0              & 0              & \\sigma^{2}_{2} & 0              & 0              \\\\\n",
    "0               & 0              & 0              & 0              & \\sigma^{2}_{2} & 0              \\\\\n",
    "0               & 0              & 0              & 0              & 0              & \\sigma^{2}_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "This was actually a special case of GLS known as *weighted least squares* (WLS)[^weights-foot], where all the off-diagonal elements of $\\boldsymbol{\\Sigma}$ are 0. However, the crucial point is that  we can use GLS to impose differences in *both* the variances *and* the covariances. So while we did not do this previously, we can include *correlation* in the GLS model. Thus, if our general problem with repeated measures is that the variance-covariance structure is not correctly handled by the normal linear model, GLS provides a direct solution. Furthermore, if a core complaint of the repeated measures ANOVA is that the covariance structure that is assumed is too restrictive, GLS again provides a direct solution. So, on the face of it, GLS directly solves many of the issues we encountered last week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f1f5c",
   "metadata": {},
   "source": [
    "### How Does GLS Work?\n",
    "At its most basic, GLS uses the residuals of an initial model fit to estimate the covariance structure. This is then *removed* from the data. The corrected data is then, in theory, *uncorrelated* with *equal variance* and we can use OLS to estimate the parameters. This can all be achieved within a single estimation framework by using *restricted maximum likelihood* (REML). This will iteratively estimate the variance structure from the residuals and then estimate the parameters after removing the estimated variance structure. This continues until convergence (i.e. you get the same result on each subsequent iteration). So, the easiest way to understand GLS is as a method for *correcting* the data to make it suitable for a normal regression model. \n",
    "\n",
    "This perspective of GLS as a *correction technique* is important because to use GLS we have to \n",
    "    \n",
    "1. Assume we know the covariance structure\n",
    "2. Estimate this structure from the data\n",
    "3. Remove the estimated structure and assume that OLS will then work fine\n",
    "\n",
    "The focus here is very much on modelling the *covariance structure* as a correction technique. As we will see later, mixed-effects models work by modelling the *data structure* and allowing the associated covariance structure to fall-out naturally. This is a change in focus, but also arguably a more beneficial approach because the structure of the data is undeniable, yet the structure of the covariance is an *assumption*. Nevertheless, we have seen that the covariance structure itself *is* the problem for modelling repeated measurements and so we shall continue with this focus for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09afb49",
   "metadata": {},
   "source": [
    "\n",
    "### Covariance Constraints\n",
    "Perhaps one of the most important elements to recognise is that some sort of *constraint* is always needed when estimating a variance-covariance matrix. For a repeated measures experiment there are $nt \\times nt$ values in this matrix. The values above and below the diagonal are a mirror image, so the true number of unknown values is $\\frac{nt(nt + 1)}{2}$. For instance, if we had $n = 5$ subjects and $t = 3$ repeated measures, there would be $\\frac{15 \\times 16}{2} = 120$ unique values in the variance-covariance matrix. If we allowed it to be completely unstructured, we would have 120 values to estimate *just* for the covariance structure. Indeed, this is not really possible unless the amount of data we have *exceeds* the number of parameters. So, the data itself imposes a *constraint* on how unstructured the covariance matrix can be.\n",
    "\n",
    "Luckily, for most applications, we not only assume that $\\boldsymbol{\\Sigma}$ has a block-diagonal structure (so most off-diagonal entries are assumed 0), but that many of the off-diagonal elements are actually *identical*. We saw this previously with the repeated measures ANOVA. Even though $\\boldsymbol{\\Sigma}$ may have *hundreds* of values we *could* fill-in, if we assume compound symmetry only within each subject, there are only *two* covariance parameters to be estimated: $\\sigma^{2}_{b}$ and $\\sigma^{2}_{w}$. The whole matrix can then be constructed using those two alone. This is an example of *extreme simplification*, but it does highlight that we generally do not estimate the *whole* variance-covariance matrix. We only estimate *small parts* of it. Indeed, making the covariance matrix more general is often a risky move because of the number of additional parameters needed. The more we estimate from the same data, the greater our uncertainty becomes. The standard errors will get *larger* because they are supported by *less data*, which will ultimately harm our inference. Complexity always comes with a price, especially working with something like the variance-covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4b682",
   "metadata": {},
   "source": [
    "## GLS in `R`\n",
    "We have seen some examples of using the `gls()` function from `nlme` last semester. At that point, we only focused on the use of the `weights=` argument with different variance structures (e.g. `varIdent()`, `varPower()` etc.). However, there is also a `correlation=` argument that similarly takes a number of pre-specified correlation structures. We can use these two arguments together to form a final variance-covariance matrix that consists of correlation and heterogenous variance groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7130e8",
   "metadata": {},
   "source": [
    "### The Paired $t$-test Using GLS\n",
    "We can start with the most simple example of the paired $t$-test using GLS. Importantly, this is an unnecessary step theoretically because the paired $t$-test is a perfectly acceptable technique. When there are only two-repeats there are no arguments about the covariance structure. There can only be a single correlation term. So compound symmetry always works. However, it is useful for us as a *starting point* because it is the simplest example of the problem.\n",
    "\n",
    "In order to specify a correlation structure, we need to pass one of the predefined correlation functions as an argument to `correlation=`. These structures include functions such as `corCompSymm()`, `corSpher()`, `corAR1()` and `corSymm()`[^corfunc-foot]. For this example, we will use `corCompSymm()`, which constructs a compound symmetric structure.\n",
    "\n",
    "In order to use `corCompSymm()`, we need to supply it with a description of how we want it structured in relation to our data. This is done using the `form=` argument, which takes a one-sided formula expressing the structure we want. For this example, we will use `corCompSymm(form= ~1|subject)`. This indicates that we want a constant correlation (`1`) grouped by subject (`|subject`). So, the term on the *right* of `|` is key here. This gives a *grouping factor* such that any observations from the same level will share a constant correlation. Because we have used `subject`, each level represents a *different* subject and thus any observations that come from the same subject will be correlated. This therefore defines our *block-diagonal* covariance structure, where the term on the right of `|` forms the *blocks*. We will see ways to visualise this in order to provide more intuition a little later.\n",
    "\n",
    "Returning to our example, we will use the `mice2` data from `datarium` again, which we have converted to long-format as discussed last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1380ad0e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   time weight\n",
      "1   1 before  187.2\n",
      "2   1  after  429.5\n",
      "3   2 before  194.2\n",
      "4   2  after  404.4\n",
      "5   3 before  231.7\n",
      "6   3  after  405.6\n",
      "7   4 before  200.5\n",
      "8   4  after  397.2\n",
      "9   5 before  201.7\n",
      "10  5  after  377.9\n",
      "11  6 before  235.0\n",
      "12  6  after  445.8\n",
      "13  7 before  208.7\n",
      "14  7  after  408.4\n",
      "15  8 before  172.4\n",
      "16  8  after  337.0\n",
      "17  9 before  184.6\n",
      "18  9  after  414.3\n",
      "19 10 before  189.6\n",
      "20 10  after  380.3\n"
     ]
    }
   ],
   "source": [
    "library('datarium')\n",
    "library('reshape2')\n",
    "data('mice2')\n",
    "\n",
    "# repeats and number of subjects\n",
    "t <- 2\n",
    "n <- dim(mice2)[1]\n",
    "\n",
    "# reshape wide -> long\n",
    "mice2.long <- melt(mice2,                       # wide data frame\n",
    "                   id.vars='id',                # what stays fixed?\n",
    "                   variable.name=\"time\",        # name for the new predictor\n",
    "                   value.name=\"weight\")         # name for the new outcome\n",
    "\n",
    "mice2.long <- mice2.long[order(mice2.long$id),] # order by ID\n",
    "rownames(mice2.long) <- seq(1,n*t)              # fix row names\n",
    "mice2.long$id <- as.factor(mice2.long$id)\n",
    "\n",
    "print(mice2.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb71201",
   "metadata": {},
   "source": [
    "To fit this model using GLS, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd3d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nlme)\n",
    "\n",
    "gls.mod <- gls(weight ~ time, correlation=corCompSymm(form=~1|id), data=mice2.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa569916",
   "metadata": {},
   "source": [
    "where we can see the use of the `correlation=` argument with the `corCompSymm()` function. We could also optionally include a `weights=` argument if we wanted the diagonal elements of the covariance matrix to differ by `time`. This would take the form `weights=varIdent(form= ~1|time)`. However, we will keep this simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df83e3",
   "metadata": {},
   "source": [
    "### Inference Using GLS\n",
    "Although we should check the assumptions of the GLS model, we will leave that to one side given that we covered it last semester. The more pressing issue for us is to discuss *inference* using the GLS model. To begin with, we can treat the returned objects from `gls()` just like an object from `lm()` and call `summary()` to examine the model estimates and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f15bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized least squares fit by REML\n",
      "  Model: weight ~ time \n",
      "  Data: mice2.long \n",
      "      AIC      BIC    logLik\n",
      "  177.349 180.9105 -84.67449\n",
      "\n",
      "Correlation Structure: Compound symmetry\n",
      " Formula: ~1 | id \n",
      " Parameter estimate(s):\n",
      "      Rho \n",
      "0.5332493 \n",
      "\n",
      "Coefficients:\n",
      "             Value Std.Error  t-value p-value\n",
      "(Intercept) 200.56  8.081914 24.81591       0\n",
      "timeafter   199.48  7.808574 25.54628       0\n",
      "\n",
      " Correlation: \n",
      "          (Intr)\n",
      "timeafter -0.483\n",
      "\n",
      "Standardized residuals:\n",
      "        Min          Q1         Med          Q3         Max \n",
      "-2.46661859 -0.54818094  0.02112903  0.38482224  1.79048964 \n",
      "\n",
      "Residual standard error: 25.55725 \n",
      "Degrees of freedom: 20 total; 18 residual\n"
     ]
    }
   ],
   "source": [
    "print(summary(gls.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf9479",
   "metadata": {},
   "source": [
    "We can see here all the usual output that matches what `lm()` gives us. We also have information on the estimated correlation structure, with the single correlation parameter given by $\\hat{\\rho} = 0.53$. Of most importance is that both the *standard error* and *$t$-value* match what we saw last week from the paired $t$-test. This is evidence enough to show that the correlation *is* being taken into account. Importantly, this is being done within a linear model framework, but *without* the need to subtract the differences *or* to manually partition the errors by including `id` in the model formula. We can also compare this with the output from the paired $t$-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9e91b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tPaired t-test\n",
      "\n",
      "data:  mice2$before and mice2$after\n",
      "t = -25.546, df = 9, p-value = 1.039e-09\n",
      "alternative hypothesis: true mean difference is not equal to 0\n",
      "95 percent confidence interval:\n",
      " -217.1442 -181.8158\n",
      "sample estimates:\n",
      "mean difference \n",
      "        -199.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t.test(mice2$before, mice2$after, paired=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8babf",
   "metadata": {},
   "source": [
    "#### The $p$-value Problem\n",
    "Although all the elements highlighted above are positives, there is a problem here in terms of the calculation of the $p$-value. The issue is that the covariance structure has been *estimated* and this creates additional uncertainty. In the normal linear model, we use the null $t$-distribution for the parameters because the $t$-statistics is formed as the ratio of *two* estimates. This takes into account the uncertainty in the parameter estimate *and* the uncertainty in the standard error[^student-foot]. This only works because the distribution of the parameter estimate is known *and* the distribution of the standard error is known. Unfortunately, when we get into complex covariance matrices built from *multiple* parameters, the distribution of the standard error is *no longer known*. This means we have no idea what the null distribution of the test statistic is anymore. We need the null distribution in order to compute a $p$-value, so when this is unknown what can we do?\n",
    "\n",
    "#### A Tale of Two Worlds\n",
    "In order to understand the solution used by `gls()`, it is useful to think about two hypothetical worlds: one where $\\boldsymbol{\\Sigma}$ is already known and one where $\\boldsymbol{\\Sigma}$ is *unknown* and must be estimated from the data. Of course, we know we live in the second world, but let us see the consequences on the inferential procedures of both these possibilities\n",
    "\n",
    "- World 1: $\\boldsymbol{\\Sigma}$ is known\n",
    "    - The covariance structure can be removed *exactly* from the data. \n",
    "    - This removal renders the errors truly $i.i.d.$ and transforms the model back into a normal regression model\n",
    "    - ... Degrees of freedom exist as an easily countable quantity ...\n",
    "\n",
    "- World 2: $\\boldsymbol{\\Sigma}$ is *unknown*\n",
    "    - $\\hat{\\boldsymbol{\\Sigma}}$ is now a random matrix that will change for each sample\n",
    "    - Because of this, ...\n",
    "    - ... Degrees of freedom completely disappear as a easily countable quantity that expresses independent information.\n",
    "\n",
    "So, as we can see, World 2 is messy and difficult and does not allow for easy calculation of a $p$-value. So what does `gls()` do? It pretends that we are in World 1 instead.\n",
    "\n",
    "Where some of this falls apart is in the disagreement between `gls()` and the repeated measures ANOVA in terms of the degrees of freedom. As indicated above, degrees of freedom only exist if we pretend that $\\boldsymbol{\\Sigma}$ is known. However, by doing that, `gls()` makes no concession to the uncertainty introduced by estimating $\\boldsymbol{\\Sigma}$, because of course we are pretending that we have *not* estimated it. As such, the repeated measures structure of the data that is embedded in $\\boldsymbol{\\Sigma}$ is *ignored*. Under the repeated measures ANOVA, the uncertainty induced by estimating this structure is implicit in the fact that each subject causes a loss in the degrees of freedom.\n",
    "\n",
    "...\n",
    "\n",
    "The only way we can get around this problem is to assume that our estimate of $\\boldsymbol{\\Sigma}$ *is* the true population covariance matrix. Now, of course, this is *not* true. However, if it *were* true then the covariance structure could be fully removed from the data and we would be back in the world of ordinary least squares. At that point, we have paid *no penalty* for estimation (because $\\boldsymbol{\\Sigma}$ is a known constant) and we can treat our model in exactly the same way as before. This makes everything nice and easy and we can just treat the GLS results exactly the same as any other regression model. This is exactly what `gls()` does. So, the $t$-statistics and $p$-values reported in the results table from `gls()` are based on assuming $\\boldsymbol{\\Sigma}$ is known and that we can just use the same inferential machinery as a regular regression model. However, the accuracy of doing this is highly questionable.\n",
    "\n",
    "Beyond the somewhat fanciful nature of this solution, the other issue is that the GLS model does not account for the uncertainty created by estimating the covariance structure. If you pretend that we *know* $\\boldsymbol{\\Sigma}$ exactly, then there is no penalty (because no estimation has taken place). However, in reality, we *have* estimated $\\boldsymbol{\\Sigma}$ and this *should* be accommodated. We can see this in the difference between the degrees of freedom in the GLS model and the paired $t$-test results.  ... In the repeated measures ANOVA, the uncertainty in estimating the covariance structure is implicit in the fact that the subjects appear in the model and we lose degrees of freedom accordingly. In GLS, we *should* be losing degrees of freedom for estimating $\\boldsymbol{\\Sigma}$, but this is being ignored. The structure of the data is terms of repeated measures is *hidden* in the structure of $\\boldsymbol{\\Sigma}$. But because the uncertainty around estimating this is being *ignored*, so too is that structure being ignored. In fact,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f87375",
   "metadata": {},
   "source": [
    "#### Omnibus Tests and Follow-ups\n",
    "... Notice that the `Anova()` function does not return $F$-statistics, it instead returns *asymptotic* $\\chi^{2}$-statistics. This is important because `Anova()` refuses to play the \"pretend degrees of freedom\" game that `gls()` does. Instead, `Anova()` accepts that the concept of residual degrees of freedom no longer exist and does not try to get them back again. Instead, we construct a test statistic where its behaviour is known when $\\boldsymbol{\\Sigma}$ is exact. Of course, $\\boldsymbol{\\Sigma}$ is *not* exact, it is still an estimate. However, if we assume the sample size is large enough then any uncertainty in $\\boldsymbol{\\Sigma}$ becomes negligible, we can treat $\\hat{\\boldsymbol{\\Sigma}} = \\boldsymbol{\\Sigma}$ and we know exactly how the statistic will behave under the null. So this is an *asymptotic* statistic that becomes more accurate the larger the sample size. As the sample gets bigger, $\\hat{\\boldsymbol{\\Sigma}}$ gets closer and closer to the true $\\boldsymbol{\\Sigma}$ and the null distribution becomes known. This approach has a certain *statistical purity* to it because we can ignore all the complication with degrees of freedom and null distributions. The price is that we have to use this very carefully in small samples, *especially* when the $p$-values are very marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82544f20",
   "metadata": {},
   "source": [
    "Follow-up tests can also be performed here using `emmeans` and the same syntax we saw last semester. For instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f4411",
   "metadata": {},
   "source": [
    "So we can see that GLS already provides a nice alternative to the repeated measures ANOVA because it exists within the linear model framework and thus allows us to use all the methods we have already learned previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24919b",
   "metadata": {},
   "source": [
    "## The GLS Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac76d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal variance covariance matrix\n",
       "       [,1]   [,2]\n",
       "[1,] 653.17 348.30\n",
       "[2,] 348.30 653.17\n",
       "  Standard Deviations: 25.557 25.557 "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getVarCov(gls.mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b075d80",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "library(Matrix)\n",
    "\n",
    "gls_marginal_cov <- function(fit) {\n",
    "  n <- nobs(fit)\n",
    "\n",
    "  # Correlation blocks (list) or single matrix if no grouping\n",
    "  cs <- fit$modelStruct$corStruct\n",
    "  if (is.null(cs)) {\n",
    "    # No correlation structure: R = I\n",
    "    Rlist <- list(Matrix::Diagonal(n))\n",
    "    g <- factor(rep(\"all\", n))\n",
    "  } else {\n",
    "    Rlist <- corMatrix(cs)\n",
    "    g <- getGroups(fit)\n",
    "    if (is.null(g)) g <- factor(rep(\"all\", n))\n",
    "  }\n",
    "\n",
    "  # Variance weights (if no varStruct, all 1s)\n",
    "  vs <- fit$modelStruct$varStruct\n",
    "  w <- if (is.null(vs)) rep(1, n) else varWeights(vs)\n",
    "\n",
    "  # Indices per group, aligned to the same order as used in the fit\n",
    "  idx <- split(seq_along(g), g)\n",
    "\n",
    "  sig2 <- fit$sigma^2\n",
    "\n",
    "  # Build block covariance matrices: Sigma_g = sig2 * D^(1/2) R D^(1/2)\n",
    "  # In nlme, varWeights are (typically) inverse-SD-type weights, so Var(e_i) = sig2 / w_i^2.\n",
    "  Sig_blocks <- Map(function(R, ii) {\n",
    "    Dhalf <- Matrix::Diagonal(x = 1 / w[ii])\n",
    "    sig2 * (Dhalf %*% R %*% Dhalf)\n",
    "  }, Rlist, idx)\n",
    "\n",
    "  Matrix::bdiag(Sig_blocks)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33416d2",
   "metadata": {},
   "source": [
    "We just print the first 8 rows to show the structure for the first 4 subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb4de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 x 8 sparse Matrix of class \"dgCMatrix\"\n",
      "                                                                            \n",
      "[1,] 653.1733 348.3042   .        .        .        .        .        .     \n",
      "[2,] 348.3042 653.1733   .        .        .        .        .        .     \n",
      "[3,]   .        .      653.1733 348.3042   .        .        .        .     \n",
      "[4,]   .        .      348.3042 653.1733   .        .        .        .     \n",
      "[5,]   .        .        .        .      653.1733 348.3042   .        .     \n",
      "[6,]   .        .        .        .      348.3042 653.1733   .        .     \n",
      "[7,]   .        .        .        .        .        .      653.1733 348.3042\n",
      "[8,]   .        .        .        .        .        .      348.3042 653.1733\n"
     ]
    }
   ],
   "source": [
    "Sigma <- gls_marginal_cov(gls.mod)\n",
    "print(Sigma[1:8,1:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91600562",
   "metadata": {},
   "source": [
    "A more general approach is to create an *image* of $\\boldsymbol{\\Sigma}$, either as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c55116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcb192",
   "metadata": {},
   "source": [
    "Or subsetted to show the general structure more clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56e95d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image(Sigma[1:20,1:20]) # first 10 subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020cbf7",
   "metadata": {},
   "source": [
    "These visualisations also help to make sense of the syntax used for defining the correlation structure. For instance, if we were to specify `form=~1|cond`, this would imply a constant correlation for each level of the repeated measures condition. What would this look like? Let us see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809a7f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "\u001b[1m\u001b[33mError\u001b[39m:\u001b[22m\n\u001b[33m!\u001b[39m object 'cond' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[33mError\u001b[39m:\u001b[22m\n",
      "\u001b[33m!\u001b[39m object 'cond' not found\n",
      "\u001b[90m    \u001b[39m\n",
      "\u001b[90m 1. \u001b[39m\u001b[1mnlme\u001b[22m::gls(...)\n",
      "\u001b[90m 2. \u001b[39m  \u001b[1mbase\u001b[22m::do.call(model.frame, mfArgs)\n",
      "\u001b[90m 3. \u001b[39m  \u001b[1mstats\u001b[22m (local) `<fn>`(...)\n",
      "\u001b[90m 4. \u001b[39m  stats::model.frame.default(...)\n",
      "\u001b[90m 5. \u001b[39m    \u001b[1mbase\u001b[22m::eval(predvars, data, env)\n",
      "\u001b[90m 6. \u001b[39m      base::eval(predvars, data, env)"
     ]
    }
   ],
   "source": [
    "gls.mod <- gls(y.long ~ cond, correlation=corCompSymm(form=~1|cond))\n",
    "image(gls_marginal_cov((gls.mod)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84410a",
   "metadata": {},
   "source": [
    "The data are now organised by whatever term is on the *right* of `|`, so the first 50 rows represent all the data from condition 1 and the final 50 rows represents all the data from condition 2. So we can see that this structure implies that all the data from condition 1 are correlated across subjects, and all the data from condition 2 are correlated across subjects. This is not particularly sensible or meaningful, but hopefully it makes it clear how the syntax works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f2f50",
   "metadata": {},
   "source": [
    "Notice, however, that GLS is ignoring the structure in the data in terms of the subjects, because it has no knowledge of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e317d",
   "metadata": {},
   "source": [
    "## GLS for ANOVA Models\n",
    "In the above example, we saw the use of GLS as an alternative to the paired $t$-test. A more usual application would be as an alternative to the Repeated Measures ANOVA as a means of side-stepping all the messy specification of different error terms for different tests.\n",
    "\n",
    "### ANOVA Models with More Flexible Variance Structures\n",
    "Perhaps the most useful elements of GLS is that we can use much more flexible variance structures than the simple compound-symmetric structure assumed by RM ANOVA. For instance, we can ask for a completely free within-subject correlations and a different variance for each between-subject group. For instance, specifying `corSymm(form=~1|subject)` and `varIdent(form=~1|group)` gives the most flexible structure that allows all correlations and variances to differ. However, we have to be careful because it is possible that there simply is not enough information in our data to allow this to be estimated, even if we want it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ecb4f",
   "metadata": {},
   "source": [
    "## Why Not Stop at GLS?\n",
    "... The problem is that GLS does not know anything about the *structure* of the data. It has no sense of *subjects* as the experimental unit, nor the idea that the outcome variable is comprised of clusters of values taken from different subjects who might themselves form clusters of values from larger groups (e.g. patients vs controls). All that GLS knows is that there is a correlation structure that we want to remove. Unfortunately, this lack of appreciation for the structure of the data means that GLS cannot use that structure to its advantage. There is no separation of the information available by pooling observations across subjects, or subjects across groups. In effect, GLS is a very *crude* solution to a bigger problem with repeated measurements. Namely, that there is a larger *hierarchical* structure at play that the model should be able to take advantage of. We have seen this in a very general way through small-sample degrees of freedom, but really this is only a *symptom* of a larger problem. As we will come to learn, mixed-effects models are advantageous precisely *because* they embed this structure in the model. This has a number of consequences, not least the fact that correlation between measurements from the same experimental unit are *automatically* embedded in the model. This is not because we tell the model to include correlation, rather it is a *natural consequence* of the structure of the data. As such, mixed-effects models are useful because features such as correlation are a natural part of the modelling framework, precisely because it does take the structure into account in a way that GLS simply cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f85f2",
   "metadata": {},
   "source": [
    "## When Can We Use GLS?\n",
    "In reality, a GLS model is useful if you do not care about the hypothesis tests and just want estimates that accommodate a given correlation structure, or if you are using non-repeated measurement data and want a more flexible between-subjects variance structure. Alternatively, if you are taking many measurements from a *single* subject, GLS can be useful to model just their individual data. For instance, time-series data from one subject as measured using EEG, or eye-tracking, or continuous monitoring of hand movement. In these cases you can end up with thousands of data points and GLS can be used to model it using some suitable correlation structure (e.g. `corAR1(form=~time)`). The estimates could then be used as summary statistics to analyse multiple subjects[^summarystat-foot]. However, as we have seen above, GLS is not necessarily suitable for multiple subjects with repeated measurements because it does not accommodate the blocked structure of the data and thus fails to consider how this changes the number of independant pieces of information. This can be side-stepped by using asymptotic statistics that do not depend upon the concept of degrees of freedom. In large samples, this issue disappears, so if you have a large sample, or are willing to treat the $p$-values cautiously in small samples, GLS is a perfectly legitimate solution to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27232f2",
   "metadata": {},
   "source": [
    "[^summarystat-foot]: This approach is, unsurprisingly, known as a *summary statistics* approach and is typical of how data analysis is handled for fMRI and M/EEG data.\n",
    "\n",
    "[^weights-foot]: This is why the argument in `gls()` was `weights=`.\n",
    "\n",
    "[^corfunc-foot]: You can look up descriptions of all of these using `?corClasses` at the prompt. \n",
    "\n",
    "[^student-foot]: Remember that historically the uncertainty in the standard error was *not* taken into account. Statisticians assumed that they could treat the standard error as a *known constant*, at which point the null distribution is simply a scaled version of the distribution of the numerator. For the normal linear model, this would mean using a *standard normal* distribution (or $z$-distribution) for inference. It was Student's insight that this does not always hold because the uncertainty in the standard error *changes* the distribution, with this change becoming more extreme the smaller the sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a60bd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
