{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# From Multilevel to Mixed-effects\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21408cb",
   "metadata": {},
   "source": [
    "## Why Collapse Our Hierarchy?\n",
    "\n",
    "- The model is always estimated as a single unit, with the levels informing each other\n",
    "- Multilevel implies that we can estimate each level separately, which loses the whole advantage of this framework (this is known as a *summary statistics* approach)\n",
    "- Software is harder to write in a multilevel fashion (though it does exist e.g. MLM, MLwin), whereas a single function call for a single model fits happily inside the usual `R` approach\n",
    "\n",
    "So we can think, very broadly, that the hierarchical perspective is useful *intuitively*, but the mixed-effects perspective is useful *practically*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0daeed9",
   "metadata": {},
   "source": [
    "## From Multilevel to Mixed-effects\n",
    "... So, the key here is recognising that we can collapse the two equations\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{ij}    &= \\mu_{i} + \\alpha_{j} + \\eta_{ij}  \\\\\n",
    "    \\mu_{i}   &= \\mu + S_{i}                       \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "After all, we can see exactly what $\\mu_{i}$ is equal to. So let us replace $\\mu_{i}$ in the first equation with the equality in the second equation. If we do so, we get\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{ij} &= (\\mu + S_{i}) + \\alpha_{j} + \\eta_{ij} \\\\\n",
    "           &= \\mu + \\alpha_{j} + S_{i} + \\eta_{ij}\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    S_{i}     &\\sim \\mathcal{N}\\left(0,\\sigma^{2}_{b}\\right) \\\\\n",
    "    \\eta_{ij} &\\sim \\mathcal{N}\\left(0,\\sigma^{2}_{w}\\right) \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "This is *exactly the partitioned error model we saw last week*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c92ba8",
   "metadata": {},
   "source": [
    "## Mixed-effects Using `nlme`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb3827",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e520704",
   "metadata": {},
   "outputs": [],
   "source": [
    "library('datarium')\n",
    "library('reshape2')\n",
    "\n",
    "data('selfesteem')\n",
    "\n",
    "# repeats and number of subjects\n",
    "t <- 3\n",
    "n <- dim(selfesteem)[1]\n",
    "\n",
    "# reshape wide -> long\n",
    "selfesteem.long <- melt(selfesteem,            # wide data frame\n",
    "                        id.vars='id',          # what stays fixed?\n",
    "                        variable.name=\"time\",  # name for the new predictor\n",
    "                        value.name=\"score\")    # name for the new outcome\n",
    "\n",
    "selfesteem.long           <- selfesteem.long[order(selfesteem.long$id),] # order by ID\n",
    "rownames(selfesteem.long) <- seq(1,n*t)                                  # fix row names\n",
    "selfesteem.long$id        <- as.factor(selfesteem.long$id)               # convert ID to factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a5ff9",
   "metadata": {},
   "source": [
    "## Mixed-effects Using `lme4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c8817",
   "metadata": {},
   "source": [
    "## The Implied Marginal Model\n",
    "... From this perspective, a mixed-effects model is simply GLS, but with a more sophisticated way of constructing the variance-covariance matrix using the random effects. This leads to some important consequences:\n",
    "\n",
    "1. The inferential issues around GLS are *identical* for mixed-effects\n",
    "2. The covariance structure is *more restricted* under mixed-effects, because it is constrained by the random effects themselves "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b8e23",
   "metadata": {},
   "source": [
    "## Advantages of Mixed-effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175f260",
   "metadata": {},
   "source": [
    "### Modelling Data Structure\n",
    "... In our previous discussion, GLS was used as a method to get around the presence of correlation within repeated measurement data. We impose a structure for GLS to remove. What LME models do instead is get us to model the *structure* of the data. We do not worry about dependence because this is accommodated *automatically* as a consequence of the structure. As long as we get the structure right, everything else follows. So this is an entirely different perspective. We are not trying to model correlation, we are trying to get the structure correct. Once we do, everything gets worked out for us, correlation and all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443611a",
   "metadata": {},
   "source": [
    "### Pooling and Shrinkage\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d35b3",
   "metadata": {},
   "source": [
    "### The Bias-Variance Tradeoff\n",
    "... So, from this perspective, LME models provide a solution to this issue via pooling and shrinkage. GLS provides *no pooling* ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465da45d",
   "metadata": {},
   "source": [
    "### Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e3be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
