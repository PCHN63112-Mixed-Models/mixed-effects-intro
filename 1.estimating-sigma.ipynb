{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Estimating Covariance Structures\n",
    "Last week, we saw that we can model repeated measures probabilistically by making using of the *multivariate* normal distribution. Key to this, was the specification of the variance-covariance matrix, which captures the pattern of correlation across the repeated measurements. We also saw that one of the problems with the repeated measures ANOVA was that it assumed a very restrictive covariance structure in the form of *compound symmetry* or, more generally, *sphericity*. This had direct consequences for inference because the covariance structure directly informs the standard errors and thus the denominator of the tests statistics and the $p$-values. So, much of our concern around repeated measures is correctly capturing the structure of the variance-covariance matrix. In an ideal world, what we want is a method that makes *no* assumptions about the covariance structure and just allows it to be estimated from the data. Unfortunately, this desire has consequences for inference. In short, the condition of compound symmetry is imposed for the repeated measures ANOVA *precisely* because that is the one situation where the results simplify back to classical form and inference still works. As soon as we move away from this to *any arbitrary covariance structure*, the inferential machinery *falls apart*. In this part of the lesson, our focus will be on understanding this situation because it has direct consequences for all the methods we will cover going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86defaa",
   "metadata": {},
   "source": [
    "## The Basic Problem\n",
    "Recall that we refer to the varianceâ€“covariance matrix as $\\boldsymbol{\\Sigma}$, as a direct analog of the single variance term $\\sigma^{2}$ from the normal linear model. Conceptually, $\\boldsymbol{\\Sigma}$ plays the same role, describing the scale and structure of the noise in the data. From a mathematical point of view, the natural generalisation of everything we have done so far is therefore to replace $\\sigma^{2}$ with $\\boldsymbol{\\Sigma}$ in the expressions derived for the normal linear model. When $\\boldsymbol{\\Sigma}$ is known, this causes no fundamental difficulties. $\\boldsymbol{\\Sigma}$ behaves like a constant object, introducing no additional randomness, and the underlying theory remains intact. In this sense, moving from a single variance to a full covariance matrix does not, by itself, create any new problems. As we will see later, a known $\\boldsymbol{\\Sigma}$ can be used to *remove* the correlation structure from the data, allowing us to work in a world that is mathematically indistinguishable from the independent case.\n",
    "\n",
    "However, the reality is that we will *almost never* know $\\boldsymbol{\\Sigma}$. As such, we will almost always be in a position where we need to *estimate* it from the data. Once we do that, the term $\\boldsymbol{\\Sigma}$ is no longer a *fixed constant*. Instead, we have $\\hat{\\boldsymbol{\\Sigma}}$, which is a *random variable*. This introduces an additional layer of uncertainty that causes some major issues. We will discuss the full story below, but the short version is that treating $\\hat{\\boldsymbol{\\Sigma}}$ as an *estimate* means we no longer know how the standard errors are distributed, which means we do not know what null distribution the test statistics have and cannot calculate a $p$-value. In short, *all our inferential machinery breaks*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76209ff",
   "metadata": {},
   "source": [
    "## How Estimating $\\boldsymbol{\\Sigma}$ Breaks Inference\n",
    "\n",
    "### Revisiting the Normal Linear Model\n",
    "To understand why the classical inferential machinery breaks, we need to go back to some of the information covered last semester on [statistical inference](https://pchn63101-advanced-data-skills.github.io/Inference-Linear-Model/2.estimation-uncertainty.html). Focusing on the normal linear model, if we know $\\sigma^{2}$ then the standard errors of the parameter estimates are a *constant* quantity. Remember, if the assumptions of the model are met, then the parameter estimates have a known distribution. For example, a single slope from a typical regression model has the distribution\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{1} \\sim \\mathcal{N}\\left(\\beta_{1}, \\frac{\\sigma^{2}}{\\sum{(x_{i} - \\bar{x})^{2}}}\\right).\n",
    "$$\n",
    "\n",
    "Its standard error is then just the square-root of this variance term, which depends upon knowing $x$ (which we always do) and $\\sigma^{2}$. This becomes important when we form a test statistic and want to know its null distribution. In the case where $\\sigma^{2}$ is a *constant* then the standard error is *also constant*. This means that if we calculate \n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{\\beta}_{1}}{\\sqrt{\\text{Var}\\left(\\hat{\\beta}_{1}\\right)}},\n",
    "$$\n",
    "\n",
    "we are just dividing a random variable by a constant. This does not change its distribution, only its *scale*. So, under the null hypothesis that $\\beta_{1} = 0$, the null distribution of $z$ is \n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(0,1).\n",
    "$$\n",
    "\n",
    "This is exactly how inference was conducted until Student came along, with statisticians effectively treating the variance *as if it were known*. So, what happens in the more *realistic* scenario when $\\sigma^{2}$ is *not* known, and must be replaced by its estimate $\\hat{\\sigma}^{2}$? Well, the main change is that the *standard errors* also become *estimates*\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{SE}}\\left(\\hat{\\beta}_{1}\\right) = \\sqrt{\\frac{\\hat{\\sigma}^{2}}{\\sum{(x_{i} - \\bar{x})^{2}}}}, \n",
    "$$\n",
    "\n",
    "which means that our test statistic is no longer a simple *scaling* of $\\hat{\\beta}_{1}$. Instead, it now has the form\n",
    "\n",
    "$$\n",
    "t = \\frac{\\hat{\\beta}_{1}}{\\widehat{\\text{SE}}\\left(\\hat{\\beta}_{1}\\right)}.\n",
    "$$\n",
    "\n",
    "This is a ratio between *two* random variables. Plugging an *estimate* of the standard error in adds an additional layer of uncertainty here, which needs to be accommodated in order to derive the null distribution. With each new sample, the scaling in the *denominator* will change. As such, we need to know the *distribution* of the estimate $\\hat{\\sigma}^{2}$. We glossed-over this last semester, but under the normal linear model the estimate of the variance has the following sampling distribution\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} \\sim \\frac{\\sigma^{2}}{\\nu}\\chi^{2}(\\nu)\n",
    "$$\n",
    "\n",
    "This is a scaled $\\chi^{2}$ distribution with $\\nu$ degrees of freedom, which is probably unfamiliar to you. However, understanding this distribution is *crucial* for understanding (1) where the $t$-distribution comes from, (2) where the concept of *degree of freedom* come from, (3) how degrees of freedom function as a method of capturing uncertainty in our estimate of $\\sigma^{2}$ and (4) why everything collapses once we move from a single variance term to an arbitrary covariance matrix. Because of this, we will stick to this topic for a little bit longer before getting back to the point of this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ceaad4",
   "metadata": {},
   "source": [
    "### Understanding the Scaled $\\chi^{2}$ Distribution\n",
    "To understand the scaled $\\chi^{2}$ distribution, we need to first note that a $\\chi^{2}$ distribution has only a single parameter, *the degrees of freedom*, that controls its *width*. This width directly captures the *uncertainty* in the distribution and thus the *uncertainty* around our estimate of $\\sigma^{2}$. However, because the distribution is only parameterised by *one number*, this means that both its *mean* and its *variance* are derived from that one number. The variance is simply ... This means that we can get the *shape* correct by using a suitable value for the degrees of freedom. However, we cannot also get the *scale* correct. So, $\\chi^{2}(5)$ may well have the right *width* to capture the uncertainty in the variance estimate, but the expected value is then $\\nu = 5$, which would only work when $\\sigma^{2} = 5$ and no other time. So, to make this actually useable, we need to *scale* it into the correct units. If we were to multiply this whole distribution by $\\sigma^{2}$, then the expected value would be $\\sigma^{2} \\times \\nu$, which is obviously $\\nu$-times too big. So if we first *divide* $\\sigma^{2}$ by $\\nu$, then the expected value becomes $\\frac{\\sigma^{2}}{\\nu} \\times \\nu = \\sigma^{2}$. We then get a distribution with the correct *width* (encoded by the *degrees of freedom*) and the correct *units* (formed by scaling the distribution by $\\sigma^{2}/\\nu$). So, the scaling term is a bit of a distraction. The real key element here is that *the degrees of freedom are a direct measure of our uncertainty in the estimate of $\\sigma^{2}$*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79848610",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the scaled chi-square variance estimator:\n",
    "#   s2_hat = (sigma2 / nu) * X,  where X ~ chisq(nu)\n",
    "# so s2_hat is centred on sigma2 and collapses onto sigma2 as nu grows.\n",
    "\n",
    "sigma2 <- 1\n",
    "dfs <- c(5, 50, 500, 5000, 50000, 500000)\n",
    "\n",
    "# Density of s2_hat via change-of-variables:\n",
    "# If S = (sigma2/nu) X, then f_S(s) = (nu/sigma2) * f_X((nu/sigma2)*s)\n",
    "d_scaled_chisq <- function(s, nu, sigma2) {\n",
    "  (nu / sigma2) * dchisq((nu / sigma2) * s, df = nu)\n",
    "}\n",
    "\n",
    "# Choose an x-range that makes the \"collapse\" obvious\n",
    "x <- seq(0, 3, length.out = 2000)\n",
    "\n",
    "# Pre-compute a common y-limit so all plots are comparable\n",
    "ymax <- max(sapply(dfs, function(nu) max(d_scaled_chisq(x, nu, sigma2))))\n",
    "\n",
    "op <- par(mfrow = c(2,3), mar = c(4, 4, 3, 1) + 0.1)\n",
    "on.exit(par(op), add = TRUE)\n",
    "\n",
    "for (nu in dfs) {\n",
    "  y <- d_scaled_chisq(x, nu, sigma2)\n",
    "\n",
    "  plot(x, y, type = \"l\",\n",
    "       xlab = expression(hat(sigma)^2),\n",
    "       ylab = \"Density\",\n",
    "       xlim = c(0,3),\n",
    "       main = bquote(nu == .(nu)))\n",
    "  #     main = bquote(frac(sigma^2,nu) * chi^2 * \"(\" * nu * \")\"))\n",
    "\n",
    "  abline(v = sigma2, lty = 2)          # true variance\n",
    "  rug(c(sigma2), ticksize = 0.03)\n",
    "  #legend(\"topright\",\n",
    "  #       legend = c(bquote(nu == .(nu)), bquote(sigma^2 == .(sigma2))),\n",
    "  #       bty = \"n\")\n",
    "}\n",
    "\n",
    "# If you want to *really* see the collapse for large nu, try narrowing xlim:\n",
    "# e.g., set x <- seq(0.6, 1.4, length.out=2000) and xlim=c(0.6,1.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e96acd",
   "metadata": {},
   "source": [
    "So notice what happens. As our sample size goes up, the degrees of freedom *also* go up. In the world of the scaled $\\chi^{2}$ distribution, this makes the width of the distribution *shrink*. So, when the same size is *small* there is a lot of uncertainty around the true value of $\\sigma^{2}$. When the sample size gets *large*, this uncertainty gets smaller and smaller until it effectively *vanishes*. SO, the degrees of freedom have a *direct* role in parameterising this distribution and quantifying our uncertainty about $\\hat{\\sigma}^{2}$ by controlling this width. Once they get larger enough, the distribution is effectively a single point sat on the true variance. At this point, we *know* $\\sigma^{2}$, for all practical purposes. This is where $\\hat{\\sigma}^{2} = \\sigma^{2}$, the $t$-distribution becomes the standard normal distribution, the degrees of freedom can be taken as *infinite* and therefore *disappear*, because they are no longer needed as a method of quantifying this uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df44b0",
   "metadata": {},
   "source": [
    "Importantly, knowing this distribution means we can show that $E(\\hat{\\sigma}^{2}) = \\sigma^{2}$ and thus the variance estimate (and by extension the standard error estimate) is *unbiased*. This means we can trust these calculations as estimates of the true population values. The other thing to notice here is that this distribution is *where degrees of freedom come from*. The reason degrees of freedom exist is because they appear as a parameter that governs the *width* of this sampling distribution. Thus, degrees of freedom directly encode uncertainty around the true value of $\\sigma^{2}$. As the sample size goes up, the degrees of freedom go up and the $\\chi^{2}$ gets *narrower* until it collapses into a single point centred on $\\sigma^{2}$. Because the standard error of the parameter estimate depends upon the estimate of $\\sigma^{2}$, the scaled $\\chi^{2}$ distribution passes this uncertainty on to the $t$-distribution. As such, the $t$-distribution is similarly parameterised by the degrees of freedom. The point where the $\\chi^{2}$ collapses to a single point is *exactly* when the $t$-distribution and the standard normal become *the same*. At that point, uncertainty is effectively 0 and the degrees of freedom are no longer important. This why most of these problems disappear once we have *a large sample size*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10861a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " the uncertainty that comes from estimating $\\sigma^{2}$ affects inference via the *denominator* of the test statistic. If we *know* $\\sigma^{2}$, then our test statistic is a $z$-statistic and is distributed as $z \\sim \\mathcal{N}(0,1)$. However, when $\\sigma^{2}$ is *estimated*, the test statistic is a $t$-statistic and is distributed as $t \\sim \\mathcal{T}(\\nu)$. Here, $\\nu$ is the *degrees of freedom*, which characterises the *uncertainty* in the estimate of $\\sigma^{2}$. This is what controls the *width* of the $t$-distribution, which will approach the standard normal as the sample size increases. In other words, we can think of the degrees of freedom as \"the amount to which the null distribution deviates from a standard normal due to uncertainty in the estimation of $\\sigma^{2}$\". So the degrees of freedom are *key*.\n",
    "\n",
    "To understand why the classical inferential machinery breaks, we need to go back to some of the information covered last semester on [statistical inference](https://pchn63101-advanced-data-skills.github.io/Inference-Linear-Model/2.estimation-uncertainty.html). Recall that, in the normal linear model, the uncertainty that comes from estimating $\\sigma^{2}$ affects inference via the *denominator* of the test statistic. If we *know* $\\sigma^{2}$, then our test statistic is a $z$-statistic and is distributed as $z \\sim \\mathcal{N}(0,1)$. However, when $\\sigma^{2}$ is *estimated*, the test statistic is a $t$-statistic and is distributed as $t \\sim \\mathcal{T}(\\nu)$. Here, $\\nu$ is the *degrees of freedom*, which characterises the *uncertainty* in the estimate of $\\sigma^{2}$. This is what controls the *width* of the $t$-distribution, which will approach the standard normal as the sample size increases. In other words, we can think of the degrees of freedom as \"the amount to which the null distribution deviates from a standard normal due to uncertainty in the estimation of $\\sigma^{2}$\". So the degrees of freedom are *key*.\n",
    "\n",
    "Now, the whole reason the $t$-distribution exists is because it can be derived from the structure of the test statistic. Because both the numerator and denominator are *estimates*, they are both random variables with a given sampling distribution. In order to work out the distribution of their ratio, *both* sampling distributions need to be derived. Last semester, we showed that the distribution of the parameter estimates was a known quantity. For a single slope from a typical regression model, we have\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{1} \\sim \\mathcal{N}\\left(\\beta_{1}, \\frac{\\sigma^{2}}{\\sum{(x_{i} - \\bar{x})^{2}}}\\right).\n",
    "$$\n",
    "\n",
    "Importantly, the variance of this distribution depends upon knowing $\\sigma^{2}$, which we do not. If we replace this with an *estimate*, $\\hat{\\sigma}^{2}$, we introduce another layer of uncertainty. In order to characterise this addition layer, we need to know the distribution of $\\hat{\\sigma}^{2}$. We glossed-over this last semester, but under the normal linear model this estimate has the following sampling distribution\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} \\sim \\frac{\\sigma^{2}}{\\nu}\\chi^{2}(\\nu)\n",
    "$$\n",
    "\n",
    "This is a $\\chi^{2}$ distribution with $\\nu$ degrees of freedom that is scaled into the same units as the variance. Importantly, knowing this distribution means we can show that $E(\\hat{\\sigma}^{2}) = \\sigma^{2}$ and thus the variance estimate (and by extension the standard error estimate) is *unbiased*. This means we can trust these calculations as estimates of the true population values. The other thing to notice here is that this distribution is *where degrees of freedom come from*. The reason degrees of freedom exist is because they appear as a parameter that governs the *width* of this sampling distribution. Thus, degrees of freedom directly encode uncertainty around the true value of $\\sigma^{2}$. As the sample size goes up, the degrees of freedom go up and the $\\chi^{2}$ gets *narrower* until it collapses into a single point centred on $\\sigma^{2}$. Because the standard error of the parameter estimate depends upon the estimate of $\\sigma^{2}$, the scaled $\\chi^{2}$ distribution passes this uncertainty on to the $t$-distribution. As such, the $t$-distribution is similarly parameterised by the degrees of freedom. The point where the $\\chi^{2}$ collapses to a single point is *exactly* when the $t$-distribution and the standard normal become *the same*. At that point, uncertainty is effectively 0 and the degrees of freedom are no longer important. This why most of these problems disappear once we have *a large sample size*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc80c53",
   "metadata": {},
   "source": [
    "### What Happens When We Use $\\hat{\\boldsymbol{\\Sigma}}$?\n",
    "Now, what happens to the standard error when $\\sigma^{2}$ is no longer *a single number* and is instead a complex function of different elements of an unstructured variance-covariance matrix? Well, the clean algebra disappears and the distribution can no longer be derived analytically. It ceases to be a consistent object across all models and, in effect, becomes *unknowable*. This means that the scaled $\\chi^{2}$ distribution disappears and, along with it, the concept of degrees of freedom. If we divide our parameter estimate by its estimated standard error it is no longer the ratio of two random variables with known distributions. It is the ratio of a random variable with a known distribution and a random variable *with no known distribution*. This makes the null distribution of this test statistic *also unknown*. And without a known null, there is no way to calculate a $p$-value. Without a known distribution, we also do not know *how* biased the standard error estimates are and we cannot calculate an accurate confidence interval. In short, *we are stuck*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0c6ee",
   "metadata": {},
   "source": [
    "## Practical Solutions to this Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71267aa",
   "metadata": {},
   "source": [
    "So, we find ourselves in a difficult spot. What we *want* is a framework where we can have any form of covariance structure to accurately represent the data-generating process. This would allow us to model any type of repeated measures experiment, irrespective of its complexity. However, the inferential devices used by the normal linear model simply *do not allow this*. The emphasis on *knowing* the distribution of the estimates in order to calculate $p$-values and confidence intervals has backed us into a corner. Once the very specific conditions that allow these to be calculated are gone, so is the whole exact inferential machinery. This does demonstrate how *fragile* these methods really are. \n",
    "\n",
    "In terms of applying methods like FGLS in practice, there are generally 4 approaches we can use: ignore the problem, invent degrees of freedom, use results that do not need degrees of freedom, or simulate the null distributions from the model. We will discuss all these below and then see how they are applied across different packages for FGLS results in `R`. Remember though, this is a *general problem* that we will see appear again when we get to mixed-effects. Do not make the mistake of thinking this is GLS-specific or that mixed-effects will solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef70c5e",
   "metadata": {},
   "source": [
    "### 1. Ignore the Problem\n",
    "Our first option is to *ignore* the problem. If we treat our estimate as *exactly* the population value, then we can carry on without any issues. So, if we take $\\hat{\\boldsymbol{\\Sigma}} = \\boldsymbol{\\Sigma}$ then there are no problems any more. In the context of GLS, this means we can remove the covariance structure *perfectly* and the whole problem reduces back to a regular regression model with $i.i.d.$ errors. So, we simply act as if we knew $\\boldsymbol{\\Sigma}$ all along.\n",
    "\n",
    "Although this is *practically* appealing, because all the mess indicated above disappears, it comes with some consequences:\n",
    "\n",
    "- The extra uncertainty from estimating $\\boldsymbol{\\Sigma}$ is simply ignored. This means the model contains no penalty for estimating all the variance and covariance parameters.\n",
    "- This means that standard errors may be too small, test statistics too large and $p$-values overly-optimistic, especially in small samples.\n",
    "- We are pretending that degrees of freedom exist, but they technically do not. Furthermore, because we are pretending that we got $\\boldsymbol{\\Sigma}$ for free, the degrees of freedom have no correction for estimating $\\boldsymbol{\\Sigma}$. As such, they will be *larger* than equivalent repeated measures ANOVA models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40910189",
   "metadata": {},
   "source": [
    "### 2. Calculate *Effective* Degrees of Freedom\n",
    "Our second option is to accept that $\\hat{\\boldsymbol{\\Sigma}}$ is an estimate and accept that we need to accommodate this uncertainty somehow. In order to do this, we can create *fictitious* degrees of freedom to allow a $p$-value to be calculated. So, although we fully accept that degrees of freedom no longer exist, what we can do is *find* a null distribution that matches our model and then use the degrees of freedom from that distribution. For instance, we can use a combination of heuristics and information in the model to approximate the *variance* of the calculated test statistic. If we know that the variance of the $t$-distribution is $\\frac{\\nu}{\\nu - 2}$, then we can use our approximated variance to solve for $\\nu$. This gives us a $t$-distribution with approximately the *correct width* for our calculated test statistic. These fictitious degrees of freedom are known as *effective* degrees of freedom.\n",
    "\n",
    "This method is perhaps more appealing than simply pretending there is no problem because it tried to accommodate small sample adjustments and uncertainty, though it also comes with some consequences:\n",
    "\n",
    "- We are assuming that the true null distribution only differs from known null distributions (such as the $t$ and $F$) by its width, but not the general shape.\n",
    "- This still remains an *approximation*, though it should behave better in smaller samples when degrees of freedom become more necessary.\n",
    "- Degrees of freedom can become fractional and no longer have a clear theoretical grounding. They are more devices to encode \"tail-heaviness\" within the familiar language of $t$ and $F$ distributions. \n",
    "\n",
    "In fact, we already saw an example of this last week in terms of the *non-sphericity corrections*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9692d",
   "metadata": {},
   "source": [
    "### 3. Produce Results that are *Asymptotically* Correct\n",
    "Our third option is to side-step degrees of freedom entirely. Recall from the normal linear model that the uncertainty that comes from estimating $\\sigma^{2}$ effectively *disappears* once the same size is large enough. This is because $\\hat{\\sigma}^{2} = \\sigma^{2}$, for all practical purposes. Thus, we can treat everything as if $\\sigma^{2}$ is known, because our uncertainty is effectively 0. We saw this in the shape of the $t$-distribution. Once the sample size is big enough, the $t$-distribution *becomes* a standard normal distribution whose width is fixed, rather than adaptive. When this happens, the degrees of freedom disappear. So, whilst we normally work with something like\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{1}}{\\text{SE}(\\hat{\\beta}_{1})} \\sim \\mathcal{T}(\\nu),\n",
    "$$\n",
    "\n",
    "it is not wrong to work with\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{1}}{\\text{SE}(\\hat{\\beta}_{1})} \\sim \\mathcal{N}(0,1).\n",
    "$$\n",
    "\n",
    "The only caveat is that the sample size needs to be *big enough* for the second option to be accurate. However, notice that this second option *does not need degrees of freedom*. We say that this test is *asymptotically correct*, meaning it gets more accurate as $n \\rightarrow \\infty$. All we need to do is make the assumption that we have enough data so that we can effectively treat our estimate of $\\sigma^{2}$ as the *true value*. At that point it becomes a *constant*. So, there is no uncertainty to deal with, no sampling distribution to know, no concept of degrees of freedom and all the messiness disappears.\n",
    "\n",
    "Although such asymptotic approaches are not necessary with the normal linear model, once we are in the realm of estimating a complex covariance structure this approach becomes more appealing. There is a *statistical purity* to this result because we do not need to pretend degrees of freedom still exist nor invent fictitious degrees of freedom based on the model. However, there are some clear issues here\n",
    "\n",
    "- We need to be comfortable assuming that our $n$ is *large-enough* for this to work, but this is an *unanswerable* question (see box below).\n",
    "- We need to be comfortable with the idea of dismissing uncertainty in the estimation of $\\boldsymbol{\\Sigma}$ as negligible.\n",
    "- In small samples this will result in inference that is *optimistic*, though the open use of asymptotic tests already embeds this as a caution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869d667",
   "metadata": {},
   "source": [
    "`````{admonition} How Large is \"Large\"?\n",
    ":class: tip\n",
    "If we want to lean on asymptotic theory, the obvious question is \"how big does $n$ need to be?\". The problem is that the definition is based on a *limit*, so it says that the approximation gets better and better as $n$ moves towards infinity. For our purpose, $n$ is the *number of subjects*, rather than the total amount of data. So, the answer is not that there is some magic sample size that is suddenly large enough, the answer is that the approximation will get better the larger $n$ becomes. The question then is more about what our tolerance for error is. The point of the asymptotic theory is to say that the error that comes from estimation becomes more negligible as $n$ grows, as does the penalty for estimating $\\boldsymbol{\\Sigma}$ from the data. So, unfortunately, there is *no honest numeric answer to this question*. The way to think about it is as a *degree of comfort*. If you are using FGLS with $n = 5$, you should feel *very uncomfortable*. If you are using $n = 50$, you should probably feel *cautious* and if you have $n > 200$ you should probably be feeling *reasonably comfortable*. As $n$ increases beyond that, you should probable feel perfectly fine about this approach. These are only ballpark figures, but the point is really to think of $n$ as a *continuum of comfort*, rather than as a *threshold*. \n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba91acf",
   "metadata": {},
   "source": [
    "#### 4. Simulate the Null\n",
    "As a final option, we can leave the world of trying to derive precise results mathematically and instead use the power of the *computer* to find a solution. This gets us into the world of *resampling methods*, which we encountered briefly last semester in the form of the *permutation test*, used when the errors are not normally distributed. For the general problem of deriving a null distribution under an arbitrary covariance structure, the *parametric bootstrap* is most commonly employed. In this method we:\n",
    "\n",
    "1. Treat a fitted null model as the \"truth\".\n",
    "2. Use this fitted model to simulate new data.\n",
    "3. Refit the model to the simulated dataset and save a copy of the test statistic.\n",
    "4. Over many repeats of 2 and 3, build up a *distribution* of the test statistic under the null.\n",
    "5. Calculate the $p$-value and confidence intervals from this distribution.\n",
    "\n",
    "So this requires *zero* theory about the distribution of anything. The uncertainty comes through naturally as part of the simulation and we can get a $p$-value irrespective of the form of $\\boldsymbol{\\Sigma}$. So this has some distinct advantages because we can get rid of much of the difficult approximation needed in classical approaches. However, the tradeoffs are\n",
    "\n",
    "- Computational burden, as calculating a single $p$-value can be a long process depending upon the complexity of refitting the model 1,000 times or more.\n",
    "- Fundamentally, we have to assume that our models is a close approximation to the truth for this to work. This can be seen as quite a *strong* assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09afb49",
   "metadata": {},
   "source": [
    "## Covariance Constraints\n",
    "As well as understanding that the very process of estimating $\\boldsymbol{\\Sigma}$ causes problems, we also need to understand that we cannot have free reign to estimate any old covariance structure we like. One of the most important elements to recognise is that some sort of *constraint* is always needed when estimating a variance-covariance matrix. To see this, note that for a repeated measures experiment there are $nt \\times nt$ values in this matrix. The values above and below the diagonal are a mirror image, so the true number of unknown values is $\\frac{nt(nt + 1)}{2}$. For instance, if we had $n = 5$ subjects and $t = 3$ repeated measures, there would be $\\frac{15 \\times 16}{2} = 120$ unique values in the variance-covariance matrix. If we allowed it to be completely unstructured, we would have 120 values to estimate *just* for the covariance structure. Indeed, this is not really possible unless the amount of data we have *exceeds* the number of parameters. So, the data itself imposes a *constraint* on how unstructured the covariance matrix can be.\n",
    "\n",
    "Luckily, for most applications, we not only assume that $\\boldsymbol{\\Sigma}$ has a block-diagonal structure (so most off-diagonal entries are 0), but that many of the off-diagonal elements are actually *identical*. We saw this previously with the repeated measures ANOVA. Even though $\\boldsymbol{\\Sigma}$ may have *hundreds* of values we *could* fill-in, if we assume compound symmetry only within each subject, there are only *two* covariance parameters to be estimated: $\\sigma^{2}_{b}$ and $\\sigma^{2}_{w}$. The whole matrix can then be constructed using those two alone. This is an example of *extreme simplification*, but it does highlight that we generally do not estimate the *whole* variance-covariance matrix. We only estimate *small parts* of it. Indeed, making the covariance matrix more general is often a risky move because of the number of additional parameters needed. The more we estimate from the same data, the greater our uncertainty will become because each element of the covariance-matrix is supported by *less data*. Complexity always comes at a price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183a5a9",
   "metadata": {},
   "source": [
    "`````{admonition} Where Does This All Leave Us?\n",
    ":class: warning\n",
    "So, where do these problems leave us in terms of leaving the world of very stringent covariance assumptions?\n",
    "\n",
    "... Ultimately, from the pure perspective of a *model* that capture the *data-generating process*, FGLS is an attractive proposition ... Unfortunately, the problems arrive as soon as we get to *inference* due to the fragility of the classic approaches to this problem. However, the reality is that as soon as we leave the world of the normal linear model, we leave the world of precise results and always end up in a world of approximations. This is not just a FGLS problem, this is a *global* problem. So if we ever want to use something more complicated than the normal linear model, we have to accept that precise inference breaks-down and we have to approximate it. The fundamental question simply becomes how *best* to approximate it so we can still reach useful conclusions from our models.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27232f2",
   "metadata": {},
   "source": [
    "[^weights-foot]: This is why the argument in `gls()` was `weights=`.\n",
    "\n",
    "[^corfunc-foot]: You can look up descriptions of all of these using `?corClasses` at the prompt. \n",
    "\n",
    "[^white-foot]: This is sometimes known as *whitening* the data. This is a term you may come across in the neuroimaging literature, particularly in relation to how fMRI is analysed.\n",
    "\n",
    "[^emmeans-foot]: The `mode=` option has been set to `df.error` so that the reported test matches the table from `summary()`. `emmeans` actually has some better ways of adjusting the degrees of freedom to accommodate the uncertainty in estimating $\\boldsymbol{\\Sigma}$, but this is a complication we will leave to one side for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a60bd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
