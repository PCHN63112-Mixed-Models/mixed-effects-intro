
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Estimating Covariance Structures &#8212; An Introduction to Mixed-effects Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/test.css?v=a80109f0" />
    <link rel="stylesheet" type="text/css" href="_static/sphericity_3d_files/rglwidgetClass-1.3.18/rgl.css?v=57907efa" />
    <link rel="stylesheet" type="text/css" href="_static/sphericity_3d_files/htmltools-fill-0.5.8.1/fill.css?v=971cc1da" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1.estimating-sigma';</script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/textures.src.js?v=9482728f"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/shadersrc.src.js?v=6ce05c17"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/buffer.src.js?v=1efca185"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/mouse.src.js?v=96f0b970"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/projection.src.js?v=98871b91"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/axes.src.js?v=3250d244"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/rglTimer.src.js?v=ac1c3151"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/pretty.src.js?v=4f2cffba"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/shaders.src.js?v=bbbdf37d"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/animation.src.js?v=74f9b9e8"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/pieces.src.js?v=280fd571"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/selection.src.js?v=5b47ed7d"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/init.src.js?v=f5bdcbbb"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/subscenes.src.js?v=42cb429d"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/controls.src.js?v=4b3dbe6f"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/draw.src.js?v=49d9cbaa"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/utils.src.js?v=56efe719"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/rglClass.src.js?v=9e593197"></script>
    <script src="_static/sphericity_3d_files/rglWebGL-binding-1.3.18/rglWebGL.js?v=8cd6f6d7"></script>
    <script src="_static/sphericity_3d_files/htmlwidgets-1.6.4/htmlwidgets.js?v=175713be"></script>
    <script src="_static/sphericity_3d_files/CanvasMatrix4-1.3.18/CanvasMatrix.src.js?v=5a2d04be"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Generalised Least Squares" href="1.gls.html" />
    <link rel="prev" title="Introduction" href="0.intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="0.intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="An Introduction to Mixed-effects Models - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="An Introduction to Mixed-effects Models - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0.intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Estimating Covariance Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.gls.html">Generalised Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.lme-framework.html">The Mixed-effects Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="5.lme-R.html">Mixed-effects Models in <code class="docutils literal notranslate"><span class="pre">R</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/PCHN63112-Mixed-Models/mixed-effects-intro" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/PCHN63112-Mixed-Models/mixed-effects-intro/issues/new?title=Issue%20on%20page%20%2F1.estimating-sigma.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1.estimating-sigma.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Estimating Covariance Structures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-basic-problem">The Basic Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-estimating-boldsymbol-sigma-breaks-inference">How Estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> Breaks Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#revisiting-the-normal-linear-model">Revisiting the Normal Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-scaled-chi-2-distribution">Understanding the Scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-origin-of-the-t-distribution">The Origin of the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-when-we-use-hat-boldsymbol-sigma">What Happens When We Use <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span>?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-solutions-to-this-problem">Practical Solutions to this Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-pretend-that-boldsymbol-sigma-is-known">Option 1 - Pretend that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is Known</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#assume-hat-boldsymbol-sigma-boldsymbol-sigma">Assume <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}} = \boldsymbol{\Sigma}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-asymptotic-results">Use <em>Asymptotic</em> Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-simulations">Use Simulations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-acknowledge-that-hat-boldsymbol-sigma-is-an-estimate">Option 2 - Acknowledge that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> is an <em>Estimate</em></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-does-all-this-leave-us">Where Does All This Leave Us?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="estimating-covariance-structures">
<h1>Estimating Covariance Structures<a class="headerlink" href="#estimating-covariance-structures" title="Link to this heading">#</a></h1>
<p>Last week, we saw that we can model repeated measures probabilistically by making using of the <em>multivariate</em> normal distribution. Key to this, was the specification of the variance-covariance matrix, which captures the pattern of correlation across the repeated measurements. We also saw that one of the problems with the repeated measures ANOVA was that it assumed a very restrictive covariance structure in the form of <em>compound symmetry</em> (more generally, <em>sphericity</em>). This had direct consequences for inference because the covariance structure directly informs the standard errors and thus the denominator of the tests statistics, the <span class="math notranslate nohighlight">\(p\)</span>-values and confidence intervals. So, much of our concern around repeated measures is about using a more suitable covariance structure to represent our data-generating process. In an ideal world, what we want is a method that makes <em>no</em> assumptions about the covariance structure and just allows it to be estimated from the data. Unfortunately, this desire has consequences for inference. In short, the condition of compound symmetry is imposed for the repeated measures ANOVA <em>precisely</em> because that is the one situation where the results simplify back to classical form and inference still works. As soon as we move away from this to an <em>arbitrary covariance structure</em>, the inferential machinery <em>falls apart</em>. In this part of the lesson, our focus will be on understanding this situation because it has direct consequences for all the methods we will cover going forward.</p>
<section id="the-basic-problem">
<h2>The Basic Problem<a class="headerlink" href="#the-basic-problem" title="Link to this heading">#</a></h2>
<p>Recall that we refer to the variance–covariance matrix as <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, as a direct analog of the single variance term <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> from the normal linear model. Conceptually, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> plays the same role, describing the scale and structure of the noise in the data. From a mathematical point of view, the natural generalisation of everything we have done so far is to simply replace <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> in the expressions used for the normal linear model. When <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is known, this causes no fundamental difficulties. <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> behaves like a constant object, introducing no additional randomness, and the underlying theory remains intact. In this sense, moving from a single variance to a full covariance matrix does not, by itself, create any new problems. As we will see later, a known <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> can be used to <em>remove</em> the correlation structure from the data, allowing us to work in a world that is mathematically indistinguishable from the independent case.</p>
<p>However, the reality is that we will <em>almost never</em> know <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> <em>a priori</em>. As such, we will almost always be in a position where we need to <em>estimate</em> it from the data. Once we do that, the term <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is no longer a <em>fixed constant</em>. Instead, we have <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span>, which is a <em>random variable</em>. This introduces an additional layer of uncertainty that causes some major issues. We will discuss the full story below, but the short version is that treating <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> as an <em>estimate</em> means we no longer know how the standard errors are distributed, which means we do not know what null distribution the test statistics have and cannot calculate a <span class="math notranslate nohighlight">\(p\)</span>-value. In short, <em>all our inferential machinery breaks</em>.</p>
</section>
<section id="how-estimating-boldsymbol-sigma-breaks-inference">
<h2>How Estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> Breaks Inference<a class="headerlink" href="#how-estimating-boldsymbol-sigma-breaks-inference" title="Link to this heading">#</a></h2>
<p>We are going to spend a bit of time discussing the reasons <em>why</em> having an arbitrary covariance structure causes the classical inference techniques to fall apart. This is a bit of a detour and can get a little complicated. However, stick with it, because by the end you will have a clearer sense of why our <em>desire</em> to have an unstructured covariance matrix does not actually gel with the classical inferential machinery. This is important going forward, because we need to recognise that all the models we will discuss have this problem. Classical inference is <em>fragile</em> and as soon as we step outside a very fixed structure, it will break.</p>
<section id="revisiting-the-normal-linear-model">
<h3>Revisiting the Normal Linear Model<a class="headerlink" href="#revisiting-the-normal-linear-model" title="Link to this heading">#</a></h3>
<p>To start understanding why an arbitrary covariance structure is problematic, we need to go back to some of the information covered last semester on <a class="reference external" href="https://pchn63101-advanced-data-skills.github.io/Inference-Linear-Model/2.estimation-uncertainty.html">Statistical Inference</a>. Focusing on the normal linear model, we start from the position of assuming we know what <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is. If this is true, then the standard errors of the parameter estimates are a <em>constant</em> quantity. If the assumptions of the model are met, then the parameter estimates have a known distribution. For example, a single slope from a typical regression model has the distribution</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_{1} \sim \mathcal{N}\left(\beta_{1}, \frac{\sigma^{2}}{\sum{(x_{i} - \bar{x})^{2}}}\right).
\]</div>
<p>The standard error is just the square-root of this variance term</p>
<div class="math notranslate nohighlight">
\[
\text{SE}\left(\hat{\beta}_{1}\right) = \sqrt{\frac{\sigma^{2}}{\sum{(x_{i} - \bar{x})^{2}}}}
\]</div>
<p>which depends upon knowing <span class="math notranslate nohighlight">\(x\)</span> (the associated predictor variable) and <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>. This becomes important when we form a test statistic and want to know its null distribution. In the case where <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is a <em>constant</em> then the standard error is <em>also constant</em>. This means that if we calculate</p>
<div class="math notranslate nohighlight">
\[
z = \frac{\hat{\beta}_{1}}{\text{SE}\left(\hat{\beta}_{1}\right)},
\]</div>
<p>we are just dividing a random variable by a constant. This does not change its distribution, only its <em>scale</em>. So, under the null hypothesis that <span class="math notranslate nohighlight">\(\beta_{1} = 0\)</span>, the null distribution of <span class="math notranslate nohighlight">\(z\)</span> is</p>
<div class="math notranslate nohighlight">
\[
z \sim \mathcal{N}(0,1).
\]</div>
<p>This is exactly how inference was conducted before Student came along, with statisticians effectively treating the variance <em>as if it were known</em>.</p>
<p>So, what happens in the more <em>realistic</em> scenario when <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is <em>not</em> known, and must be replaced by its estimate <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>? Well, the main change is that the <em>standard errors</em> also become <em>estimates</em></p>
<div class="math notranslate nohighlight">
\[
\widehat{\text{SE}}\left(\hat{\beta}_{1}\right) = \sqrt{\frac{\hat{\sigma}^{2}}{\sum{(x_{i} - \bar{x})^{2}}}}, 
\]</div>
<p>which means that our test statistic is no longer a simple <em>scaling</em> of <span class="math notranslate nohighlight">\(\hat{\beta}_{1}\)</span>. Instead, it now has the form</p>
<div class="math notranslate nohighlight">
\[
t = \frac{\hat{\beta}_{1}}{\widehat{\text{SE}}\left(\hat{\beta}_{1}\right)}.
\]</div>
<p>This is a ratio between <em>two</em> random variables. Using an <em>estimate</em> of the standard error adds an additional layer of uncertainty, because now both the <em>numerator</em> and <em>denominator</em> will change with each new sample. This additional uncertainty needs to be accommodated. In order to do this, we need to know the <em>sampling distribution</em> of <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>.</p>
<p>We glossed-over this last semester, but under the normal linear model the variance estimate has the following sampling distribution</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^{2} \sim \frac{\sigma^{2}}{\nu}\chi^{2}(\nu)
\]</div>
<p>This is a scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom. This is probably unfamiliar to you, but understanding this distribution is <em>crucial</em> for understanding several key ideas</p>
<ol class="arabic simple">
<li><p>Where the <span class="math notranslate nohighlight">\(t\)</span>-distribution comes from</p></li>
<li><p>Where the concept of <em>degree of freedom</em> come from</p></li>
<li><p>How degrees of freedom function as a method of quantifying uncertainty</p></li>
<li><p>Why everything collapses once we move to an arbitrary covariance matrix.</p></li>
</ol>
<p>Because of this, we will stick to this topic for a little bit longer before getting back to the point of this section.</p>
</section>
<section id="understanding-the-scaled-chi-2-distribution">
<h3>Understanding the Scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> Distribution<a class="headerlink" href="#understanding-the-scaled-chi-2-distribution" title="Link to this heading">#</a></h3>
<p>In order to understand the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution, we need to first understand the <em>regular</em> <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution. This is written, for some random variable <span class="math notranslate nohighlight">\(Y\)</span>, as</p>
<div class="math notranslate nohighlight">
\[
Y \sim \chi^{2}(\nu).
\]</div>
<p>Notice here that this distribution only has a <em>single</em> parameter, <span class="math notranslate nohighlight">\(\nu\)</span>, called the <em>degrees of freedom</em>. This terminology arises because the <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> is the distribution of the sum-of-squares calculated from <span class="math notranslate nohighlight">\(\nu\)</span> standard normal variates<a class="footnote-reference brackets" href="#chisq-foot" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. So <span class="math notranslate nohighlight">\(\nu\)</span> is literally the count of the number of random variables that form this sum. What this means is that both the <em>mean</em> and <em>variance</em> are functions of <span class="math notranslate nohighlight">\(\nu\)</span>. So, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{alignat*}{1}
    E(Y)          &amp;= \nu \\
    \text{Var}(Y) &amp;= 2\nu.
\end{alignat*}
\end{split}\]</div>
<p>Some example <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distributions for different values of <span class="math notranslate nohighlight">\(\nu\)</span> are shown below</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/c22c85090fc345dddf07daf09f4c0ff254ba8ddc198be979fca3507326b17e96.png" src="_images/c22c85090fc345dddf07daf09f4c0ff254ba8ddc198be979fca3507326b17e96.png" />
</div>
</div>
<p>Notice that <span class="math notranslate nohighlight">\(\nu\)</span> controls the <em>width</em> of the distribution. Thus, the <em>uncertainty</em> around the value of <span class="math notranslate nohighlight">\(Y\)</span> can be quantified using <span class="math notranslate nohighlight">\(\nu\)</span>. This is very important for understanding <em>where</em> degrees of freedom come from.</p>
<p>However, because <span class="math notranslate nohighlight">\(\nu\)</span> also controls the <em>mean</em>, we have a slight problem. As a sampling distribution for <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>, we can get the <em>shape</em> correct by using a suitable value for <span class="math notranslate nohighlight">\(\nu\)</span>. However, this will not necessarily be <em>scaled</em> correctly. For instance, <span class="math notranslate nohighlight">\(\chi^{2}(5)\)</span> may well have the right <em>width</em> to capture the uncertainty in <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>, but the expected value is then <span class="math notranslate nohighlight">\(\nu = 5\)</span>. This would only work with data on a scale where <span class="math notranslate nohighlight">\(\sigma^{2} = 5\)</span>. So, the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span> <em>cannot</em> be a plain old <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> distribution, because the units would be wrong. Therefore, the actual sampling distribution must be a <em>scaled</em> version of the <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span>.</p>
<p>To see how this scaling works, notice that if we were to multiply the whole distribution by <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, then the expected value would be <span class="math notranslate nohighlight">\(\sigma^{2} \times \nu\)</span>. This is obviously <span class="math notranslate nohighlight">\(\nu\)</span>-times too big. So if we first <em>divide</em> <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> by <span class="math notranslate nohighlight">\(\nu\)</span>, then the expected value becomes <span class="math notranslate nohighlight">\(\frac{\sigma^{2}}{\nu} \times \nu = \sigma^{2}\)</span>. We then end up with a distribution with the correct <em>width</em> (encoded by the <em>degrees of freedom</em>) and the correct <em>units</em> (by scaling the distribution by <span class="math notranslate nohighlight">\(\sigma^{2}/\nu\)</span>). So, the scaling term is a little bit of a distraction here. The key element is that <em>the degrees of freedom are a direct measure of our uncertainty around the estimate of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span></em>.</p>
<p>As <span class="math notranslate nohighlight">\(\nu\)</span> is directly tied to the <em>sample size</em> (because <span class="math notranslate nohighlight">\(\nu = n - p\)</span>), as <span class="math notranslate nohighlight">\(n\)</span> increases, so too does <span class="math notranslate nohighlight">\(\nu\)</span>. So, let us have a look and see what happens to the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> as we increase <span class="math notranslate nohighlight">\(n\)</span> and thus increase <span class="math notranslate nohighlight">\(\nu\)</span>. Below are 6 plots demonstrating what happens to the <em>shape</em> of scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution as the sample size increases.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/e711b8f858f07b11fef21c1bba900f5490df3b32355dd6b71198098c71f565da.png" src="_images/e711b8f858f07b11fef21c1bba900f5490df3b32355dd6b71198098c71f565da.png" />
</div>
</div>
<p>So, notice what happens. As the degrees of freedom go up, the width of the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution <em>shrinks</em>. When the sample size is <em>small</em>, there is a lot of uncertainty around the true value of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>. As the sample size gets <em>larger</em>, this uncertainty gets smaller and smaller until it effectively <em>vanishes</em>. So, the degrees of freedom exist <em>precisely</em> because they are the parameter of the sampling distribution that quantified our uncertainty about <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>. Once they get larger enough, the distribution is effectively a single point sat on the true variance and, for all practical purposes, we <em>know</em> <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>.</p>
</section>
<section id="the-origin-of-the-t-distribution">
<h3>The Origin of the <span class="math notranslate nohighlight">\(t\)</span>-distribution<a class="headerlink" href="#the-origin-of-the-t-distribution" title="Link to this heading">#</a></h3>
<p>So, how does this behaviour of the scaled <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> connect to the null distribution of the test statistic? Well, we know that if you divide a normal random variate by a <em>constant</em> you just get another normal random variate on a different scale. Hence, when <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is known, we get a <span class="math notranslate nohighlight">\(z\)</span>-statistic with a null distribution that is a standard normal. So what happens when you instead divide a normal random variate and a scaled <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> random variate? You get a new random variable whose distribution changes <em>dynamically</em> depending upon the uncertainty in the denominator. This is a <span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom.</p>
<p>So, the definition of the <span class="math notranslate nohighlight">\(t\)</span>-distribution is <em>exactly</em> the ratio between these two types of random variable. <em>This is where the <span class="math notranslate nohighlight">\(t\)</span>-distribution comes from</em>. The uncertainty in the denominator is passed <em>directly</em> through to the <span class="math notranslate nohighlight">\(t\)</span>-distribution via the degrees of freedom. We do not have to take the word of the theory to see this, as we can simulate this situation in <code class="docutils literal notranslate"><span class="pre">R</span></code>. The code below simulates values from a normal distribution and a scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution. The simulated values are shown in the histograms, with the theoretical distribution curves overlaid. We then divide these two random variables and plot the histogram of the result. As we can see, the theoretical curve from the <span class="math notranslate nohighlight">\(t\)</span>-distribution fits <em>exactly</em>, showing that the theory is indeed correct.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">555</span><span class="p">)</span>

<span class="n">sigma</span><span class="w">   </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">3</span><span class="w">               </span><span class="c1"># population standard deviation</span>
<span class="n">sigma2</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sigma</span><span class="o">^</span><span class="m">2</span><span class="w">         </span><span class="c1"># population variance</span>
<span class="n">n</span><span class="w">       </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">20</span><span class="w">              </span><span class="c1"># sample size</span>
<span class="n">nu</span><span class="w">      </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="w">           </span><span class="c1"># degrees of freedom</span>
<span class="n">n.sim</span><span class="w">   </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2e5</span><span class="w">             </span><span class="c1"># number of simulations</span>
<span class="n">std.err</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sigma</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="c1"># true standard error </span>

<span class="c1"># Numerator: sampling distribution of the mean (assuming the null is TRUE)</span>
<span class="n">numerator</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n.sim</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="n">std.err</span><span class="p">)</span>

<span class="c1"># Denominator: sampling distribution of the variance</span>
<span class="n">denominator</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sigma2</span><span class="o">/</span><span class="n">nu</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">rchisq</span><span class="p">(</span><span class="n">n.sim</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span><span class="w"> </span><span class="c1"># scaled chi^2</span>

<span class="c1"># t-statistic</span>
<span class="bp">T</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">numerator</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="n">denominator</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>

<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">mar</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">1</span><span class="p">))</span>

<span class="c1"># 1. Sampling distribution of the numerator</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span>
<span class="w">     </span><span class="n">main</span><span class="o">=</span><span class="s">&#39;Numerator ~ Normal Random Variate&#39;</span><span class="p">,</span>
<span class="w">     </span><span class="n">xlab</span><span class="o">=</span><span class="s">&#39;&#39;</span><span class="p">)</span>
<span class="nf">curve</span><span class="p">(</span><span class="nf">dnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="o">/</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)),</span><span class="w"> </span><span class="n">add</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">)</span>

<span class="c1"># 2. Sample distribution of the denominator</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">denominator</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span>
<span class="w">     </span><span class="n">main</span><span class="o">=</span><span class="s">&#39;Denominator ~ Scaled Chi-square Random Variate&#39;</span><span class="p">,</span>
<span class="w">     </span><span class="n">xlab</span><span class="o">=</span><span class="s">&#39;&#39;</span><span class="p">)</span>
<span class="nf">curve</span><span class="p">((</span><span class="n">nu</span><span class="o">/</span><span class="n">sigma2</span><span class="p">)</span><span class="o">*</span><span class="nf">dchisq</span><span class="p">((</span><span class="n">nu</span><span class="o">/</span><span class="n">sigma2</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">nu</span><span class="p">),</span><span class="w"> </span><span class="n">add</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>

<span class="c1"># 4. Resulting t-statistic</span>
<span class="nf">hist</span><span class="p">(</span><span class="bp">T</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span>
<span class="w">     </span><span class="n">main</span><span class="o">=</span><span class="s">&#39;Numerator/Denominator ~ T Random Variate&#39;</span><span class="p">,</span>
<span class="w">     </span><span class="n">xlab</span><span class="o">=</span><span class="s">&#39;&#39;</span><span class="p">)</span>
<span class="nf">curve</span><span class="p">(</span><span class="nf">dt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">df</span><span class="o">=</span><span class="n">nu</span><span class="p">),</span><span class="w"> </span><span class="n">add</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">)</span>

<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/74a76e459cc978a1439bda2f7fd7fe0aed64ee85e860b4ead5e3e7b2dbb75cb4.png" src="_images/74a76e459cc978a1439bda2f7fd7fe0aed64ee85e860b4ead5e3e7b2dbb75cb4.png" />
</div>
</div>
<p>We can also tie the behaviour of the <span class="math notranslate nohighlight">\(t\)</span>-distribution to the behaviour of the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> we saw above. As the sample size goes <em>up</em>, <span class="math notranslate nohighlight">\(\nu\)</span> also goes <em>up</em> and the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> collapsed to a single point. As <span class="math notranslate nohighlight">\(\nu\)</span> approaches <em>infinity</em>, we effectively have <span class="math notranslate nohighlight">\(\hat{\sigma}^{2} = \sigma^{2}\)</span> and the <span class="math notranslate nohighlight">\(t\)</span>-statistic is dividing a random variable by a <em>constant</em>. At this point, the <span class="math notranslate nohighlight">\(t\)</span>-distribution becomes the standard normal distribution. So, once the degrees of freedom get large enough to be taken as <em>effectively infinite</em>, they basically <em>disappear</em> because they are no longer relevant as a method of quantifying uncertainty. This is because our uncertainty has collapsed to <em>nothing</em> and we are back to our original position of treating <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> as a constant and using <span class="math notranslate nohighlight">\(z \sim \mathcal{N}(0,1)\)</span> for inference. This is why most of these problems disappear with large samples and why we can effectively <em>ignore</em> the whole idea of degrees of freedom once <span class="math notranslate nohighlight">\(n\)</span> is big enough. This idea is important for understanding some of the “solutions” we will discuss further below.</p>
</section>
<section id="what-happens-when-we-use-hat-boldsymbol-sigma">
<h3>What Happens When We Use <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span>?<a class="headerlink" href="#what-happens-when-we-use-hat-boldsymbol-sigma" title="Link to this heading">#</a></h3>
<p>We have now established what happens when we replace the unknown variance <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> with an <em>estimate</em> within the normal linear model. Crucially, this relies on knowing the sampling distribution of the estimate which, under the normal linear model, is scaled <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> distribution.​ This exact result is what allows us to derive the null distribution of the test statistic. Because this null distribution underpins both confidence intervals and <span class="math notranslate nohighlight">\(p\)</span>-values, having this distributional information is essential.</p>
<p>What changes when we move to a model with a non-trivial variance–covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, rather than a single variance parameter <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>? The standard errors are no longer functions of a single variance term. Instead, they are complicated functions of multiple elements of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. In the normal linear model, the distribution of the test statistic depended on the fact that the variance estimator had a known scaled scaled <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> distribution. Once the variance enters through an estimated covariance matrix, that structure disappears. The denominator of the test statistic is now a <em>complicated function of several estimated variance components</em>. So, the natural question is therefore: <em>what is the sampling distribution of a test statistic whose denominator is a complicated function of several estimated variance components?</em>.</p>
<p>Unfortunately, this question has <em>no answer</em>. And without answer, we have no way of calculating <span class="math notranslate nohighlight">\(p\)</span>-values or confidence intervals. We are, in effect, <em>stuck</em>.</p>
<div class="info dropdown admonition">
<p class="admonition-title">Why Does This Question Have No Answer?</p>
<p>If you want to understand in more detail <em>why</em> the sampling distribution is <em>unknown</em>, the simplest answer is that when there are complex dependencies in the data, all of the clean algebra of the normal linear model disappears. The consequence is that the distribution of the test statistic can no longer be derived analytically. As noted above, the <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> distribution arises from summing independent squared normal variates. Under the normal linear model the errors are independent, and so the variance estimate fits exactly into the definition of a scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution.</p>
<p>Methods that allow for more complex covariance structures can be conceptualised as <em>removing</em> the correlational structure from the data, returning us to the world of the normal linear model. If we knew <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> in advance, this removal would be perfect: the transformed errors would be independent, and the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> construction would apply exactly.</p>
<p>In practice, however, we only ever have an <em>estimate</em> of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. This removal is therefore <em>imperfect</em>. The transformed errors are no longer guaranteed to be <em>independent</em>, and the form used to estimate variance is no longer a sum of independent squared normals. The defining conditions for a <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> distribution are lost.</p>
<p>One might hope that the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> result would still hold <em>approximately</em>. The problem is that “approximately” now means something different for every sample. Each dataset will be closer to or further from the true <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> in its own idiosyncratic way, and this feeds directly into the behaviour of the variance estimate. There is therefore no longer a single, universal <span class="math notranslate nohighlight">\(\chi^{2}(\nu)\)</span> distribution that governs behaviour across repeated samples. Instead, what remains is a <em>moving target</em>: the distribution of the test statistic is conditional on the particular realised estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>Remember, the whole point of a <em>sampling distribution</em> is to describe the behaviour of an estimate across hypothetical repeats of the <em>same</em> experiment. It is this fixed reference distribution that allows us to make probability statements about whether our estimate is <em>expected</em> or <em>unexpected</em>, and to quantify its uncertainty through standard errors and confidence intervals. If the distribution itself <em>changes</em> with each new sample, then it can no longer serve this role. There is no longer any structure that is invariant across repeats. The distribution becomes data-dependent. It warps from sample to sample according to how that particular dataset happened to estimate <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>In that situation, probability statements cease to have their usual meaning. Any distribution we can write down applies only to <em>this</em> realised dataset, not to the hypothetical repetitions that define Frequentist inference. We are no longer reasoning about “what would happen if we repeated the experiment”, but only about the specifics of the single sample in front of us. Once that fixed cross-sample reference point disappears, the inferential machinery breaks. The very idea of a sampling distribution is lost, and with it the foundation on which classical inductive inference is built.</p>
</div>
</section>
</section>
<section id="practical-solutions-to-this-problem">
<h2>Practical Solutions to this Problem<a class="headerlink" href="#practical-solutions-to-this-problem" title="Link to this heading">#</a></h2>
<p>So, we find ourselves in a difficult spot. What we <em>want</em> is a framework where we can have any form of covariance structure to accurately represent the data-generating process. This would allow us to model any type of repeated measures experiment, irrespective of its complexity. However, the inferential devices used by the normal linear model simply <em>do not allow this</em>. The emphasis on <em>knowing</em> the sampling distribution of the estimates in order to calculate <span class="math notranslate nohighlight">\(p\)</span>-values and confidence intervals has backed us into a corner. Once the very specific conditions that allow these to be calculated are gone, so too is the whole inferential machinery. What this really demonstrates is how <em>fragile</em> these methods are.</p>
<p>So, we have two options available to us. One is to simply give up and spend our whole lives restricting inference to only those datasets where the classical results to still apply. The other is that we try and find a way forward, acknowledging that <em>no</em> perfect solution is going to exist.</p>
<p>Clearly, our plan is to push forward with the <em>second option</em>, but it is important to understand from the very beginning that we are making a <em>comprise</em>. To understand why, we need to separate the idea of <em>model building</em> from <em>inference</em>. As a means of developing a description of the <em>data-generating process</em>, methods that allow for a more general covariance structure are much more applicable than those with very restrictive definitions. As such, we can develop much better <em>models</em> that describe <em>where the data came from</em> and can make better <em>predictions</em> of future data. The problem, of course, is performing <em>inference</em> using those models. So we have <em>better models</em> that we want to use, but <em>complexities</em> around reaching exact conclusions from these. As such, the compromise is that as soon as we leave the world of the normal linear model, our inference <em>has to become approximate</em>. This can be an uncomfortable conclusion, but it is the reality of trying to move beyond the traditional normal linear model.</p>
<section id="option-1-pretend-that-boldsymbol-sigma-is-known">
<h3>Option 1 - Pretend that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is Known<a class="headerlink" href="#option-1-pretend-that-boldsymbol-sigma-is-known" title="Link to this heading">#</a></h3>
<p>In terms of trying to push forward, despite all these complexities, our first solution is to simply assume that <em>we know</em> <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. Of course, we <em>do not</em>, that is the whole problem we are trying to address. However, if we simply assume that we have got <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> correct, pretty much all the problems highlighted above <em>disappear</em>. In general, there are three ways this is realised in practice:</p>
<ul class="simple">
<li><p>Simply treat our estimate as the true value and move forward</p></li>
<li><p>Treat our estimate as the true value, but <em>only</em> because we have enough data that the uncertainty has disappeared</p></li>
<li><p>Treat our estimate as the true value, but only for the purpose of trying to <em>simulate</em> the uncertainty using computational methods</p></li>
</ul>
<p>We will now discuss each of these in turn.</p>
<section id="assume-hat-boldsymbol-sigma-boldsymbol-sigma">
<h4>Assume <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}} = \boldsymbol{\Sigma}\)</span><a class="headerlink" href="#assume-hat-boldsymbol-sigma-boldsymbol-sigma" title="Link to this heading">#</a></h4>
<p>Our first option is to <em>ignore</em> the problem. If we treat our estimate as <em>exactly</em> the population value, then we can carry on without any issues. So, if we take <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}} = \boldsymbol{\Sigma}\)</span> then there are no problems any more. In the context of GLS, this means we can remove the covariance structure <em>perfectly</em> and the whole problem reduces back to a regular regression model with <span class="math notranslate nohighlight">\(i.i.d.\)</span> errors. So, we simply act as if we knew <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> all along.</p>
<p>Although this is <em>practically</em> appealing, because all the mess indicated above disappears, it comes with some consequences:</p>
<ul class="simple">
<li><p>The extra uncertainty from estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is simply ignored. This means the model contains no penalty for estimating all the variance and covariance parameters.</p></li>
<li><p>This means that standard errors may be too small, test statistics too large and <span class="math notranslate nohighlight">\(p\)</span>-values overly-optimistic, especially in small samples.</p></li>
<li><p>We are pretending that degrees of freedom exist as a universal marker of uncertainty, but they technically do not. Furthermore, because we are pretending that we got <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> for free, the degrees of freedom have no correction for estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. As such, they will be <em>larger</em> than equivalent repeated measures ANOVA models.</p></li>
</ul>
</section>
<section id="use-asymptotic-results">
<h4>Use <em>Asymptotic</em> Results<a class="headerlink" href="#use-asymptotic-results" title="Link to this heading">#</a></h4>
<p>Our third option is to side-step degrees of freedom entirely. Recall from the normal linear model that the uncertainty that comes from estimating <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> effectively <em>disappears</em> once the same size is large enough. This is because <span class="math notranslate nohighlight">\(\hat{\sigma}^{2} = \sigma^{2}\)</span>, for all practical purposes. Thus, we can treat everything as if <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is known, because our uncertainty is effectively 0. We saw this in the shape of the <span class="math notranslate nohighlight">\(t\)</span>-distribution. Once the sample size is big enough, the <span class="math notranslate nohighlight">\(t\)</span>-distribution <em>becomes</em> a standard normal distribution whose width is fixed, rather than adaptive. When this happens, the degrees of freedom disappear. So, whilst we normally work with something like</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\beta}_{1}}{\text{SE}(\hat{\beta}_{1})} \sim \mathcal{T}(\nu),
\]</div>
<p>it is not wrong to work with</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\beta}_{1}}{\text{SE}(\hat{\beta}_{1})} \sim \mathcal{N}(0,1).
\]</div>
<p>The only caveat is that the sample size needs to be <em>big enough</em> for the second option to be accurate. However, notice that this second option <em>does not need degrees of freedom</em>. We say that this test is <em>asymptotically correct</em>, meaning it gets more accurate as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>. All we need to do is make the assumption that we have enough data so that we can effectively treat our estimate of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> as the <em>true value</em>. At that point it becomes a <em>constant</em>. So, there is no uncertainty to deal with, no sampling distribution to know, no concept of degrees of freedom and all the messiness disappears.</p>
<p>Although such asymptotic approaches are not necessary with the normal linear model, once we are in the realm of estimating a complex covariance structure this approach becomes more appealing. There is a <em>statistical purity</em> to this result because we do not need to pretend degrees of freedom still exist nor invent fictitious degrees of freedom based on the model. However, there are some clear issues here</p>
<ul class="simple">
<li><p>We need to be comfortable assuming that our <span class="math notranslate nohighlight">\(n\)</span> is <em>large-enough</em> for this to work, but this is an <em>unanswerable</em> question (see box below).</p></li>
<li><p>We need to be comfortable with the idea of dismissing uncertainty in the estimation of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> as negligible.</p></li>
<li><p>In small samples this will result in inference that is <em>optimistic</em>, though the open use of asymptotic tests already embeds this as a caution.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">How Large is “Large”?</p>
<p>If we want to lean on asymptotic theory, the obvious question is “how big does <span class="math notranslate nohighlight">\(n\)</span> need to be?”. The problem is that the definition is based on a <em>limit</em>, so it says that the approximation gets better and better as <span class="math notranslate nohighlight">\(n\)</span> moves towards infinity. For our purpose, <span class="math notranslate nohighlight">\(n\)</span> is the <em>number of subjects</em>, rather than the total amount of data. So, the answer is not that there is some magic sample size that is suddenly large enough, the answer is that the approximation will get better the larger <span class="math notranslate nohighlight">\(n\)</span> becomes. The question then is more about what our tolerance for error is. The point of the asymptotic theory is to say that the error that comes from estimation becomes more negligible as <span class="math notranslate nohighlight">\(n\)</span> grows, as does the penalty for estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> from the data. So, unfortunately, there is <em>no honest numeric answer to this question</em>. The way to think about it is as a <em>degree of comfort</em>. If you are using FGLS with <span class="math notranslate nohighlight">\(n = 5\)</span>, you should feel <em>very uncomfortable</em>. If you are using <span class="math notranslate nohighlight">\(n = 50\)</span>, you should probably feel <em>cautious</em> and if you have <span class="math notranslate nohighlight">\(n &gt; 200\)</span> you should probably be feeling <em>reasonably comfortable</em>. As <span class="math notranslate nohighlight">\(n\)</span> increases beyond that, you should probable feel perfectly fine about this approach. These are only ballpark figures, but the point is really to think of <span class="math notranslate nohighlight">\(n\)</span> as a <em>continuum of comfort</em>, rather than as a <em>threshold</em>.</p>
</div>
</section>
<section id="use-simulations">
<h4>Use Simulations<a class="headerlink" href="#use-simulations" title="Link to this heading">#</a></h4>
<p>The advantage here is that we do not assume the uncertainty around estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> has a particular shape. Yes, we fix the value estimated from the data as the “truth”, but the uncertainty around that “truth” is still captured via the simulations. This means we do not assume this uncertainty will have a particular sampling distribution, we let the simulations build the distribution over its many iterations. This neither requires assuming that the classical results still hold, nor requires enough data so that all these problems disappear.</p>
<p>As a final option, we can leave the world of trying to derive precise results mathematically and instead use the power of the <em>computer</em> to find a solution. This gets us into the world of <em>resampling methods</em>, which we encountered briefly last semester in the form of the <em>permutation test</em>, used when the errors are not normally distributed. For the general problem of deriving a null distribution under an arbitrary covariance structure, the <em>parametric bootstrap</em> is most commonly employed. In this method we:</p>
<ol class="arabic simple">
<li><p>Treat a fitted null model as the “truth”.</p></li>
<li><p>Use this fitted model to simulate new data.</p></li>
<li><p>Refit the model to the simulated dataset and save a copy of the test statistic.</p></li>
<li><p>Over many repeats of 2 and 3, build up a <em>distribution</em> of the test statistic under the null.</p></li>
<li><p>Calculate the <span class="math notranslate nohighlight">\(p\)</span>-value and confidence intervals from this distribution.</p></li>
</ol>
<p>So this requires <em>zero</em> theory about the distribution of anything. The uncertainty comes through naturally as part of the simulation and we can get a <span class="math notranslate nohighlight">\(p\)</span>-value irrespective of the form of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. So this has some distinct advantages because we can get rid of much of the difficult approximation needed in classical approaches. However, the tradeoffs are</p>
<ul class="simple">
<li><p>Computational burden, as calculating a single <span class="math notranslate nohighlight">\(p\)</span>-value can be a long process depending upon the complexity of refitting the model 1,000 times or more.</p></li>
<li><p>Fundamentally, we have to assume that our models is a close approximation to the truth for this to work. This can be seen as quite a <em>strong</em> assumption.</p></li>
</ul>
</section>
</section>
<section id="option-2-acknowledge-that-hat-boldsymbol-sigma-is-an-estimate">
<h3>Option 2 - Acknowledge that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> is an <em>Estimate</em><a class="headerlink" href="#option-2-acknowledge-that-hat-boldsymbol-sigma-is-an-estimate" title="Link to this heading">#</a></h3>
<p>Our second option is to accept that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> is an estimate and accept that we need to accommodate this uncertainty somehow. In order to do this, we can create <em>fictitious</em> degrees of freedom to allow a <span class="math notranslate nohighlight">\(p\)</span>-value to be calculated. So, although we fully accept that degrees of freedom no longer exist, what we can do is <em>find</em> a null distribution that matches our model and then use the degrees of freedom from that distribution. For instance, we can use a combination of heuristics and information in the model to approximate the <em>variance</em> of the calculated test statistic. If we know that the variance of the <span class="math notranslate nohighlight">\(t\)</span>-distribution is <span class="math notranslate nohighlight">\(\frac{\nu}{\nu - 2}\)</span>, then we can use our approximated variance to solve for <span class="math notranslate nohighlight">\(\nu\)</span>. This gives us a <span class="math notranslate nohighlight">\(t\)</span>-distribution with approximately the <em>correct width</em> for our calculated test statistic. These fictitious degrees of freedom are known as <em>effective</em> degrees of freedom. They attempt to capture <em>universal uncertainty</em> in the same way that traditional degrees of freedom do, but within a context where this definition is no longer applicable.</p>
<p>This method is perhaps more appealing than simply pretending there is no problem because it tried to accommodate small sample adjustments and uncertainty, though it also comes with some consequences:</p>
<ul class="simple">
<li><p>We are assuming that the true null distribution only differs from known null distributions (such as the <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span>) by its width, but not the general shape.</p></li>
<li><p>This still remains an <em>approximation</em>, though it should behave better in smaller samples when degrees of freedom become more necessary.</p></li>
<li><p>Degrees of freedom can become fractional and no longer have a clear theoretical grounding. They are more devices to encode “tail-heaviness” within the familiar language of <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span> distributions.</p></li>
</ul>
<p>In fact, we already saw an example of this last week in terms of the <em>non-sphericity corrections</em>.</p>
</section>
</section>
<section id="where-does-all-this-leave-us">
<h2>Where Does All This Leave Us?<a class="headerlink" href="#where-does-all-this-leave-us" title="Link to this heading">#</a></h2>
<p>So, where do these problems leave us in terms of leaving the world of very stringent covariance assumptions?</p>
<p>… Ultimately, from the pure perspective of a <em>model</em> that capture the <em>data-generating process</em>, FGLS is an attractive proposition … Unfortunately, the problems arrive as soon as we get to <em>inference</em> due to the fragility of the classic approaches to this problem. However, the reality is that as soon as we leave the world of the normal linear model, we leave the world of precise results and always end up in a world of approximations. This is not just a FGLS problem, this is a <em>global</em> problem. So if we ever want to use something more complicated than the normal linear model, we have to accept that precise inference breaks-down and we have to approximate it. The fundamental question simply becomes how <em>best</em> to approximate it so we can still reach useful conclusions from our models.</p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="chisq-foot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Sound suspiciously close to how the variance is estimated from the residuals in a normal linear model?</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="0.intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="1.gls.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Generalised Least Squares</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-basic-problem">The Basic Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-estimating-boldsymbol-sigma-breaks-inference">How Estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> Breaks Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#revisiting-the-normal-linear-model">Revisiting the Normal Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-scaled-chi-2-distribution">Understanding the Scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-origin-of-the-t-distribution">The Origin of the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-when-we-use-hat-boldsymbol-sigma">What Happens When We Use <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span>?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-solutions-to-this-problem">Practical Solutions to this Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-pretend-that-boldsymbol-sigma-is-known">Option 1 - Pretend that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is Known</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#assume-hat-boldsymbol-sigma-boldsymbol-sigma">Assume <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}} = \boldsymbol{\Sigma}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-asymptotic-results">Use <em>Asymptotic</em> Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-simulations">Use Simulations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-acknowledge-that-hat-boldsymbol-sigma-is-an-estimate">Option 2 - Acknowledge that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> is an <em>Estimate</em></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-does-all-this-leave-us">Where Does All This Leave Us?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr Martyn McFarquhar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>