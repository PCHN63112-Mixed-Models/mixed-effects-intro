
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Generalised Least Squares &#8212; An Introduction to Mixed-effects Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/test.css?v=a80109f0" />
    <link rel="stylesheet" type="text/css" href="_static/sphericity_3d_files/rglwidgetClass-1.3.18/rgl.css?v=57907efa" />
    <link rel="stylesheet" type="text/css" href="_static/sphericity_3d_files/htmltools-fill-0.5.8.1/fill.css?v=971cc1da" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1.gls';</script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/textures.src.js?v=9482728f"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/shadersrc.src.js?v=6ce05c17"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/buffer.src.js?v=1efca185"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/mouse.src.js?v=96f0b970"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/projection.src.js?v=98871b91"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/axes.src.js?v=3250d244"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/rglTimer.src.js?v=ac1c3151"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/pretty.src.js?v=4f2cffba"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/shaders.src.js?v=bbbdf37d"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/animation.src.js?v=74f9b9e8"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/pieces.src.js?v=280fd571"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/selection.src.js?v=5b47ed7d"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/init.src.js?v=f5bdcbbb"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/subscenes.src.js?v=42cb429d"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/controls.src.js?v=4b3dbe6f"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/draw.src.js?v=49d9cbaa"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/utils.src.js?v=56efe719"></script>
    <script src="_static/sphericity_3d_files/rglwidgetClass-1.3.18/rglClass.src.js?v=9e593197"></script>
    <script src="_static/sphericity_3d_files/rglWebGL-binding-1.3.18/rglWebGL.js?v=8cd6f6d7"></script>
    <script src="_static/sphericity_3d_files/htmlwidgets-1.6.4/htmlwidgets.js?v=175713be"></script>
    <script src="_static/sphericity_3d_files/CanvasMatrix4-1.3.18/CanvasMatrix.src.js?v=5a2d04be"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Mixed-effects Framework" href="2.lme-framework.html" />
    <link rel="prev" title="Introduction" href="0.intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="0.intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="An Introduction to Mixed-effects Models - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="An Introduction to Mixed-effects Models - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0.intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Generalised Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.lme-framework.html">The Mixed-effects Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.lme-advantages.html">Advantages of Mixed-effects</a></li>
<li class="toctree-l1"><a class="reference internal" href="4.estimation-inference.html">Estimation and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="5.lme-R.html">LME Models in <code class="docutils literal notranslate"><span class="pre">R</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/PCHN63112-Mixed-Models/mixed-effects-intro" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/PCHN63112-Mixed-Models/mixed-effects-intro/issues/new?title=Issue%20on%20page%20%2F1.gls.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1.gls.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Generalised Least Squares</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gls-theory">GLS Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-gls-do">What Does GLS Do?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-covariance-structure">Estimating the Covariance Structure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ignore-the-problem">1. Ignore the Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-effective-degrees-of-freedom">2. Calculate <em>Effective</em> Degrees of Freedom</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#produce-results-that-are-asymptotically-correct">3. Produce Results that are <em>Asymptotically</em> Correct</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance-constraints">Covariance Constraints</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gls-in-r">GLS in <code class="docutils literal notranslate"><span class="pre">R</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-paired-t-test-using-gls">The Paired <span class="math notranslate nohighlight">\(t\)</span>-test Using GLS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-using-gls">Inference Using GLS</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-p-value-problem">The <span class="math notranslate nohighlight">\(p\)</span>-value Problem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#omnibus-tests-and-follow-ups">Omnibus Tests and Follow-ups</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-we-use-gls">When Should We Use GLS?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="generalised-least-squares">
<h1>Generalised Least Squares<a class="headerlink" href="#generalised-least-squares" title="Link to this heading">#</a></h1>
<p>We will start our journey into the world of mixed-effects models by first examining a <em>related</em> approach that we have seen before: Generalised Least Squares (GLS). The reason for doing this is twofold. Firstly, GLS actually provides a simpler solution to many of the issues with the repeated measures ANOVA and thus presents a more logical starting point. Secondly, limitations in the way that GLS does this will provide some motivation for mixed-effects as a more complex, but ultimately more flexible, method of dealing with this problem.</p>
<section id="gls-theory">
<h2>GLS Theory<a class="headerlink" href="#gls-theory" title="Link to this heading">#</a></h2>
<p>We previously came across GLS in the context of allowing different variances for different groups of data in ANOVA-type models. This was motivated as a way of lifting the assumption of <em>homogeneity of variance</em>. However, GLS is actually a much more general technique. To see this, note that the probability model for GLS is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} \sim \mathcal{N}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> can take on <em>any structure</em>. In other words, GLS has exactly the same probability model as the normal linear model, except that it allows for a flexible specification of the variance-covariance matrix. In our previous examples, we used GLS to populate the variance-covariance matrix with different variances for each group. For instance, if we had two groups with three subjects each, our GLS model would be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
y_{11} \\
y_{21} \\
y_{31} \\
y_{12} \\
y_{22} \\
y_{32} \\
\end{bmatrix}
\sim\mathcal{N}\left(
\begin{bmatrix}
\mu_{1} \\
\mu_{1} \\
\mu_{1} \\
\mu_{2} \\
\mu_{2} \\
\mu_{2} \\
\end{bmatrix},
\begin{bmatrix}
\sigma^{2}_{1}  &amp; 0              &amp; 0              &amp; 0              &amp; 0              &amp; 0              \\
0               &amp; \sigma^{2}_{1} &amp; 0              &amp; 0              &amp; 0              &amp; 0              \\
0               &amp; 0              &amp; \sigma^{2}_{1} &amp; 0              &amp; 0              &amp; 0              \\
0               &amp; 0              &amp; 0              &amp; \sigma^{2}_{2} &amp; 0              &amp; 0              \\
0               &amp; 0              &amp; 0              &amp; 0              &amp; \sigma^{2}_{2} &amp; 0              \\
0               &amp; 0              &amp; 0              &amp; 0              &amp; 0              &amp; \sigma^{2}_{2} \\
\end{bmatrix}
\right).
\end{split}\]</div>
<p>This was actually a special case of GLS known as <em>weighted least squares</em> (WLS)<a class="footnote-reference brackets" href="#weights-foot" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, where all the off-diagonal elements of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> are 0. However, the crucial point is that  we can use GLS to impose differences in <em>both</em> the variances <em>and</em> the covariances. So while we did not do this previously, we can include <em>correlation</em> in the GLS model. Thus, if our general problem with repeated measures is that the variance-covariance structure is not correctly handled by the normal linear model, GLS provides a direct solution. Furthermore, if a core complaint of the repeated measures ANOVA is that the assumed covariance structure is too restrictive, GLS again provides a direct solution. So, on the face of it, GLS directly solves many of the issues we encountered last week.</p>
<section id="what-does-gls-do">
<h3>What Does GLS Do?<a class="headerlink" href="#what-does-gls-do" title="Link to this heading">#</a></h3>
<p>Technically, the machinery behind GLS is based on assuming we know <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> <em>a priori</em>. Although this would seem a silly place to start (given that we will almost <em>never</em> know this), we can go along with it and see where it gets us. So, <em>if</em> we know what the true covariance structure is, GLS provides a way of <em>removing</em> it from the data<a class="footnote-reference brackets" href="#white-foot" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. Once removed, the errors return to <span class="math notranslate nohighlight">\(i.i.d.\)</span> and we are back in the world of the normal linear model. This is a very enticing prospect because all the difficulties associated with correlation effectively <em>disappear</em> and we can treat the data as a regular collection of independent values. So, although this <em>removal</em> procedure happens behind the scenes, we can conceptualise GLS as effectively a <em>transformation</em> that treats the covariance structure as <em>nuisance</em> and removes it, allowing all the theory from last semester to still apply.</p>
<p>In terms of the real world, if we do not known <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> then we cannot technically use GLS. This would seem a bit of a dead-end. However, it is possible to use a method such as REML to <em>estimate</em> <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> from the data. This is known as <em>feasible generalised least squares</em> (FGLS). The question then becomes, how does working with <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> rather than <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> change things? This is a much bigger question that goes <em>beyond</em> FGLS and extends to <em>any</em> methods where the covariance structure is <em>estimated</em>, mixed-effects included.</p>
</section>
</section>
<section id="estimating-the-covariance-structure">
<h2>Estimating the Covariance Structure<a class="headerlink" href="#estimating-the-covariance-structure" title="Link to this heading">#</a></h2>
<p>If we <em>do not</em> know <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> ahead of time, then the term <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is no longer a <em>fixed constant</em>. Instead, we have <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span>, which is a <em>random variable</em>. This introduces an additional layer of uncertainty that causes some major issues. The full story is given in the drop-down below, but the short version is that treating <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> as an <em>estimate</em> means we no longer know what null distribution our test statistic has. In fact, the whole concept of degrees of freedom as a metric that captures this uncertainty suddenly disappear. This means we do not know how the standard errors are distributed, do not know how our test statistic is distributed and have no way of calculating a <span class="math notranslate nohighlight">\(p\)</span>-value. In short, <em>our inferential machinery falls apart</em>.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">How Estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> Breaks Inference</p>
<p>To understand why the NHST machinery breaks, we need to go back to some of the information covered last semester on <a class="reference external" href="https://pchn63101-advanced-data-skills.github.io/Inference-Linear-Model/2.estimation-uncertainty.html">statistical inference</a>. Recall that in the normal linear model the uncertainty that comes from estimating <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> affects inference via the <em>denominator</em> of the test statistic. If we <em>know</em> <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, then our test statistic is a <span class="math notranslate nohighlight">\(z\)</span>-statistic and is distributed as <span class="math notranslate nohighlight">\(z \sim \mathcal{N}(0,1)\)</span>. However, when <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is <em>estimated</em>, the test statistic is a <span class="math notranslate nohighlight">\(t\)</span>-statistic and is distributed as <span class="math notranslate nohighlight">\(t \sim \mathcal{T}(\nu)\)</span>. Here, <span class="math notranslate nohighlight">\(\nu\)</span> is the <em>degrees of freedom</em>, which characterises the <em>uncertainty</em> in the estimate of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>. This is what controls the <em>width</em> of the <span class="math notranslate nohighlight">\(t\)</span>-distribution, which will approach the standard normal as the sample size increases. In other words, we can think of the degrees of freedom as “the amount to which the null distribution deviates from a standard normal due to uncertainty in the estimation of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>”. So the degrees of freedom are <em>key</em>.</p>
<p>Now, the whole reason the <span class="math notranslate nohighlight">\(t\)</span>-distribution exists is because it can be derived from the structure of the test statistic. Because both the numerator and denominator are <em>estimates</em>, they are both random variables with a given sampling distribution. In order to work out the distribution of their ratio, <em>both</em> sampling distributions need to be derived. Last semester, we showed that the distribution of the parameter estimates was a known quantity. For a single slope from a typical regression model, we have</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_{1} \sim \mathcal{N}\left(\beta_{1}, \frac{\sigma^{2}}{\sum{(x_{i} - \bar{x})^{2}}}\right).
\]</div>
<p>Importantly, the variance of this distribution depends upon knowing <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, which we do not. If we replace this with an <em>estimate</em>, <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>, we introduce another layer of uncertainty. In order to characterise this addition layer, we need to know the distribution of <span class="math notranslate nohighlight">\(\hat{\sigma}^{2}\)</span>. We glossed-over this last semester, but under the normal linear model this estimate has the following sampling distribution</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^{2} \sim \frac{\sigma^{2}}{\nu}\chi^{2}(\nu)
\]</div>
<p>This is a <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom that is scaled into the same units as the variance. The thing to notice here is that this distribution is <em>where degrees of freedom come from</em>. The reason degrees of freedom exist is because they appear as a parameter that governs the <em>width</em> of this sampling distribution. Thus, degrees of freedom directly encode uncertainty around the true value of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>. As the sample size goes up, the degrees of freedom go up and the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> gets <em>narrower</em> until it collapses into a single point centred on <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>. Because the standard error of the parameter estimate depends upon the estimate of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution passes this uncertainty on to the <span class="math notranslate nohighlight">\(t\)</span>-distribution. As such, the <span class="math notranslate nohighlight">\(t\)</span>-distribution is similarly parameterised by the degrees of freedom. The point where the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> collapses to a single point is <em>exactly</em> when the <span class="math notranslate nohighlight">\(t\)</span>-distribution and the standard normal become <em>the same</em>. At that point, uncertainty is effectively 0 and the degrees of freedom are no longer important.</p>
<p>Now, getting back to FGLS, what happens to the standard error when <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is no longer <em>a single number</em> and is instead a complex function of different elements of an unstructured variance-covariance matrix? Well, the formula for the standard error remains <em>correct</em>, but the distribution can no longer be derived analytically. It ceases to be a consistent object across all models and, in effect, becomes <em>unknowable</em>. This means that the scaled <span class="math notranslate nohighlight">\(\chi^{2}\)</span> distribution disappears and, along with it, so too does the concept of degrees of freedom as a means of characterising uncertainty. If we divide our parameter estimate by its standard error it is no longer the ratio of two random variables with known distributions. It is the ratio of a random variable with a known distribution and a random variable <em>with no known distribution</em>. This makes the null distribution of this test statistic <em>also unknown</em>. And without a known null, there is no way to calculate a <span class="math notranslate nohighlight">\(p\)</span>-value. So, if we accept that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> is an <em>estimate</em> then our test statistic has no known null distribution, our ability to calculate <span class="math notranslate nohighlight">\(p\)</span>-values disappears and the concept of degrees of freedom vanishes. In short, we are stuck.</p>
</div>
<p>There are generally 3 ways to deal with this situation and we will see that different packages have differing opinions on the best way to go about it. This is true both for FGLS and for mixed-effects models</p>
<section id="ignore-the-problem">
<h3>1. Ignore the Problem<a class="headerlink" href="#ignore-the-problem" title="Link to this heading">#</a></h3>
<p>Our first option is to <em>ignore</em> the problem. If we treat our estimate as <em>exactly</em> the population value, then we can carry on without any issues. So, if we take <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}} = \boldsymbol{\Sigma}\)</span> then there are no problems any more. In the context of GLS, this means we can remove the covariance structure <em>perfectly</em> and the whole problem reduces back to a regular regression model with <span class="math notranslate nohighlight">\(i.i.d.\)</span> errors. So, we simply act as if we knew <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> all along.</p>
<p>Although this is <em>practically</em> appealing, because all the mess indicated above disappears, it comes with some consequences:</p>
<ul class="simple">
<li><p>The extra uncertainty from estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is simply ignored. This means the model contains no penalty for estimating all the variance and covariance parameters.</p></li>
<li><p>This means that standard errors may be too small, test statistics too large and <span class="math notranslate nohighlight">\(p\)</span>-values overly-optimistic, especially in small samples.</p></li>
<li><p>We are pretending that degrees of freedom exist, but they technically do not.</p></li>
</ul>
<p>As we will see, this what the <code class="docutils literal notranslate"><span class="pre">gls()</span></code> function choses to do by default.</p>
</section>
<section id="calculate-effective-degrees-of-freedom">
<h3>2. Calculate <em>Effective</em> Degrees of Freedom<a class="headerlink" href="#calculate-effective-degrees-of-freedom" title="Link to this heading">#</a></h3>
<p>Our second option is to accept that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> is an estimate and accept that we need to accommodate this uncertainty somehow. In order to do this, we can create <em>fictitious</em> degrees of freedom to allow a <span class="math notranslate nohighlight">\(p\)</span>-value to be calculated. So, although we fully accept that degrees of freedom no longer exist, what we can do is <em>find</em> a null distribution that matches our model and then use the degrees of freedom from that distribution as our metric of uncertainty. For instance, we can use information in the model to approximate the <em>variance</em> of the calculated test statistic. If we know that the variance of the <span class="math notranslate nohighlight">\(t\)</span>-distribution is <span class="math notranslate nohighlight">\(\frac{\nu}{\nu - 2}\)</span>, then we can use our approximated variance to solve for <span class="math notranslate nohighlight">\(\nu\)</span>. This gives us a <span class="math notranslate nohighlight">\(t\)</span>-distribution with the <em>correct width</em> for our calculated test statistic. These fictitious degrees of freedom are known as <em>effective</em> degrees of freedom and allow a <span class="math notranslate nohighlight">\(p\)</span>-value to be calculated.</p>
<p>This method is perhaps more appealing than simply pretending there is no problem, though it also comes with some consequences:</p>
<ul class="simple">
<li><p>We are assuming that the true null distribution only differs from known null distributions (such as the <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span>) by its width, but not the general shape.</p></li>
<li><p>This still remains an <em>approximation</em>, though it should behave better in smaller samples when degrees of freedom become more necessary.</p></li>
<li><p>Degrees of freedom can become fractional and no longer have a clear theoretical grounding. They are more devices to encode “tail-heaviness” within a familiar language of <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span> distributions.</p></li>
</ul>
<p>As we will see, this is what some methods associated with FGLS models use and is often the solution employed in mixed-effects models.</p>
</section>
<section id="produce-results-that-are-asymptotically-correct">
<h3>3. Produce Results that are <em>Asymptotically</em> Correct<a class="headerlink" href="#produce-results-that-are-asymptotically-correct" title="Link to this heading">#</a></h3>
<p>Finally, our third option is to side-step all the messiness with degrees of freedom entirely. Recall from the normal linear model that the uncertainty that comes from estimating <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> effectively <em>disappears</em> once the same size is large enough. This is because <span class="math notranslate nohighlight">\(\hat{\sigma}^{2} = \sigma^{2}\)</span>, for all practical purposes. Thus, we can treat everything as if <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is known, because our uncertainty is effectively 0. We saw this in the shape of the <span class="math notranslate nohighlight">\(t\)</span>-distribution. Once the sample size is big enough, the <span class="math notranslate nohighlight">\(t\)</span>-distribution <em>becomes</em> a standard normal distribution whose width is fixed, rather than adaptive. When this happens, the degrees of freedom disappear. So, whilst we normally work with something like</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\beta}_{1}}{\text{SE}(\hat{\beta}_{1})} \sim \mathcal{T}(\nu),
\]</div>
<p>it is not wrong to work with</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\beta}_{1}}{\text{SE}(\hat{\beta}_{1})} \sim \mathcal{N}(0,1).
\]</div>
<p>The only caveat is that the sample size needs to be <em>big enough</em> for the second option to be accurate. However, notice that this second option <em>does not need degrees of freedom</em>. We say that this test is <em>asymptotically correct</em>, meaning it gets more accurate as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>. All we need to do is make the assumption that we have enough data so that we can effectively treat our estimate of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> as the <em>true value</em>. At that point it becomes a <em>constant</em>. So, there is no uncertainty to deal with, no sampling distribution to know, no concept of degrees of freedom and all the messiness disappears.</p>
<p>Although this is not necessary with the normal linear model, once we are in the realm of estimating a complex covariance structure this approach becomes more appealing. There is a <em>statistical purity</em> to this result because we do not need to pretend degrees of freedom still exist nor invent fictitious degrees of freedom based on the model. However, there are some clear issues here</p>
<ul class="simple">
<li><p>We need to be comfortable assuming that our <span class="math notranslate nohighlight">\(n\)</span> is <em>large-enough</em> for this to work, but this is an <em>unanswerable</em> question (see box below).</p></li>
<li><p>We need to be comfortable with the idea of dismissing uncertainty in the estimation of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> as negligible.</p></li>
<li><p>Any sense of “exactness” is gone.</p></li>
<li><p>In small samples, this will result in inference that is <em>optimistic</em> (though we can at least be open about this)</p></li>
</ul>
<p>As we will see, this is also what some methods associated with FGLS and mixed-effects models use.</p>
<div class="tip admonition">
<p class="admonition-title">How Large is “Large”?</p>
<p>If we want to lean on asymptotic theory and pretend that everything is fine when using FGLS, the obvious question is “how big does <span class="math notranslate nohighlight">\(n\)</span> need to be?”. The problem is that the definition is based on a <em>limit</em>, so it says that the approximation gets better and better as <span class="math notranslate nohighlight">\(n\)</span> moves towards infinity. For our purpose, <span class="math notranslate nohighlight">\(n\)</span> is the <em>number of subjects</em>, rather than the total amount of data. So, the answer is not that there is some magic sample size that is suddenly large enough, the answer is that the approximation will get better the larger <span class="math notranslate nohighlight">\(n\)</span> becomes. The question then is more about what our tolerance for error is. The point of the asymptotic theory is to say that the error that comes from estimation becomes more negligible as <span class="math notranslate nohighlight">\(n\)</span> grows, as does the penalty for estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> from the data. So, unfortunately, there is <em>no honest numeric answer to this question</em>. The way to think about it is as a <em>degree of comfort</em>. If you are using FGLS with <span class="math notranslate nohighlight">\(n = 5\)</span>, you should feel <em>very uncomfortable</em>. If you are using <span class="math notranslate nohighlight">\(n = 50\)</span>, you should probably feel <em>cautious</em> and if you have <span class="math notranslate nohighlight">\(n &gt; 200\)</span> you should probably be feeling <em>reasonably comfortable</em>. As <span class="math notranslate nohighlight">\(n\)</span> increases beyond that, you should probable feel perfectly fine about the FGLS assumptions. These are only ballpark figures, but the point is really to think of <span class="math notranslate nohighlight">\(n\)</span> as a <em>continuum of comfort</em>, rather than as a <em>threshold</em>.</p>
</div>
</section>
<section id="covariance-constraints">
<h3>Covariance Constraints<a class="headerlink" href="#covariance-constraints" title="Link to this heading">#</a></h3>
<p>As well as understanding that the very process of estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> causes problems, we also need to understand that we cannot have free reign to estimate any old covariance structure we like. One of the most important elements to recognise is that some sort of <em>constraint</em> is always needed when estimating a variance-covariance matrix. To see this, note that for a repeated measures experiment there are <span class="math notranslate nohighlight">\(nt \times nt\)</span> values in this matrix. The values above and below the diagonal are a mirror image, so the true number of unknown values is <span class="math notranslate nohighlight">\(\frac{nt(nt + 1)}{2}\)</span>. For instance, if we had <span class="math notranslate nohighlight">\(n = 5\)</span> subjects and <span class="math notranslate nohighlight">\(t = 3\)</span> repeated measures, there would be <span class="math notranslate nohighlight">\(\frac{15 \times 16}{2} = 120\)</span> unique values in the variance-covariance matrix. If we allowed it to be completely unstructured, we would have 120 values to estimate <em>just</em> for the covariance structure. Indeed, this is not really possible unless the amount of data we have <em>exceeds</em> the number of parameters. So, the data itself imposes a <em>constraint</em> on how unstructured the covariance matrix can be.</p>
<p>Luckily, for most applications, we not only assume that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> has a block-diagonal structure (so most off-diagonal entries are 0), but that many of the off-diagonal elements are actually <em>identical</em>. We saw this previously with the repeated measures ANOVA. Even though <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> may have <em>hundreds</em> of values we <em>could</em> fill-in, if we assume compound symmetry only within each subject, there are only <em>two</em> covariance parameters to be estimated: <span class="math notranslate nohighlight">\(\sigma^{2}_{b}\)</span> and <span class="math notranslate nohighlight">\(\sigma^{2}_{w}\)</span>. The whole matrix can then be constructed using those two alone. This is an example of <em>extreme simplification</em>, but it does highlight that we generally do not estimate the <em>whole</em> variance-covariance matrix. We only estimate <em>small parts</em> of it. Indeed, making the covariance matrix more general is often a risky move because of the number of additional parameters needed. The more we estimate from the same data, the greater our uncertainty will become because each element of the covariance-matrix is supported by <em>less data</em>. Complexity always comes at a price.</p>
</section>
</section>
<section id="gls-in-r">
<h2>GLS in <code class="docutils literal notranslate"><span class="pre">R</span></code><a class="headerlink" href="#gls-in-r" title="Link to this heading">#</a></h2>
<p>We have seen some examples of using the <code class="docutils literal notranslate"><span class="pre">gls()</span></code> function from <code class="docutils literal notranslate"><span class="pre">nlme</span></code> last semester. At that point, we only focused on the use of the <code class="docutils literal notranslate"><span class="pre">weights=</span></code> argument with different variance structures (e.g. <code class="docutils literal notranslate"><span class="pre">varIdent()</span></code>, <code class="docutils literal notranslate"><span class="pre">varPower()</span></code> etc.). However, there is also a <code class="docutils literal notranslate"><span class="pre">correlation=</span></code> argument that similarly takes a number of pre-specified correlation structures. We can use these two arguments together to form a final variance-covariance matrix that consists of correlation and heterogenous variance groups.</p>
<section id="the-paired-t-test-using-gls">
<h3>The Paired <span class="math notranslate nohighlight">\(t\)</span>-test Using GLS<a class="headerlink" href="#the-paired-t-test-using-gls" title="Link to this heading">#</a></h3>
<p>We can start with the most simple example of the paired <span class="math notranslate nohighlight">\(t\)</span>-test using GLS. Importantly, this is an unnecessary step theoretically because the paired <span class="math notranslate nohighlight">\(t\)</span>-test is a perfectly acceptable technique. When there are only two-repeats there are no arguments about the covariance structure. There can only be a single correlation term. So compound symmetry always works. However, it is useful for us as a <em>starting point</em> because it is the simplest example of the problem.</p>
<p>In order to specify a correlation structure, we need to pass one of the predefined correlation functions as an argument to <code class="docutils literal notranslate"><span class="pre">correlation=</span></code>. These structures include functions such as <code class="docutils literal notranslate"><span class="pre">corCompSymm()</span></code>, <code class="docutils literal notranslate"><span class="pre">corSpher()</span></code>, <code class="docutils literal notranslate"><span class="pre">corAR1()</span></code> and <code class="docutils literal notranslate"><span class="pre">corSymm()</span></code><a class="footnote-reference brackets" href="#corfunc-foot" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>. For this example, we will use <code class="docutils literal notranslate"><span class="pre">corCompSymm()</span></code>, which constructs a compound symmetric structure.</p>
<p>In order to use <code class="docutils literal notranslate"><span class="pre">corCompSymm()</span></code>, we need to supply it with a description of how we want it structured in relation to our data. This is done using the <code class="docutils literal notranslate"><span class="pre">form=</span></code> argument, which takes a one-sided formula expressing the structure we want. For this example, we will use <code class="docutils literal notranslate"><span class="pre">corCompSymm(form=</span> <span class="pre">~1|subject)</span></code>. This indicates that we want a constant correlation (<code class="docutils literal notranslate"><span class="pre">1</span></code>) grouped by subject (<code class="docutils literal notranslate"><span class="pre">|subject</span></code>). So, the term on the <em>right</em> of <code class="docutils literal notranslate"><span class="pre">|</span></code> is key here. This gives a <em>grouping factor</em> such that any observations from the same level will share a constant correlation. Because we have used <code class="docutils literal notranslate"><span class="pre">subject</span></code>, each level represents a <em>different</em> subject and thus any observations that come from the same subject will be correlated. This therefore defines our <em>block-diagonal</em> covariance structure, where the term on the right of <code class="docutils literal notranslate"><span class="pre">|</span></code> forms the <em>blocks</em>. We will see ways to visualise this in order to provide more intuition a little later.</p>
<p>Returning to our example, we will use the <code class="docutils literal notranslate"><span class="pre">mice2</span></code> data from <code class="docutils literal notranslate"><span class="pre">datarium</span></code> again, which we have converted to long-format as discussed last week.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   id   time weight
1   1 before  187.2
2   1  after  429.5
3   2 before  194.2
4   2  after  404.4
5   3 before  231.7
6   3  after  405.6
7   4 before  200.5
8   4  after  397.2
9   5 before  201.7
10  5  after  377.9
11  6 before  235.0
12  6  after  445.8
13  7 before  208.7
14  7  after  408.4
15  8 before  172.4
16  8  after  337.0
17  9 before  184.6
18  9  after  414.3
19 10 before  189.6
20 10  after  380.3
</pre></div>
</div>
</div>
</div>
<p>To fit this model using GLS, we use</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">nlme</span><span class="p">)</span>

<span class="n">gls.mod</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">gls</span><span class="p">(</span><span class="n">weight</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">time</span><span class="p">,</span><span class="w"> </span><span class="n">correlation</span><span class="o">=</span><span class="nf">corCompSymm</span><span class="p">(</span><span class="n">form</span><span class="o">=~</span><span class="m">1</span><span class="o">|</span><span class="n">id</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">mice2.long</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>where we can see the use of the <code class="docutils literal notranslate"><span class="pre">correlation=</span></code> argument with the <code class="docutils literal notranslate"><span class="pre">corCompSymm()</span></code> function. We could also optionally include a <code class="docutils literal notranslate"><span class="pre">weights=</span></code> argument if we wanted the diagonal elements of the covariance matrix to differ by <code class="docutils literal notranslate"><span class="pre">time</span></code>. This would take the form <code class="docutils literal notranslate"><span class="pre">weights=varIdent(form=</span> <span class="pre">~1|time)</span></code>. However, we will keep this simple for now.</p>
</section>
<section id="inference-using-gls">
<h3>Inference Using GLS<a class="headerlink" href="#inference-using-gls" title="Link to this heading">#</a></h3>
<p>Although we should check the assumptions of the GLS model, we will leave that to one side given that we covered it last semester. The more pressing issue for us is to discuss <em>inference</em> using the GLS model, especially given the problems highlighted earlier. To begin with, we can treat the returned objects from <code class="docutils literal notranslate"><span class="pre">gls()</span></code> just like an object from <code class="docutils literal notranslate"><span class="pre">lm()</span></code> and call <code class="docutils literal notranslate"><span class="pre">summary()</span></code> to examine the model estimates and tests.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">summary</span><span class="p">(</span><span class="n">gls.mod</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generalized least squares fit by REML
  Model: weight ~ time 
  Data: mice2.long 
      AIC      BIC    logLik
  177.349 180.9105 -84.67449

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
      Rho 
0.5332493 

Coefficients:
             Value Std.Error  t-value p-value
(Intercept) 200.56  8.081914 24.81591       0
timeafter   199.48  7.808574 25.54628       0

 Correlation: 
          (Intr)
timeafter -0.483

Standardized residuals:
        Min          Q1         Med          Q3         Max 
-2.46661859 -0.54818094  0.02112903  0.38482224  1.79048964 

Residual standard error: 25.55725 
Degrees of freedom: 20 total; 18 residual
</pre></div>
</div>
</div>
</div>
<p>We can see here all the usual output that matches what <code class="docutils literal notranslate"><span class="pre">lm()</span></code> gives us. Indeed, despite what we mentioned earlier, we still get <span class="math notranslate nohighlight">\(t\)</span>-statistics and <span class="math notranslate nohighlight">\(p\)</span>-values (though note that the <span class="math notranslate nohighlight">\(p\)</span>-values are <em>rounded</em> rather than displayed using scientific notation). We also have information on the estimated correlation structure, with the single correlation parameter given by <span class="math notranslate nohighlight">\(\hat{\rho} = 0.53\)</span>. Of most importance is that both the <em>standard error</em> and <em><span class="math notranslate nohighlight">\(t\)</span>-value</em> match what we saw last week from the paired <span class="math notranslate nohighlight">\(t\)</span>-test. This is evidence enough to show that the correlation <em>is</em> being taken into account. Importantly, this is being done within a linear model framework, but <em>without</em> the need to subtract the differences <em>or</em> to manually partition the errors by including <code class="docutils literal notranslate"><span class="pre">id</span></code> in the model formula.</p>
<section id="the-p-value-problem">
<h4>The <span class="math notranslate nohighlight">\(p\)</span>-value Problem<a class="headerlink" href="#the-p-value-problem" title="Link to this heading">#</a></h4>
<p>Although all the elements highlighted above are positives, there is something important we need to recognise about the reported <span class="math notranslate nohighlight">\(p\)</span>-values. As mentioned earlier, if we work with the idea that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is <em>estimated</em> then the null distribution of the test statistics become unknown and there is no way to calculate a <span class="math notranslate nohighlight">\(p\)</span>-value. And yet, the <code class="docutils literal notranslate"><span class="pre">gls()</span></code> function <em>does</em> produce <span class="math notranslate nohighlight">\(p\)</span>-values. How? <em>Because it is acting as if <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is known</em>. In effect, even though we are using FGLS, the <code class="docutils literal notranslate"><span class="pre">gls()</span></code> function ignores this and acts as if we are doing <em>proper</em> GLS with a known covariance structure. All the calculations of <span class="math notranslate nohighlight">\(t\)</span>-statistics, degrees of freedom and <span class="math notranslate nohighlight">\(p\)</span>-values are based on assuming that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is known and thus we can perfectly remove the covariance structure from the data and end-up back in the world of a normal linear model.</p>
<p>The problem with this is that the uncertainty in estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is not taken into account. Indeed, we have seen that there is no exact way of quantifying this uncertainty because the concept of degrees of freedom vanishes as soon as the sampling distribution becomes unknown. The <code class="docutils literal notranslate"><span class="pre">gls()</span></code> function is <em>pretending</em> that we known <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> and can return the model to a world where degrees of freedom exist and null distributions are known. The extent to which we can trust the <span class="math notranslate nohighlight">\(p\)</span>-values therefore depends upon the extent to which we are willing to follow along with this fiction. At best, we need to treat these values as <em>very rough approximations</em> and nothing more. This is especially true in <em>small samples</em>. As mentioned earlier, as <span class="math notranslate nohighlight">\(n\)</span> gets larger the estimate of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span> gets closer to the true value and the uncertainty largely disappears. So if our sample is <em>large</em>, we have less concern with the GLS tests. However, great caution is needed if our sample is <em>small</em>.</p>
<p>Part of the problem here is that FGLS knows <em>nothing</em> about the structure of the data. The very fact that we have repeated measurements from multiple subjects is somewhat lost inside of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. All GLS knows is that there <em>is</em> correlation, but not <em>where it came from</em>. As we will see, mixed-effects models actively embed the structure that causes the data to be correlated. This does not magically get rid of the problem of an unknown sampling distribution under an arbitrary covariance structure, but it does allow for a better approximation of the uncertainty that comes from estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. So, a mixed-effects model will not fix this situation, but it does give us better options beyond simply pretending that <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is known. This is not the main reason for using mixed-effects over GLS, but it is an advantage.</p>
</section>
<section id="omnibus-tests-and-follow-ups">
<h4>Omnibus Tests and Follow-ups<a class="headerlink" href="#omnibus-tests-and-follow-ups" title="Link to this heading">#</a></h4>
<p>Putting aside the inferential issues above, the utility of the GLS framework is that we can simply treat the model in the same way as a result from <code class="docutils literal notranslate"><span class="pre">lm()</span></code>. This means we can compute ANOVA-style tests</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">anova</span><span class="p">(</span><span class="n">gls.mod</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Denom. DF: 18 
            numDF   F-value p-value
(Intercept)     1 1800.9410  &lt;.0001
time            1  652.6124  &lt;.0001
</pre></div>
</div>
</div>
</div>
<p>generate confidence intervals (though these should similarly be treated cautiously),</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint</span><span class="p">(</span><span class="n">gls.mod</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>               2.5 %   97.5 %
(Intercept) 184.7197 216.4003
timeafter   184.1755 214.7845
</pre></div>
</div>
</div>
</div>
<p>create plots using the <code class="docutils literal notranslate"><span class="pre">effects</span></code> package</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="s">&#39;effects&#39;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="nf">effect</span><span class="p">(</span><span class="s">&#39;time&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">gls.mod</span><span class="p">,</span><span class="w"> </span><span class="n">residuals</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">),</span><span class="w"> </span><span class="n">partial.residuals</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">smooth</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/607e36d043a0a73cb4e713e591c1b6b6ae653c7b4b76a357f58e3294a81e4245.png" src="_images/607e36d043a0a73cb4e713e591c1b6b6ae653c7b4b76a357f58e3294a81e4245.png" />
</div>
</div>
<p>and compute follow-up tests using <code class="docutils literal notranslate"><span class="pre">emmeans</span></code><a class="footnote-reference brackets" href="#emmeans-foot" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">emmeans</span><span class="p">)</span>
<span class="n">emm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">emmeans</span><span class="p">(</span><span class="n">gls.mod</span><span class="p">,</span><span class="w"> </span><span class="n">pairwise</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">time</span><span class="p">,</span><span class="w"> </span><span class="n">mode</span><span class="o">=</span><span class="s">&quot;df.error&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">emm</span><span class="o">$</span><span class="n">contrasts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> contrast       estimate   SE df t.ratio p.value
 before - after     -199 7.81 17 -25.546  &lt;.0001

Degrees-of-freedom method: df.error 
</pre></div>
</div>
</div>
</div>
<p>So, if we put the inferential issues to one side, we can see that GLS provides a nice alternative to the repeated measures ANOVA because it exists within the linear model framework and thus allows us to use all the methods we have seen previously. Furthermore, we can lift the assumption of compound symmetry and use a variety of different covariance matrices. The main downside is that this comes with a price in terms of the <span class="math notranslate nohighlight">\(p\)</span>-value not taking the uncertainty in the estimation of the covariance structure into account. This is especially problematic in <em>small samples</em>, which means we should really treat the hypothesis tests from a GLS model as only <em>asymptotically correct</em>. If we are happy to do so, GLS becomes quite a useful method to have on hand. We will see a few more examples of using GLS with more complex ANOVA models in the associated workshop.</p>
</section>
</section>
</section>
<section id="when-should-we-use-gls">
<h2>When Should We Use GLS?<a class="headerlink" href="#when-should-we-use-gls" title="Link to this heading">#</a></h2>
<p>GLS is most useful in situations where we know what covariance structure we want to impose on the data and are not as concerned about the model not understanding any form of deeper structure in the data. However, if we do not know the covariance structure and know that the data has a deeper structure that the model should be able to exploit, this is where mixed-effects models come into their own.</p>
<p>… The problem is that GLS does not know anything about the <em>structure</em> of the data. It has no sense of <em>subjects</em> as the experimental unit, nor the idea that the outcome variable is comprised of clusters of values taken from different subjects who might themselves form clusters of values from larger groups (e.g. patients vs controls). All that GLS knows is that there is a correlation structure that we want to remove. Unfortunately, this lack of appreciation for the structure of the data means that GLS cannot use that structure to its advantage. There is no separation of the information available by pooling observations across subjects, or subjects across groups. In effect, GLS is a very <em>crude</em> solution to a bigger problem with repeated measurements. Namely, that there is a larger <em>hierarchical</em> structure at play that the model should be able to take advantage of. We have seen this in a very general way through small-sample degrees of freedom, but really this is only a <em>symptom</em> of a larger problem. As we will come to learn, mixed-effects models are advantageous precisely <em>because</em> they embed this structure in the model. This has a number of consequences, not least the fact that correlation between measurements from the same experimental unit are <em>automatically</em> embedded in the model. This is not because we tell the model to include correlation, rather it is a <em>natural consequence</em> of the structure of the data. As such, mixed-effects models are useful because features such as correlation are a natural part of the modelling framework, precisely because it does take the structure into account in a way that GLS simply cannot.</p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="weights-foot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>This is why the argument in <code class="docutils literal notranslate"><span class="pre">gls()</span></code> was <code class="docutils literal notranslate"><span class="pre">weights=</span></code>.</p>
</aside>
<aside class="footnote brackets" id="white-foot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>This is sometimes known as <em>whitening</em> the data. This is a term you may come across in the neuroimaging literature, particularly in relation to how fMRI is analysed.</p>
</aside>
<aside class="footnote brackets" id="corfunc-foot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>You can look up descriptions of all of these using <code class="docutils literal notranslate"><span class="pre">?corClasses</span></code> at the prompt.</p>
</aside>
<aside class="footnote brackets" id="emmeans-foot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>The <code class="docutils literal notranslate"><span class="pre">mode=</span></code> option has been set to <code class="docutils literal notranslate"><span class="pre">df.error</span></code> so that the reported test matches the table from <code class="docutils literal notranslate"><span class="pre">summary()</span></code>. <code class="docutils literal notranslate"><span class="pre">emmeans</span></code> actually has some better ways of adjusting the degrees of freedom to accommodate the uncertainty in estimating <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, but this is a complication we will leave to one side for now.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="0.intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="2.lme-framework.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Mixed-effects Framework</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gls-theory">GLS Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-gls-do">What Does GLS Do?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-covariance-structure">Estimating the Covariance Structure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ignore-the-problem">1. Ignore the Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-effective-degrees-of-freedom">2. Calculate <em>Effective</em> Degrees of Freedom</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#produce-results-that-are-asymptotically-correct">3. Produce Results that are <em>Asymptotically</em> Correct</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance-constraints">Covariance Constraints</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gls-in-r">GLS in <code class="docutils literal notranslate"><span class="pre">R</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-paired-t-test-using-gls">The Paired <span class="math notranslate nohighlight">\(t\)</span>-test Using GLS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-using-gls">Inference Using GLS</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-p-value-problem">The <span class="math notranslate nohighlight">\(p\)</span>-value Problem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#omnibus-tests-and-follow-ups">Omnibus Tests and Follow-ups</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-we-use-gls">When Should We Use GLS?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr Martyn McFarquhar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>